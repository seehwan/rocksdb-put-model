#!/usr/bin/env python3

import re
import json
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import seaborn as sns

def parse_fillrandom_results(file_path):
    """FillRandom ê²°ê³¼ì—ì„œ ì„±ëŠ¥ ë°ì´í„° ì¶”ì¶œ"""
    data = []
    
    with open(file_path, 'r') as f:
        content = f.read()
    
    # 30ì´ˆ ê°„ê²© í†µê³„ ì¶”ì¶œ
    pattern = r'thread (\d+): \((\d+),(\d+)\) ops and \(([\d.]+),([\d.]+)\) ops/second in \(([\d.]+),([\d.]+)\) seconds'
    matches = re.findall(pattern, content)
    
    for match in matches:
        thread_id, current_ops, total_ops, current_ops_sec, total_ops_sec, current_time, total_time = match
        data.append({
            'thread': int(thread_id),
            'current_ops': int(current_ops),
            'total_ops': int(total_ops),
            'current_ops_sec': float(current_ops_sec),
            'total_ops_sec': float(total_ops_sec),
            'current_time': float(current_time),
            'total_time': float(total_time)
        })
    
    return pd.DataFrame(data)

def extract_benchmark_summary(file_path):
    """ë²¤ì¹˜ë§ˆí¬ ìš”ì•½ ì •ë³´ ì¶”ì¶œ"""
    with open(file_path, 'r') as f:
        content = f.read()
    
    # ìµœì¢… ê²°ê³¼ ì¶”ì¶œ
    pattern = r'(\w+)\s+:\s+([\d.]+)\s+micros/op\s+([\d.]+)\s+ops/sec\s+([\d.]+)\s+seconds\s+([\d.]+)\s+operations'
    match = re.search(pattern, content)
    
    if match:
        benchmark, micros_per_op, ops_per_sec, duration, operations = match.groups()
        return {
            'benchmark': benchmark,
            'micros_per_op': float(micros_per_op),
            'ops_per_sec': float(ops_per_sec),
            'duration': float(duration),
            'operations': float(operations)
        }
    return None

def parse_rocksdb_log(log_path):
    """RocksDB LOGì—ì„œ Flush/Compaction ì´ë²¤íŠ¸ ì¶”ì¶œ"""
    flush_events = []
    compaction_events = []
    
    with open(log_path, 'r') as f:
        for line in f:
            # Flush ì´ë²¤íŠ¸
            if 'flush' in line.lower() and 'level' in line:
                flush_events.append(line.strip())
            
            # Compaction ì´ë²¤íŠ¸
            if 'compaction' in line.lower() and 'level' in line:
                compaction_events.append(line.strip())
    
    return flush_events, compaction_events

def create_performance_visualizations():
    """ì„±ëŠ¥ ì‹œê°í™” ìƒì„±"""
    results_dir = Path("phase_b_final_results")
    output_dir = Path("phase_b_visualizations")
    output_dir.mkdir(exist_ok=True)
    
    print("=== Phase-B ê²°ê³¼ ì‹œê°í™” ===")
    
    # 1. FillRandom ì„±ëŠ¥ ì¶”ì´
    print("1. FillRandom ì„±ëŠ¥ ì¶”ì´ ë¶„ì„...")
    try:
        df = parse_fillrandom_results(results_dir / "fillrandom_results.txt")
        if not df.empty:
            plt.figure(figsize=(15, 10))
            
            # ì„œë¸Œí”Œë¡¯ 1: ìŠ¤ë ˆë“œë³„ ì„±ëŠ¥ ì¶”ì´
            plt.subplot(2, 2, 1)
            for thread in df['thread'].unique():
                thread_data = df[df['thread'] == thread]
                plt.plot(thread_data['total_time'], thread_data['current_ops_sec'], 
                        label=f'Thread {thread}', alpha=0.7)
            plt.xlabel('Time (seconds)')
            plt.ylabel('Operations/sec')
            plt.title('FillRandom: Thread Performance Over Time')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            # ì„œë¸Œí”Œë¡¯ 2: ì „ì²´ ì„±ëŠ¥ ì¶”ì´
            plt.subplot(2, 2, 2)
            total_ops_sec = df.groupby('total_time')['current_ops_sec'].sum()
            plt.plot(total_ops_sec.index, total_ops_sec.values, 'b-', linewidth=2)
            plt.xlabel('Time (seconds)')
            plt.ylabel('Total Operations/sec')
            plt.title('FillRandom: Total Throughput Over Time')
            plt.grid(True, alpha=0.3)
            
            # ì„œë¸Œí”Œë¡¯ 3: ìŠ¤ë ˆë“œë³„ ëˆ„ì  ì‘ì—…ëŸ‰
            plt.subplot(2, 2, 3)
            for thread in df['thread'].unique():
                thread_data = df[df['thread'] == thread]
                plt.plot(thread_data['total_time'], thread_data['total_ops'], 
                        label=f'Thread {thread}', alpha=0.7)
            plt.xlabel('Time (seconds)')
            plt.ylabel('Cumulative Operations')
            plt.title('FillRandom: Cumulative Operations by Thread')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            # ì„œë¸Œí”Œë¡¯ 4: ì„±ëŠ¥ ë¶„í¬
            plt.subplot(2, 2, 4)
            plt.hist(df['current_ops_sec'], bins=30, alpha=0.7, edgecolor='black')
            plt.xlabel('Operations/sec')
            plt.ylabel('Frequency')
            plt.title('FillRandom: Performance Distribution')
            plt.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(output_dir / "fillrandom_performance.png", dpi=300, bbox_inches='tight')
            plt.close()
            print(f"  âœ… FillRandom ì„±ëŠ¥ ì°¨íŠ¸ ì €ì¥: {output_dir / 'fillrandom_performance.png'}")
    except Exception as e:
        print(f"  âŒ FillRandom ë¶„ì„ ì˜¤ë¥˜: {e}")
    
    # 2. ë²¤ì¹˜ë§ˆí¬ ë¹„êµ
    print("2. ë²¤ì¹˜ë§ˆí¬ ì„±ëŠ¥ ë¹„êµ...")
    benchmarks = ['fillrandom', 'readrandomwriterandom', 'overwrite', 'mixgraph']
    benchmark_data = []
    
    for bench in benchmarks:
        file_path = results_dir / f"{bench}_results.txt"
        if file_path.exists():
            summary = extract_benchmark_summary(file_path)
            if summary:
                benchmark_data.append(summary)
    
    if benchmark_data:
        df_bench = pd.DataFrame(benchmark_data)
        
        plt.figure(figsize=(15, 5))
        
        # ì„œë¸Œí”Œë¡¯ 1: Ops/sec ë¹„êµ
        plt.subplot(1, 3, 1)
        bars = plt.bar(df_bench['benchmark'], df_bench['ops_per_sec'], 
                      color=['skyblue', 'lightgreen', 'lightcoral', 'gold'])
        plt.ylabel('Operations/sec')
        plt.title('Benchmark Throughput Comparison')
        plt.xticks(rotation=45)
        for bar, ops in zip(bars, df_bench['ops_per_sec']):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(df_bench['ops_per_sec'])*0.01,
                    f'{ops:,.0f}', ha='center', va='bottom', fontsize=8)
        plt.grid(True, alpha=0.3)
        
        # ì„œë¸Œí”Œë¡¯ 2: Latency ë¹„êµ
        plt.subplot(1, 3, 2)
        bars = plt.bar(df_bench['benchmark'], df_bench['micros_per_op'], 
                      color=['skyblue', 'lightgreen', 'lightcoral', 'gold'])
        plt.ylabel('Microseconds/Operation')
        plt.title('Benchmark Latency Comparison')
        plt.xticks(rotation=45)
        for bar, lat in zip(bars, df_bench['micros_per_op']):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(df_bench['micros_per_op'])*0.01,
                    f'{lat:.1f}', ha='center', va='bottom', fontsize=8)
        plt.grid(True, alpha=0.3)
        
        # ì„œë¸Œí”Œë¡¯ 3: ì´ ì‘ì—…ëŸ‰ ë¹„êµ
        plt.subplot(1, 3, 3)
        bars = plt.bar(df_bench['benchmark'], df_bench['operations']/1e6, 
                      color=['skyblue', 'lightgreen', 'lightcoral', 'gold'])
        plt.ylabel('Operations (Millions)')
        plt.title('Total Operations Comparison')
        plt.xticks(rotation=45)
        for bar, ops in zip(bars, df_bench['operations']/1e6):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(df_bench['operations']/1e6)*0.01,
                    f'{ops:.1f}M', ha='center', va='bottom', fontsize=8)
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_dir / "benchmark_comparison.png", dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  âœ… ë²¤ì¹˜ë§ˆí¬ ë¹„êµ ì°¨íŠ¸ ì €ì¥: {output_dir / 'benchmark_comparison.png'}")
    
    # 3. RocksDB LOG ë¶„ì„
    print("3. RocksDB LOG ì´ë²¤íŠ¸ ë¶„ì„...")
    try:
        flush_events, compaction_events = parse_rocksdb_log(results_dir / "rocksdb.log")
        
        plt.figure(figsize=(12, 8))
        
        # ì„œë¸Œí”Œë¡¯ 1: ì´ë²¤íŠ¸ íƒ€ì…ë³„ ê°œìˆ˜
        plt.subplot(2, 2, 1)
        event_counts = {'Flush Events': len(flush_events), 'Compaction Events': len(compaction_events)}
        bars = plt.bar(event_counts.keys(), event_counts.values(), 
                      color=['lightblue', 'lightgreen'])
        plt.ylabel('Event Count')
        plt.title('RocksDB Event Distribution')
        for bar, count in zip(bars, event_counts.values()):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(event_counts.values())*0.01,
                    f'{count}', ha='center', va='bottom', fontsize=12)
        plt.grid(True, alpha=0.3)
        
        # ì„œë¸Œí”Œë¡¯ 2: Flush ì´ë²¤íŠ¸ ì‹œê°„ ë¶„í¬ (ê°„ë‹¨í•œ ë¶„ì„)
        plt.subplot(2, 2, 2)
        if flush_events:
            # ê°„ë‹¨í•œ ì‹œê°„ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ íŒŒì‹± í•„ìš”)
            flush_times = [i for i in range(len(flush_events))]
            plt.hist(flush_times, bins=min(20, len(flush_times)), alpha=0.7, color='lightblue', edgecolor='black')
            plt.xlabel('Event Index')
            plt.ylabel('Frequency')
            plt.title('Flush Events Distribution')
        else:
            plt.text(0.5, 0.5, 'No Flush Events Found', ha='center', va='center', transform=plt.gca().transAxes)
        plt.grid(True, alpha=0.3)
        
        # ì„œë¸Œí”Œë¡¯ 3: Compaction ì´ë²¤íŠ¸ ì‹œê°„ ë¶„í¬
        plt.subplot(2, 2, 3)
        if compaction_events:
            compaction_times = [i for i in range(len(compaction_events))]
            plt.hist(compaction_times, bins=min(20, len(compaction_times)), alpha=0.7, color='lightgreen', edgecolor='black')
            plt.xlabel('Event Index')
            plt.ylabel('Frequency')
            plt.title('Compaction Events Distribution')
        else:
            plt.text(0.5, 0.5, 'No Compaction Events Found', ha='center', va='center', transform=plt.gca().transAxes)
        plt.grid(True, alpha=0.3)
        
        # ì„œë¸Œí”Œë¡¯ 4: ì´ë²¤íŠ¸ ë¹„ìœ¨
        plt.subplot(2, 2, 4)
        total_events = len(flush_events) + len(compaction_events)
        if total_events > 0:
            sizes = [len(flush_events), len(compaction_events)]
            labels = ['Flush', 'Compaction']
            colors = ['lightblue', 'lightgreen']
            plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
            plt.title('Event Type Ratio')
        else:
            plt.text(0.5, 0.5, 'No Events Found', ha='center', va='center', transform=plt.gca().transAxes)
        
        plt.tight_layout()
        plt.savefig(output_dir / "rocksdb_events.png", dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  âœ… RocksDB ì´ë²¤íŠ¸ ë¶„ì„ ì°¨íŠ¸ ì €ì¥: {output_dir / 'rocksdb_events.png'}")
    except Exception as e:
        print(f"  âŒ RocksDB LOG ë¶„ì„ ì˜¤ë¥˜: {e}")
    
    # 4. ì‹¤í—˜ ìš”ì•½ ëŒ€ì‹œë³´ë“œ
    print("4. ì‹¤í—˜ ìš”ì•½ ëŒ€ì‹œë³´ë“œ ìƒì„±...")
    try:
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        
        # ì‹¤í—˜ ì •ë³´
        ax1.text(0.1, 0.8, "Phase-B Final Experiment Summary", fontsize=16, fontweight='bold', transform=ax1.transAxes)
        ax1.text(0.1, 0.7, f"Completion Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", transform=ax1.transAxes)
        ax1.text(0.1, 0.6, "Key Count: 1,000,000,000 (1 billion)", transform=ax1.transAxes)
        ax1.text(0.1, 0.5, "Expected Data Size: 1000 GB", transform=ax1.transAxes)
        ax1.text(0.1, 0.4, "Completed Workloads: 4", transform=ax1.transAxes)
        ax1.text(0.1, 0.3, "Total Duration: ~2 days 15 hours", transform=ax1.transAxes)
        ax1.set_xlim(0, 1)
        ax1.set_ylim(0, 1)
        ax1.axis('off')
        ax1.set_title('Experiment Overview', fontsize=14, fontweight='bold')
        
        # ì£¼ìš” ê°œì„ ì‚¬í•­
        improvements = [
            "âœ… ë°ì´í„° í¬ê¸° 10ë°° ì¦ê°€ (10ì–µ í‚¤)",
            "âœ… ìë™ ë””ìŠ¤í¬ ì •ë¦¬ ë° ì´ˆê¸°í™”",
            "âœ… ê° ë²¤ì¹˜ë§ˆí¬ ì‚¬ì´ Compaction ëŒ€ê¸° ì‹œê°„ ì¶”ê°€",
            "âœ… ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ìˆ˜ ìµœì í™” (8â†’4)",
            "âœ… Compaction ìƒíƒœ ëª¨ë‹ˆí„°ë§"
        ]
        ax2.text(0.1, 0.8, "Key Improvements:", fontsize=14, fontweight='bold', transform=ax2.transAxes)
        for i, improvement in enumerate(improvements):
            ax2.text(0.1, 0.7 - i*0.1, improvement, fontsize=10, transform=ax2.transAxes)
        ax2.set_xlim(0, 1)
        ax2.set_ylim(0, 1)
        ax2.axis('off')
        ax2.set_title('Improvements', fontsize=14, fontweight='bold')
        
        # ì„±ëŠ¥ ìš”ì•½ (ê°„ë‹¨í•œ ë§‰ëŒ€ ì°¨íŠ¸)
        if benchmark_data:
            df_bench = pd.DataFrame(benchmark_data)
            ax3.bar(df_bench['benchmark'], df_bench['ops_per_sec']/1000, 
                   color=['skyblue', 'lightgreen', 'lightcoral', 'gold'])
            ax3.set_ylabel('Throughput (K ops/sec)')
            ax3.set_title('Benchmark Performance Summary')
            ax3.tick_params(axis='x', rotation=45)
            ax3.grid(True, alpha=0.3)
        
        # íŒŒì¼ í¬ê¸° ë¶„í¬
        result_files = list(results_dir.glob("*.txt"))
        file_sizes = [f.stat().st_size / (1024*1024) for f in result_files]  # MB
        file_names = [f.stem for f in result_files]
        
        ax4.bar(range(len(file_names)), file_sizes, color='lightcoral')
        ax4.set_ylabel('File Size (MB)')
        ax4.set_title('Result Files Size Distribution')
        ax4.set_xticks(range(len(file_names)))
        ax4.set_xticklabels(file_names, rotation=45, ha='right')
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_dir / "experiment_dashboard.png", dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  âœ… ì‹¤í—˜ ëŒ€ì‹œë³´ë“œ ì €ì¥: {output_dir / 'experiment_dashboard.png'}")
    except Exception as e:
        print(f"  âŒ ëŒ€ì‹œë³´ë“œ ìƒì„± ì˜¤ë¥˜: {e}")
    
    # 5. ìš”ì•½ JSON ìƒì„±
    print("5. ì‹œê°í™” ìš”ì•½ JSON ìƒì„±...")
    summary = {
        'visualization_info': {
            'created_at': datetime.now().isoformat(),
            'total_charts': 4,
            'charts_created': [
                'fillrandom_performance.png',
                'benchmark_comparison.png', 
                'rocksdb_events.png',
                'experiment_dashboard.png'
            ]
        },
        'data_summary': {
            'fillrandom_data_points': len(df) if 'df' in locals() and not df.empty else 0,
            'benchmark_count': len(benchmark_data),
            'rocksdb_events': {
                'flush_events': len(flush_events) if 'flush_events' in locals() else 0,
                'compaction_events': len(compaction_events) if 'compaction_events' in locals() else 0
            }
        }
    }
    
    with open(output_dir / "visualization_summary.json", 'w') as f:
        json.dump(summary, f, indent=2)
    
    print(f"  âœ… ì‹œê°í™” ìš”ì•½ ì €ì¥: {output_dir / 'visualization_summary.json'}")
    
    print(f"\n=== ì‹œê°í™” ì™„ë£Œ ===")
    print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    print(f"ìƒì„±ëœ ì°¨íŠ¸:")
    for chart_file in output_dir.glob("*.png"):
        print(f"  ğŸ“Š {chart_file.name}")

if __name__ == "__main__":
    create_performance_visualizations()
