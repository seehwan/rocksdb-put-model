#!/usr/bin/env python3
"""
ìƒì„¸ ëˆ„ì  I/O ì²˜ë¦¬ëŸ‰ ë¶„ì„
Flushë¶€í„° ê° ë ˆë²¨ê¹Œì§€ì˜ ìƒì„¸í•œ ëˆ„ì  I/O ì²˜ë¦¬ëŸ‰ ë¶„ì„
"""

import os
import re
import json
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns

# Liberation Serif í°íŠ¸ ì„¤ì • (Times ìŠ¤íƒ€ì¼)
plt.rcParams['font.family'] = 'Liberation Serif'
plt.rcParams['axes.unicode_minus'] = False

def parse_log_file(log_file):
    """LOG íŒŒì¼ íŒŒì‹±"""
    print(f"ğŸ“– LOG íŒŒì¼ íŒŒì‹± ì¤‘: {log_file}")
    
    stats_data = []
    compaction_data = []
    flush_data = []
    current_timestamp = None
    
    with open(log_file, 'r') as f:
        for line in f:
            line = line.strip()
            
            # ì‹œê°„ ì •ë³´ ì¶”ì¶œ
            time_match = re.search(r'(\d{4}/\d{2}/\d{2}-\d{2}:\d{2}:\d{2}\.\d+)', line)
            if time_match:
                current_timestamp = time_match.group(1)
            
            # Stats ë¡œê·¸ íŒŒì‹±
            if "Cumulative writes:" in line:
                stats_data.append(parse_stats_line(line, current_timestamp))
            
            # Compaction ë¡œê·¸ íŒŒì‹±
            elif "Compaction start" in line or "Compaction finished" in line:
                compaction_data.append(parse_compaction_line(line))
            
            # Flush ë¡œê·¸ íŒŒì‹±
            elif "Flushing memtable" in line or "Flush lasted" in line:
                flush_data.append(parse_flush_line(line))
    
    print(f"âœ… íŒŒì‹± ì™„ë£Œ: Stats {len(stats_data)}ê°œ, Compaction {len(compaction_data)}ê°œ, Flush {len(flush_data)}ê°œ")
    return stats_data, compaction_data, flush_data

def parse_stats_line(line, timestamp):
    """Stats ë¼ì¸ íŒŒì‹±"""
    try:
        # cumulative_writes ì¶”ì¶œ (K ë‹¨ìœ„ ì²˜ë¦¬)
        cum_writes_match = re.search(r'Cumulative writes: (\d+)K writes', line)
        if cum_writes_match:
            cum_writes = int(cum_writes_match.group(1)) * 1000
        else:
            cum_writes_match = re.search(r'Cumulative writes: (\d+) writes', line)
            cum_writes = int(cum_writes_match.group(1)) if cum_writes_match else 0
        
        # MB/sì—ì„œ write_rate ê³„ì‚°
        mbps_match = re.search(r'ingest: [\d.]+ GB, ([\d.]+) MB/s', line)
        if mbps_match:
            mbps = float(mbps_match.group(1))
            write_rate = mbps * 1024  # MB/s * 1024 = ops/sec (1KB per op)
        else:
            write_rate = 0
        
        return {
            'timestamp': timestamp,
            'write_rate': write_rate,
            'cumulative_writes': cum_writes
        }
    except Exception as e:
        return None

def parse_compaction_line(line):
    """Compaction ë¼ì¸ íŒŒì‹±"""
    try:
        timestamp_match = re.search(r'(\d{4}/\d{2}/\d{2}-\d{2}:\d{2}:\d{2}\.\d+)', line)
        timestamp = timestamp_match.group(1) if timestamp_match else None
        
        # level ì¶”ì¶œ (Base level ì •ë³´ì—ì„œ)
        level_match = re.search(r'Base level (\d+)', line)
        level = int(level_match.group(1)) if level_match else -1
        
        # Compaction íƒ€ì… ì¶”ì¶œ
        compaction_type = "unknown"
        if "start" in line:
            compaction_type = "start"
        elif "finished" in line:
            compaction_type = "finished"
        
        return {
            'timestamp': timestamp,
            'level': level,
            'type': compaction_type
        }
    except Exception as e:
        return None

def parse_flush_line(line):
    """Flush ë¼ì¸ íŒŒì‹±"""
    try:
        timestamp_match = re.search(r'(\d{4}/\d{2}/\d{2}-\d{2}:\d{2}:\d{2}\.\d+)', line)
        timestamp = timestamp_match.group(1) if timestamp_match else None
        
        # Flush íƒ€ì… ì¶”ì¶œ
        flush_type = "unknown"
        if "Flushing memtable" in line:
            flush_type = "start"
        elif "Flush lasted" in line:
            flush_type = "finished"
        
        return {
            'timestamp': timestamp,
            'type': flush_type
        }
    except Exception as e:
        return None

def analyze_detailed_cumulative_io(stats_data, compaction_data, flush_data):
    """ìƒì„¸ ëˆ„ì  I/O ì²˜ë¦¬ëŸ‰ ë¶„ì„"""
    print("ğŸ“Š ìƒì„¸ ëˆ„ì  I/O ì²˜ë¦¬ëŸ‰ ë¶„ì„ ì¤‘...")
    
    try:
        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        stats_df = pd.DataFrame([d for d in stats_data if d is not None])
        compaction_df = pd.DataFrame([d for d in compaction_data if d is not None])
        flush_df = pd.DataFrame([d for d in flush_data if d is not None])
        
        if stats_df.empty:
            print("âŒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
            return
        
        # ì‹œê°„ ë³€í™˜
        stats_df['datetime'] = pd.to_datetime(stats_df['timestamp'], format='%Y/%m/%d-%H:%M:%S.%f')
        if not compaction_df.empty:
            compaction_df['datetime'] = pd.to_datetime(compaction_df['timestamp'], format='%Y/%m/%d-%H:%M:%S.%f')
        if not flush_df.empty:
            flush_df['datetime'] = pd.to_datetime(flush_df['timestamp'], format='%Y/%m/%d-%H:%M:%S.%f')
        
        # ì‹œê°„ë³„ ê·¸ë£¹í™” (ì‹œê°„ ë‹¨ìœ„)
        stats_df['hour'] = stats_df['datetime'].dt.floor('h')
        if not compaction_df.empty:
            compaction_df['hour'] = compaction_df['datetime'].dt.floor('h')
        if not flush_df.empty:
            flush_df['hour'] = flush_df['datetime'].dt.floor('h')
        
        # ì‹œê°í™” ìƒì„±
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))
        
        # 1. ì‹œê°„ë³„ ëˆ„ì  I/O ì²˜ë¦¬ëŸ‰ (Flush + Compaction)
        if not flush_df.empty:
            hourly_flush = flush_df.groupby('hour').size()
            ax1.plot(hourly_flush.index, hourly_flush.values, 'b-', linewidth=2, marker='o', label='Flush I/O')
        
        if not compaction_df.empty:
            # ë ˆë²¨ë³„ ì‹œê°„ë³„ I/O ëŸ‰ ê³„ì‚°
            hourly_level_io = compaction_df.groupby(['hour', 'level']).size().unstack(fill_value=0)
            
            # ë ˆë²¨ë³„ ëˆ„ì  I/O ì²˜ë¦¬ëŸ‰
            level_order = sorted([col for col in hourly_level_io.columns if col != -1])
            colors = plt.cm.Set3(np.linspace(0, 1, len(level_order)))
            
            for i, level in enumerate(level_order):
                if level in hourly_level_io.columns:
                    ax1.plot(hourly_level_io.index, hourly_level_io[level], 
                            color=colors[i], linewidth=2, marker='s', markersize=4,
                            label=f'Level {level} I/O')
        
        ax1.set_title('Hourly Cumulative I/O Volume by Level', fontsize=16, fontweight='bold')
        ax1.set_xlabel('Time')
        ax1.set_ylabel('I/O Count per Hour')
        ax1.grid(True, alpha=0.3)
        ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        ax1.tick_params(axis='x', rotation=45)
        
        # 2. ë ˆë²¨ë³„ ëˆ„ì  I/O ì²˜ë¦¬ëŸ‰ íˆíŠ¸ë§µ
        if not compaction_df.empty:
            # ì‹œê°„ë³„ ë ˆë²¨ë³„ I/O ëŸ‰ íˆíŠ¸ë§µ
            hourly_level_io = compaction_df.groupby(['hour', 'level']).size().unstack(fill_value=0)
            
            if not hourly_level_io.empty:
                # ë ˆë²¨ ìˆœì„œ ì •ë ¬
                level_order = sorted([col for col in hourly_level_io.columns if col != -1])
                hourly_level_io_sorted = hourly_level_io[level_order]
                
                im = ax2.imshow(hourly_level_io_sorted.T.values, cmap='YlOrRd', aspect='auto')
                ax2.set_title('Hourly I/O Volume Heatmap by Level', fontsize=16, fontweight='bold')
                ax2.set_xlabel('Time (Hours)')
                ax2.set_ylabel('Level')
                ax2.set_yticks(range(len(level_order)))
                ax2.set_yticklabels([f'Level {l}' for l in level_order])
                
                # ì‹œê°„ ì¶• ë ˆì´ë¸” (24ì‹œê°„ë§ˆë‹¤)
                time_labels = hourly_level_io_sorted.index[::24]
                time_positions = range(0, len(hourly_level_io_sorted), 24)
                ax2.set_xticks(time_positions)
                ax2.set_xticklabels([t.strftime('%m/%d %H:%M') for t in time_labels], rotation=45)
                
                # ì»¬ëŸ¬ë°” ì¶”ê°€
                cbar = plt.colorbar(im, ax=ax2)
                cbar.set_label('I/O Count')
        
        # 3. ë ˆë²¨ë³„ I/O ì²˜ë¦¬ëŸ‰ ë¶„í¬ (ë°•ìŠ¤í”Œë¡¯)
        if not compaction_df.empty:
            level_data = []
            level_labels = []
            for level in level_order:
                if level in hourly_level_io.columns:
                    level_data.append(hourly_level_io[level].values)
                    level_labels.append(f'Level {level}')
            
            if level_data:
                bp = ax3.boxplot(level_data, labels=level_labels, patch_artist=True)
                colors = plt.cm.Set3(np.linspace(0, 1, len(level_data)))
                for patch, color in zip(bp['boxes'], colors):
                    patch.set_facecolor(color)
                    patch.set_alpha(0.7)
        
        ax3.set_title('I/O Volume Distribution by Level', fontsize=16, fontweight='bold')
        ax3.set_xlabel('Level')
        ax3.set_ylabel('I/O Count per Hour')
        ax3.grid(True, alpha=0.3)
        ax3.tick_params(axis='x', rotation=45)
        
        # 4. ëˆ„ì  I/O ì²˜ë¦¬ëŸ‰ ë¹„ìœ¨ (íŒŒì´ ì°¨íŠ¸)
        if not compaction_df.empty:
            total_io_by_level = compaction_df['level'].value_counts().sort_index()
            level_labels_pie = [f'Level {l}' for l in total_io_by_level.index]
            colors_pie = plt.cm.Set3(np.linspace(0, 1, len(total_io_by_level)))
            
            wedges, texts, autotexts = ax4.pie(total_io_by_level.values, labels=level_labels_pie, 
                                              autopct='%1.1f%%', colors=colors_pie, startangle=90)
            ax4.set_title('Cumulative I/O Volume Ratio by Level', fontsize=16, fontweight='bold')
            
            # í†µê³„ ì •ë³´ ì¶”ê°€
            stats_text = f"""
            Total I/O Operations: {total_io_by_level.sum():,}
            Analysis Period: {len(hourly_level_io)} hours
            Most Active Level: {level_labels_pie[total_io_by_level.idxmax()]}
            """
            ax4.text(1.3, 0.5, stats_text, transform=ax4.transAxes, fontsize=10,
                    verticalalignment='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
        
        plt.tight_layout()
        plt.savefig('detailed_cumulative_io_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print("âœ… ìƒì„¸ ëˆ„ì  I/O ì²˜ë¦¬ëŸ‰ ë¶„ì„ ì‹œê°í™” ì €ì¥ ì™„ë£Œ: detailed_cumulative_io_analysis.png")
        
        # ìƒì„¸ ë¶„ì„ ê²°ê³¼ ì¶œë ¥
        print("\nğŸ“Š ìƒì„¸ ëˆ„ì  I/O ì²˜ë¦¬ëŸ‰ ë¶„ì„ ê²°ê³¼:")
        print(f"  ë¶„ì„ ê¸°ê°„: {len(hourly_level_io)} ì‹œê°„")
        print(f"  ì´ I/O ì‘ì—…: {total_io_by_level.sum():,}íšŒ")
        print(f"  ë ˆë²¨ë³„ I/O ëŸ‰:")
        for level, count in total_io_by_level.items():
            level_name = f'Level {level}'
            percentage = (count / total_io_by_level.sum()) * 100
            print(f"    {level_name}: {count:,}íšŒ ({percentage:.1f}%)")
        
        # Flush ë¶„ì„ (ìˆëŠ” ê²½ìš°)
        if not flush_df.empty:
            print(f"\n  Flush ë¶„ì„:")
            print(f"    ì´ Flush: {len(flush_df)}íšŒ")
            flush_type_counts = flush_df['type'].value_counts()
            for flush_type, count in flush_type_counts.items():
                print(f"    {flush_type}: {count}íšŒ")
        
        # ì‹œê°„ëŒ€ë³„ ë¶„ì„
        print(f"\n  ì‹œê°„ëŒ€ë³„ I/O íŒ¨í„´:")
        hourly_total = hourly_level_io.sum(axis=1)
        peak_hour = hourly_total.idxmax()
        min_hour = hourly_total.idxmin()
        print(f"    ìµœëŒ€ I/O ì‹œê°„: {peak_hour.strftime('%Y-%m-%d %H:%M')} ({hourly_total.max():,}íšŒ)")
        print(f"    ìµœì†Œ I/O ì‹œê°„: {min_hour.strftime('%Y-%m-%d %H:%M')} ({hourly_total.min():,}íšŒ)")
        print(f"    í‰ê·  I/O ëŸ‰: {hourly_total.mean():.1f}íšŒ/ì‹œê°„")
        
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ ìƒì„¸ ëˆ„ì  I/O ì²˜ë¦¬ëŸ‰ ë¶„ì„ ì‹œì‘...")
    
    # LOG íŒŒì¼ ì°¾ê¸°
    log_files = list(Path('.').glob('LOG*')) + list(Path('logs').glob('LOG*'))
    
    if not log_files:
        print("âŒ LOG íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        print("Phase-B ì‹¤í–‰ í›„ LOG íŒŒì¼ì´ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return
    
    # ê°€ì¥ í° LOG íŒŒì¼ ì„ íƒ (ë©”ì¸ ë¡œê·¸)
    main_log = max(log_files, key=lambda f: f.stat().st_size)
    print(f"ğŸ“– ë©”ì¸ LOG íŒŒì¼: {main_log}")
    
    # LOG íŒŒì¼ ë¶„ì„
    stats_data, compaction_data, flush_data = parse_log_file(main_log)
    
    if not stats_data:
        print("âŒ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    # ìƒì„¸ ëˆ„ì  I/O ì²˜ë¦¬ëŸ‰ ë¶„ì„
    analyze_detailed_cumulative_io(stats_data, compaction_data, flush_data)
    
    print("\nâœ… ìƒì„¸ ëˆ„ì  I/O ì²˜ë¦¬ëŸ‰ ë¶„ì„ ì™„ë£Œ!")

if __name__ == "__main__":
    main()


