#!/usr/bin/env python3
"""
ìƒì„¸ ë ˆë²¨ë³„ I/O ë¶„ì„
RocksDB LOG íŒŒì¼ì—ì„œ ë ˆë²¨ë³„ I/O íŒ¨í„´ì˜ ì„¸ë¶€ ë¶„ì„
"""

import os
import re
import json
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns

# Liberation Serif í°íŠ¸ ì„¤ì • (Times ìŠ¤íƒ€ì¼)
plt.rcParams['font.family'] = 'Liberation Serif'
plt.rcParams['axes.unicode_minus'] = False

def parse_log_file(log_file):
    """LOG íŒŒì¼ íŒŒì‹±"""
    print(f"ğŸ“– LOG íŒŒì¼ íŒŒì‹± ì¤‘: {log_file}")
    
    stats_data = []
    compaction_data = []
    flush_data = []
    current_timestamp = None
    
    with open(log_file, 'r') as f:
        for line in f:
            line = line.strip()
            
            # ì‹œê°„ ì •ë³´ ì¶”ì¶œ
            time_match = re.search(r'(\d{4}/\d{2}/\d{2}-\d{2}:\d{2}:\d{2}\.\d+)', line)
            if time_match:
                current_timestamp = time_match.group(1)
            
            # Stats ë¡œê·¸ íŒŒì‹±
            if "Cumulative writes:" in line:
                stats_data.append(parse_stats_line(line, current_timestamp))
            
            # Compaction ë¡œê·¸ íŒŒì‹±
            elif "Compaction start" in line or "Compaction finished" in line:
                compaction_data.append(parse_compaction_line(line))
            
            # Flush ë¡œê·¸ íŒŒì‹±
            elif "Flush start" in line or "Flush finished" in line:
                flush_data.append(parse_flush_line(line))
    
    print(f"âœ… íŒŒì‹± ì™„ë£Œ: Stats {len(stats_data)}ê°œ, Compaction {len(compaction_data)}ê°œ, Flush {len(flush_data)}ê°œ")
    return stats_data, compaction_data, flush_data

def parse_stats_line(line, timestamp):
    """Stats ë¼ì¸ íŒŒì‹±"""
    try:
        write_rate_match = re.search(r'write_rate:(\d+(?:\.\d+)?)', line)
        write_rate = float(write_rate_match.group(1)) if write_rate_match else 0
        
        cum_writes_match = re.search(r'Cumulative writes:(\d+)', line)
        cum_writes = int(cum_writes_match.group(1)) if cum_writes_match else 0
        
        return {
            'timestamp': timestamp,
            'write_rate': write_rate,
            'cumulative_writes': cum_writes
        }
    except Exception as e:
        return None

def parse_compaction_line(line):
    """Compaction ë¼ì¸ íŒŒì‹±"""
    try:
        level_match = re.search(r'level:(\d+)', line)
        level = int(level_match.group(1)) if level_match else -1
        
        timestamp_match = re.search(r'(\d{4}/\d{2}/\d{2}-\d{2}:\d{2}:\d{2}\.\d+)', line)
        timestamp = timestamp_match.group(1) if timestamp_match else None
        
        # Compaction íƒ€ì… ì¶”ì¶œ
        compaction_type = "unknown"
        if "start" in line:
            compaction_type = "start"
        elif "finished" in line:
            compaction_type = "finished"
        
        return {
            'timestamp': timestamp,
            'level': level,
            'type': compaction_type
        }
    except Exception as e:
        return None

def parse_flush_line(line):
    """Flush ë¼ì¸ íŒŒì‹±"""
    try:
        timestamp_match = re.search(r'(\d{4}/\d{2}/\d{2}-\d{2}:\d{2}:\d{2}\.\d+)', line)
        timestamp = timestamp_match.group(1) if timestamp_match else None
        
        # Flush íƒ€ì… ì¶”ì¶œ
        flush_type = "unknown"
        if "start" in line:
            flush_type = "start"
        elif "finished" in line:
            flush_type = "finished"
        
        return {
            'timestamp': timestamp,
            'type': flush_type
        }
    except Exception as e:
        return None

def analyze_detailed_level_io(stats_data, compaction_data, flush_data):
    """ìƒì„¸ ë ˆë²¨ë³„ I/O ë¶„ì„"""
    print("ğŸ“Š ìƒì„¸ ë ˆë²¨ë³„ I/O ë¶„ì„ ì¤‘...")
    
    try:
        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        stats_df = pd.DataFrame([d for d in stats_data if d is not None])
        compaction_df = pd.DataFrame([d for d in compaction_data if d is not None])
        flush_df = pd.DataFrame([d for d in flush_data if d is not None])
        
        if stats_df.empty:
            print("âŒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
            return
        
        # ì‹œê°„ ë³€í™˜
        stats_df['datetime'] = pd.to_datetime(stats_df['timestamp'], format='%Y/%m/%d-%H:%M:%S.%f')
        if not compaction_df.empty:
            compaction_df['datetime'] = pd.to_datetime(compaction_df['timestamp'], format='%Y/%m/%d-%H:%M:%S.%f')
        if not flush_df.empty:
            flush_df['datetime'] = pd.to_datetime(flush_df['timestamp'], format='%Y/%m/%d-%H:%M:%S.%f')
        
        # ì‹œê°„ë³„ ê·¸ë£¹í™”
        stats_df['hour'] = stats_df['datetime'].dt.floor('h')
        
        # ì‹œê°í™” ìƒì„±
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))
        
        # 1. ì„±ëŠ¥ ì§€í‘œ ì‹œê°„ë³„ ë³€í™”
        ax1.plot(stats_df['datetime'], stats_df['write_rate'], 'b-', linewidth=2, alpha=0.7, label='Write Rate')
        ax1.set_title('Write Rate Time Series', fontsize=16, fontweight='bold')
        ax1.set_xlabel('Time')
        ax1.set_ylabel('Write Rate (ops/sec)')
        ax1.grid(True, alpha=0.3)
        ax1.legend()
        ax1.tick_params(axis='x', rotation=45)
        
        # ì´ë™í‰ê·  ì¶”ê°€
        stats_df['ma_10'] = stats_df['write_rate'].rolling(window=10).mean()
        ax1.plot(stats_df['datetime'], stats_df['ma_10'], 'r--', alpha=0.8, linewidth=2, label='10-point MA')
        ax1.legend()
        
        # 2. ëˆ„ì  ì“°ê¸° vs ì„±ëŠ¥ ìƒê´€ê´€ê³„
        ax2.scatter(stats_df['cumulative_writes'], stats_df['write_rate'], 
                   c=range(len(stats_df)), cmap='viridis', alpha=0.6, s=50)
        ax2.set_title('Cumulative Writes vs Write Rate', fontsize=16, fontweight='bold')
        ax2.set_xlabel('Cumulative Writes')
        ax2.set_ylabel('Write Rate (ops/sec)')
        ax2.grid(True, alpha=0.3)
        
        # ì»¬ëŸ¬ë°” ì¶”ê°€
        cbar = plt.colorbar(ax2.collections[0], ax=ax2)
        cbar.set_label('Time Progression')
        
        # 3. ì‹œê°„ëŒ€ë³„ ì„±ëŠ¥ ë¶„í¬
        stats_df['hour_of_day'] = stats_df['datetime'].dt.hour
        hourly_stats = stats_df.groupby('hour_of_day')['write_rate'].agg(['mean', 'std', 'min', 'max'])
        
        ax3.errorbar(hourly_stats.index, hourly_stats['mean'], 
                    yerr=hourly_stats['std'], fmt='o-', capsize=5, capthick=2, linewidth=2)
        ax3.fill_between(hourly_stats.index, 
                        hourly_stats['mean'] - hourly_stats['std'],
                        hourly_stats['mean'] + hourly_stats['std'], 
                        alpha=0.3)
        ax3.set_title('Hourly Performance Distribution', fontsize=16, fontweight='bold')
        ax3.set_xlabel('Hour of Day')
        ax3.set_ylabel('Write Rate (ops/sec)')
        ax3.grid(True, alpha=0.3)
        ax3.set_xticks(range(0, 24, 2))
        
        # 4. ì„±ëŠ¥ ì €í•˜ ë¶„ì„
        # ì´ˆê¸° 10% vs ë§ˆì§€ë§‰ 10% ì„±ëŠ¥ ë¹„êµ
        initial_10_percent = int(len(stats_df) * 0.1)
        final_10_percent = int(len(stats_df) * 0.9)
        
        initial_performance = stats_df.iloc[:initial_10_percent]['write_rate'].mean()
        final_performance = stats_df.iloc[final_10_percent:]['write_rate'].mean()
        degradation_percent = ((initial_performance - final_performance) / initial_performance) * 100
        
        # ì„±ëŠ¥ êµ¬ê°„ë³„ ë¶„ì„
        performance_ranges = [
            (stats_df['write_rate'].quantile(0.8), stats_df['write_rate'].max(), 'High'),
            (stats_df['write_rate'].quantile(0.6), stats_df['write_rate'].quantile(0.8), 'Medium-High'),
            (stats_df['write_rate'].quantile(0.4), stats_df['write_rate'].quantile(0.6), 'Medium-Low'),
            (stats_df['write_rate'].min(), stats_df['write_rate'].quantile(0.4), 'Low')
        ]
        
        range_counts = []
        range_labels = []
        for min_val, max_val, label in performance_ranges:
            count = len(stats_df[(stats_df['write_rate'] >= min_val) & (stats_df['write_rate'] < max_val)])
            range_counts.append(count)
            range_labels.append(f'{label}\n({min_val:.1f}-{max_val:.1f})')
        
        colors = ['red', 'orange', 'yellow', 'green']
        bars = ax4.bar(range_labels, range_counts, color=colors, alpha=0.7)
        ax4.set_title('Performance Range Distribution', fontsize=16, fontweight='bold')
        ax4.set_xlabel('Performance Range (ops/sec)')
        ax4.set_ylabel('Data Points Count')
        ax4.grid(True, alpha=0.3)
        ax4.tick_params(axis='x', rotation=45)
        
        # ê°’ í‘œì‹œ
        for bar in bars:
            height = bar.get_height()
            ax4.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                    f'{int(height)}', ha='center', va='bottom', fontsize=10)
        
        # ì„±ëŠ¥ ì €í•˜ ì •ë³´ ì¶”ê°€
        degradation_text = f"""
        Performance Degradation Analysis:
        Initial Performance: {initial_performance:.1f} ops/sec
        Final Performance: {final_performance:.1f} ops/sec
        Degradation: {degradation_percent:.1f}%
        """
        ax4.text(0.02, 0.98, degradation_text, transform=ax4.transAxes, 
                fontsize=10, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
        
        plt.tight_layout()
        plt.savefig('detailed_level_io_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print("âœ… ìƒì„¸ ë ˆë²¨ë³„ I/O ë¶„ì„ ì‹œê°í™” ì €ì¥ ì™„ë£Œ: detailed_level_io_analysis.png")
        
        # ìƒì„¸ ë¶„ì„ ê²°ê³¼ ì¶œë ¥
        print("\nğŸ“Š ìƒì„¸ ë ˆë²¨ë³„ I/O ë¶„ì„ ê²°ê³¼:")
        print(f"  ë¶„ì„ ê¸°ê°„: {len(stats_df)} ë°ì´í„° í¬ì¸íŠ¸")
        print(f"  ì´ˆê¸° ì„±ëŠ¥: {initial_performance:.1f} ops/sec")
        print(f"  ìµœì¢… ì„±ëŠ¥: {final_performance:.1f} ops/sec")
        print(f"  ì„±ëŠ¥ ì €í•˜ìœ¨: {degradation_percent:.1f}%")
        print(f"  í‰ê·  ì„±ëŠ¥: {stats_df['write_rate'].mean():.1f} ops/sec")
        print(f"  ì„±ëŠ¥ í‘œì¤€í¸ì°¨: {stats_df['write_rate'].std():.1f} ops/sec")
        print(f"  ìµœëŒ€ ì„±ëŠ¥: {stats_df['write_rate'].max():.1f} ops/sec")
        print(f"  ìµœì†Œ ì„±ëŠ¥: {stats_df['write_rate'].min():.1f} ops/sec")
        
        # Compaction ë¶„ì„ (ìˆëŠ” ê²½ìš°)
        if not compaction_df.empty:
            print(f"\n  Compaction ë¶„ì„:")
            print(f"    ì´ Compaction: {len(compaction_df)}íšŒ")
            level_counts = compaction_df['level'].value_counts().sort_index()
            for level, count in level_counts.items():
                level_name = f'Level {level}' if level != -1 else 'Memtable'
                print(f"    {level_name}: {count}íšŒ")
        
        # Flush ë¶„ì„ (ìˆëŠ” ê²½ìš°)
        if not flush_df.empty:
            print(f"\n  Flush ë¶„ì„:")
            print(f"    ì´ Flush: {len(flush_df)}íšŒ")
            flush_type_counts = flush_df['type'].value_counts()
            for flush_type, count in flush_type_counts.items():
                print(f"    {flush_type}: {count}íšŒ")
        
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ ìƒì„¸ ë ˆë²¨ë³„ I/O ë¶„ì„ ì‹œì‘...")
    
    # LOG íŒŒì¼ ì°¾ê¸°
    log_files = list(Path('.').glob('LOG*')) + list(Path('logs').glob('LOG*'))
    
    if not log_files:
        print("âŒ LOG íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        print("Phase-B ì‹¤í–‰ í›„ LOG íŒŒì¼ì´ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return
    
    # ê°€ì¥ í° LOG íŒŒì¼ ì„ íƒ (ë©”ì¸ ë¡œê·¸)
    main_log = max(log_files, key=lambda f: f.stat().st_size)
    print(f"ğŸ“– ë©”ì¸ LOG íŒŒì¼: {main_log}")
    
    # LOG íŒŒì¼ ë¶„ì„
    stats_data, compaction_data, flush_data = parse_log_file(main_log)
    
    if not stats_data:
        print("âŒ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    # ìƒì„¸ ë ˆë²¨ë³„ I/O ë¶„ì„
    analyze_detailed_level_io(stats_data, compaction_data, flush_data)
    
    print("\nâœ… ìƒì„¸ ë ˆë²¨ë³„ I/O ë¶„ì„ ì™„ë£Œ!")

if __name__ == "__main__":
    main()


