#!/usr/bin/env python3
"""
ì„±ëŠ¥ ë³€í™” ê¸°ë°˜ êµ¬ê°„ ë¶„í•  ë¶„ì„
LOG ë°ì´í„°ì—ì„œ ì„±ëŠ¥ ë³€í™”ìœ¨ì„ ë¶„ì„í•˜ì—¬ ì˜ë¯¸ìˆëŠ” êµ¬ê°„ìœ¼ë¡œ ë¶„í• 
"""

import os
import re
import json
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import signal
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Liberation Serif í°íŠ¸ ì„¤ì • (Times ìŠ¤íƒ€ì¼)
plt.rcParams['font.family'] = 'Liberation Serif'
plt.rcParams['axes.unicode_minus'] = False

def parse_log_file(log_file):
    """LOG íŒŒì¼ íŒŒì‹±"""
    print(f"ğŸ“– LOG íŒŒì¼ íŒŒì‹± ì¤‘: {log_file}")
    
    stats_data = []
    current_timestamp = None
    
    with open(log_file, 'r') as f:
        for line in f:
            line = line.strip()
            
            # ì‹œê°„ ì •ë³´ ì¶”ì¶œ
            time_match = re.search(r'(\d{4}/\d{2}/\d{2}-\d{2}:\d{2}:\d{2}\.\d+)', line)
            if time_match:
                current_timestamp = time_match.group(1)
            
            # Stats ë¡œê·¸ íŒŒì‹±
            if "Cumulative writes:" in line:
                stats_data.append(parse_stats_line(line, current_timestamp))
    
    print(f"âœ… íŒŒì‹± ì™„ë£Œ: Stats {len(stats_data)}ê°œ")
    return stats_data

def parse_stats_line(line, timestamp):
    """Stats ë¼ì¸ íŒŒì‹±"""
    try:
        # Cumulative writes: 1130K writes, 1130K keys, 72K commit groups, 15.5 writes per commit group, ingest: 1.10 GB, 280.18 MB/s
        write_match = re.search(r'Cumulative writes: (\d+[KM]?) writes', line)
        mbps_match = re.search(r'(\d+\.\d+) MB/s', line)
        
        if write_match and mbps_match:
            # K, M ë‹¨ìœ„ ì²˜ë¦¬
            writes_str = write_match.group(1)
            if writes_str.endswith('K'):
                cumulative_writes = int(writes_str[:-1]) * 1000
            elif writes_str.endswith('M'):
                cumulative_writes = int(writes_str[:-1]) * 1000000
            else:
                cumulative_writes = int(writes_str)
            
            return {
                'timestamp': timestamp,
                'cumulative_writes': cumulative_writes,
                'write_rate': float(mbps_match.group(1))
            }
    except Exception:
        pass
    return None

def calculate_performance_metrics(stats_df):
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°"""
    print("ğŸ“Š ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘...")
    
    # ì‹œê°„ ì •ê·œí™” (ì´ˆ ë‹¨ìœ„)
    stats_df['time_seconds'] = (stats_df['datetime'] - stats_df['datetime'].iloc[0]).dt.total_seconds()
    
    # ì´ë™í‰ê· ì„ í†µí•œ ë…¸ì´ì¦ˆ ì œê±° (ìœˆë„ìš° í¬ê¸°: 100)
    window_size = 100
    stats_df['write_rate_smooth'] = stats_df['write_rate'].rolling(window=window_size, center=True).mean()
    
    # ì„±ëŠ¥ ë³€í™”ìœ¨ ê³„ì‚°
    stats_df['performance_change_rate'] = stats_df['write_rate_smooth'].pct_change().fillna(0)
    
    # ì„±ëŠ¥ ë³€í™”ìœ¨ì˜ ì ˆëŒ“ê°’ (ë³€í™” ê°•ë„)
    stats_df['performance_change_abs'] = np.abs(stats_df['performance_change_rate'])
    
    # ì„±ëŠ¥ ë³€í™”ìœ¨ì˜ ì´ë™í‰ê·  (ì¶”ì„¸ ë¶„ì„)
    stats_df['change_rate_trend'] = stats_df['performance_change_rate'].rolling(window=200, center=True).mean().fillna(0)
    
    # ì„±ëŠ¥ ì•ˆì •ì„± (ë³€ë™ê³„ìˆ˜)
    rolling_window = 500
    stats_df['performance_stability'] = stats_df['write_rate_smooth'].rolling(window=rolling_window).std() / stats_df['write_rate_smooth'].rolling(window=rolling_window).mean()
    stats_df['performance_stability'] = stats_df['performance_stability'].fillna(1.0)
    
    print("âœ… ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚° ì™„ë£Œ")
    return stats_df

def detect_performance_transitions(stats_df):
    """ì„±ëŠ¥ ì „í™˜ì  íƒì§€"""
    print("ğŸ“Š ì„±ëŠ¥ ì „í™˜ì  íƒì§€ ì¤‘...")
    
    # 1. ì„±ëŠ¥ ë³€í™”ìœ¨ ê¸°ë°˜ ì „í™˜ì  íƒì§€
    change_threshold = 0.02  # 2% ë³€í™”ìœ¨ ì„ê³„ê°’
    significant_changes = stats_df[np.abs(stats_df['performance_change_rate']) > change_threshold].copy()
    
    # 2. ì„±ëŠ¥ ìˆ˜ì¤€ ë³€í™” íƒì§€ (í° í­ ë³€í™”)
    performance_levels = []
    current_level = stats_df['write_rate_smooth'].iloc[0]
    level_threshold = 5.0  # 5 MB/s ì°¨ì´
    
    for i, row in stats_df.iterrows():
        if abs(row['write_rate_smooth'] - current_level) > level_threshold:
            performance_levels.append(i)
            current_level = row['write_rate_smooth']
    
    # 3. ì•ˆì •ì„± ë³€í™” íƒì§€
    stability_threshold = 0.5  # ì•ˆì •ì„± ë³€í™” ì„ê³„ê°’
    stability_changes = []
    
    for i in range(1, len(stats_df)):
        if i < len(stats_df) - 1:
            prev_stability = stats_df.iloc[i-1]['performance_stability']
            curr_stability = stats_df.iloc[i]['performance_stability']
            
            if not np.isnan(prev_stability) and not np.isnan(curr_stability):
                if abs(curr_stability - prev_stability) > stability_threshold:
                    stability_changes.append(i)
    
    # 4. ì‹œê³„ì—´ í´ëŸ¬ìŠ¤í„°ë§ì„ í†µí•œ êµ¬ê°„ ë¶„í• 
    # ì„±ëŠ¥ íŠ¹ì„± ë²¡í„° ìƒì„±
    features = ['write_rate_smooth', 'performance_change_abs', 'performance_stability']
    feature_data = stats_df[features].fillna(stats_df[features].mean())
    
    # í‘œì¤€í™”
    scaler = StandardScaler()
    feature_data_scaled = scaler.fit_transform(feature_data)
    
    # K-means í´ëŸ¬ìŠ¤í„°ë§ (3ê°œ í´ëŸ¬ìŠ¤í„°)
    kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
    stats_df['cluster'] = kmeans.fit_predict(feature_data_scaled)
    
    # í´ëŸ¬ìŠ¤í„° ê²½ê³„ ì°¾ê¸°
    cluster_boundaries = []
    current_cluster = stats_df['cluster'].iloc[0]
    
    for i, cluster in enumerate(stats_df['cluster']):
        if cluster != current_cluster:
            cluster_boundaries.append(i)
            current_cluster = cluster
    
    print(f"âœ… ì„±ëŠ¥ ì „í™˜ì  íƒì§€ ì™„ë£Œ:")
    print(f"   - í° ë³€í™”ìœ¨ ì§€ì : {len(significant_changes)}ê°œ")
    print(f"   - ì„±ëŠ¥ ìˆ˜ì¤€ ë³€í™”: {len(performance_levels)}ê°œ")
    print(f"   - ì•ˆì •ì„± ë³€í™”: {len(stability_changes)}ê°œ")
    print(f"   - í´ëŸ¬ìŠ¤í„° ê²½ê³„: {len(cluster_boundaries)}ê°œ")
    
    return {
        'significant_changes': significant_changes,
        'performance_levels': performance_levels,
        'stability_changes': stability_changes,
        'cluster_boundaries': cluster_boundaries,
        'kmeans_model': kmeans,
        'scaler': scaler
    }

def determine_optimal_segmentation(stats_df, transitions):
    """ìµœì  êµ¬ê°„ ë¶„í•  ê²°ì •"""
    print("ğŸ“Š ìµœì  êµ¬ê°„ ë¶„í•  ê²°ì • ì¤‘...")
    
    total_length = len(stats_df)
    
    # ë°©ë²• 1: ì„±ëŠ¥ ë³€í™”ìœ¨ ê¸°ë°˜ ë¶„í• 
    method1_boundaries = []
    
    # ì´ˆê¸° êµ¬ê°„: ë¹ ë¥¸ ì„±ëŠ¥ ë³€í™” êµ¬ê°„ (ë†’ì€ ë³€í™”ìœ¨)
    high_change_period = stats_df[stats_df['performance_change_abs'] > 0.01]  # 1% ì´ìƒ ë³€í™”
    if len(high_change_period) > 0:
        initial_end = high_change_period.index[-1]
        method1_boundaries.append(initial_end)
    
    # ì¤‘ê¸° êµ¬ê°„: ì•ˆì •í™” ì§„í–‰ êµ¬ê°„ (ì¤‘ê°„ ì•ˆì •ì„±)
    remaining_data = stats_df.iloc[method1_boundaries[0]:] if method1_boundaries else stats_df
    stable_threshold = remaining_data['performance_stability'].quantile(0.3)  # í•˜ìœ„ 30% ì•ˆì •ì„±
    stable_period = remaining_data[remaining_data['performance_stability'] < stable_threshold]
    
    if len(stable_period) > 0:
        middle_end = stable_period.index[-1]
        method1_boundaries.append(middle_end)
    
    # ë°©ë²• 2: í´ëŸ¬ìŠ¤í„° ê¸°ë°˜ ë¶„í• 
    method2_boundaries = transitions['cluster_boundaries'][:2]  # ì²˜ìŒ 2ê°œ ê²½ê³„ë§Œ ì‚¬ìš©
    
    # ë°©ë²• 3: ì„±ëŠ¥ ìˆ˜ì¤€ ê¸°ë°˜ ë¶„í• 
    method3_boundaries = []
    
    # ì„±ëŠ¥ ìˆ˜ì¤€ë³„ ë¶„í• 
    performance_quantiles = stats_df['write_rate_smooth'].quantile([0.33, 0.67])
    
    # ì²« ë²ˆì§¸ ê²½ê³„: ì„±ëŠ¥ì´ 67% ë¶„ìœ„ìˆ˜ ì•„ë˜ë¡œ ë–¨ì–´ì§€ëŠ” ì§€ì 
    high_to_mid = stats_df[stats_df['write_rate_smooth'] < performance_quantiles[0.67]].index
    if len(high_to_mid) > 0:
        method3_boundaries.append(high_to_mid[0])
    
    # ë‘ ë²ˆì§¸ ê²½ê³„: ì„±ëŠ¥ì´ 33% ë¶„ìœ„ìˆ˜ ì•„ë˜ë¡œ ë–¨ì–´ì§€ëŠ” ì§€ì 
    mid_to_low = stats_df[stats_df['write_rate_smooth'] < performance_quantiles[0.33]].index
    if len(mid_to_low) > 0:
        method3_boundaries.append(mid_to_low[0])
    
    # ìµœì  ë¶„í•  ì„ íƒ (ì—¬ëŸ¬ ë°©ë²•ì˜ ê²°ê³¼ë¥¼ ì¢…í•©)
    all_boundaries = []
    if method1_boundaries:
        all_boundaries.extend(method1_boundaries)
    if method2_boundaries:
        all_boundaries.extend(method2_boundaries)
    if method3_boundaries:
        all_boundaries.extend(method3_boundaries)
    
    # ê²½ê³„ì ë“¤ì„ ì •ë ¬í•˜ê³  ì¤‘ë³µ ì œê±°
    all_boundaries = sorted(list(set(all_boundaries)))
    
    # ë„ˆë¬´ ê°€ê¹Œìš´ ê²½ê³„ì ë“¤ ì œê±° (ì „ì²´ ê¸¸ì´ì˜ 10% ì´ë‚´)
    min_distance = total_length * 0.1
    filtered_boundaries = []
    
    for boundary in all_boundaries:
        if not filtered_boundaries or boundary - filtered_boundaries[-1] > min_distance:
            filtered_boundaries.append(boundary)
    
    # ìµœëŒ€ 2ê°œì˜ ê²½ê³„ì ë§Œ ì„ íƒ (3êµ¬ê°„)
    if len(filtered_boundaries) > 2:
        # ì„±ëŠ¥ ë³€í™”ê°€ ê°€ì¥ í° ì§€ì ë“¤ ì„ íƒ
        boundary_scores = []
        for boundary in filtered_boundaries:
            if boundary > 0 and boundary < total_length - 1:
                before_perf = stats_df.iloc[max(0, boundary-50):boundary]['write_rate_smooth'].mean()
                after_perf = stats_df.iloc[boundary:min(total_length, boundary+50)]['write_rate_smooth'].mean()
                score = abs(before_perf - after_perf)
                boundary_scores.append((boundary, score))
        
        # ì ìˆ˜ê°€ ë†’ì€ ìƒìœ„ 2ê°œ ì„ íƒ
        boundary_scores.sort(key=lambda x: x[1], reverse=True)
        filtered_boundaries = sorted([bs[0] for bs in boundary_scores[:2]])
    
    # ìµœì¢… êµ¬ê°„ ì •ì˜
    if len(filtered_boundaries) >= 2:
        boundaries = filtered_boundaries[:2]
    elif len(filtered_boundaries) == 1:
        # í•˜ë‚˜ë§Œ ìˆìœ¼ë©´ ì¤‘ê°„ ì§€ì ì— í•˜ë‚˜ ë” ì¶”ê°€
        boundaries = [filtered_boundaries[0], int(total_length * 0.75)]
    else:
        # ê²½ê³„ë¥¼ ì°¾ì§€ ëª»í•˜ë©´ ê¸°ë³¸ ë¶„í• 
        boundaries = [int(total_length * 0.4), int(total_length * 0.75)]
    
    # êµ¬ê°„ ì •ì˜
    segments = {
        'initial': {
            'start_idx': 0,
            'end_idx': boundaries[0],
            'description': 'ë¹ˆ DBì—ì„œ ë¹ ë¥´ê²Œ ì„±ëŠ¥ì´ ë³€í•˜ëŠ” êµ¬ê°„'
        },
        'middle': {
            'start_idx': boundaries[0],
            'end_idx': boundaries[1],
            'description': 'ì»´íŒ©ì…˜ì´ ì§„í–‰ë˜ë©° ì•ˆì •í™”ë˜ì–´ ê°€ëŠ” êµ¬ê°„'
        },
        'final': {
            'start_idx': boundaries[1],
            'end_idx': total_length - 1,
            'description': 'ì•ˆì •í™” êµ¬ê°„'
        }
    }
    
    print(f"âœ… ìµœì  êµ¬ê°„ ë¶„í•  ê²°ì • ì™„ë£Œ:")
    for segment_name, segment_info in segments.items():
        start_time = stats_df.iloc[segment_info['start_idx']]['datetime']
        end_time = stats_df.iloc[segment_info['end_idx']]['datetime']
        duration = (end_time - start_time).total_seconds() / 3600
        print(f"   - {segment_name.title()} êµ¬ê°„: {segment_info['start_idx']} ~ {segment_info['end_idx']} ({duration:.1f}ì‹œê°„)")
        print(f"     ì„¤ëª…: {segment_info['description']}")
    
    return segments

def analyze_segment_characteristics(stats_df, segments):
    """êµ¬ê°„ë³„ íŠ¹ì„± ë¶„ì„"""
    print("ğŸ“Š êµ¬ê°„ë³„ íŠ¹ì„± ë¶„ì„ ì¤‘...")
    
    segment_analysis = {}
    
    for segment_name, segment_info in segments.items():
        start_idx = segment_info['start_idx']
        end_idx = segment_info['end_idx']
        segment_data = stats_df.iloc[start_idx:end_idx+1]
        
        if len(segment_data) == 0:
            continue
            
        # ê¸°ë³¸ í†µê³„
        basic_stats = {
            'duration_hours': (segment_data['datetime'].iloc[-1] - segment_data['datetime'].iloc[0]).total_seconds() / 3600,
            'sample_count': len(segment_data),
            'start_time': segment_data['datetime'].iloc[0],
            'end_time': segment_data['datetime'].iloc[-1],
            'start_idx': start_idx,
            'end_idx': end_idx
        }
        
        # ì„±ëŠ¥ í†µê³„
        performance_stats = {
            'avg_write_rate': segment_data['write_rate'].mean(),
            'max_write_rate': segment_data['write_rate'].max(),
            'min_write_rate': segment_data['write_rate'].min(),
            'std_write_rate': segment_data['write_rate'].std(),
            'cv': segment_data['write_rate'].std() / segment_data['write_rate'].mean() if segment_data['write_rate'].mean() > 0 else 0,
            'median_write_rate': segment_data['write_rate'].median(),
            'q25_write_rate': segment_data['write_rate'].quantile(0.25),
            'q75_write_rate': segment_data['write_rate'].quantile(0.75)
        }
        
        # ì„±ëŠ¥ ë³€í™” ë¶„ì„
        if len(segment_data) > 1:
            performance_trend = {
                'trend_slope': np.polyfit(range(len(segment_data)), segment_data['write_rate'], 1)[0],
                'trend_r2': np.corrcoef(range(len(segment_data)), segment_data['write_rate'])[0,1]**2 if len(segment_data) > 1 else 0,
                'performance_change': (segment_data['write_rate'].iloc[-1] - segment_data['write_rate'].iloc[0]) / segment_data['write_rate'].iloc[0] * 100 if segment_data['write_rate'].iloc[0] > 0 else 0,
                'avg_change_rate': segment_data['performance_change_rate'].mean(),
                'change_rate_volatility': segment_data['performance_change_rate'].std()
            }
        else:
            performance_trend = {
                'trend_slope': 0,
                'trend_r2': 0,
                'performance_change': 0,
                'avg_change_rate': 0,
                'change_rate_volatility': 0
            }
        
        # êµ¬ê°„ë³„ íŠ¹ì„± ë¶„ë¥˜
        characteristics = classify_segment_characteristics(performance_stats, performance_trend)
        
        segment_analysis[segment_name] = {
            'basic_stats': basic_stats,
            'performance_stats': performance_stats,
            'performance_trend': performance_trend,
            'characteristics': characteristics,
            'description': segment_info['description']
        }
    
    print("âœ… êµ¬ê°„ë³„ íŠ¹ì„± ë¶„ì„ ì™„ë£Œ")
    return segment_analysis

def classify_segment_characteristics(performance_stats, performance_trend):
    """êµ¬ê°„ë³„ íŠ¹ì„± ë¶„ë¥˜"""
    characteristics = {}
    
    # ì„±ëŠ¥ ì•ˆì •ì„±
    if performance_stats['cv'] < 0.1:
        characteristics['stability'] = 'high'
    elif performance_stats['cv'] < 0.3:
        characteristics['stability'] = 'medium'
    else:
        characteristics['stability'] = 'low'
    
    # ì„±ëŠ¥ íŠ¸ë Œë“œ
    if performance_trend['trend_slope'] > 0.1:
        characteristics['trend'] = 'increasing'
    elif performance_trend['trend_slope'] < -0.1:
        characteristics['trend'] = 'decreasing'
    else:
        characteristics['trend'] = 'stable'
    
    # ì„±ëŠ¥ ìˆ˜ì¤€
    if performance_stats['avg_write_rate'] > 20:
        characteristics['performance_level'] = 'high'
    elif performance_stats['avg_write_rate'] > 15:
        characteristics['performance_level'] = 'medium'
    else:
        characteristics['performance_level'] = 'low'
    
    # ë³€í™”ìœ¨ íŠ¹ì„±
    if abs(performance_trend['avg_change_rate']) > 0.01:
        characteristics['change_intensity'] = 'high'
    elif abs(performance_trend['avg_change_rate']) > 0.005:
        characteristics['change_intensity'] = 'medium'
    else:
        characteristics['change_intensity'] = 'low'
    
    return characteristics

def create_performance_segmentation_visualization(stats_df, segments, segment_analysis, transitions):
    """ì„±ëŠ¥ ê¸°ë°˜ êµ¬ê°„ ë¶„í•  ì‹œê°í™”"""
    print("ğŸ“Š ì„±ëŠ¥ ê¸°ë°˜ êµ¬ê°„ ë¶„í•  ì‹œê°í™” ìƒì„± ì¤‘...")
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))
    fig.suptitle('Performance-based Segmentation Analysis', fontsize=18, fontweight='bold')
    
    # ì‹œê°„ ì •ê·œí™” (ì‹œê°„ ë‹¨ìœ„)
    time_hours = stats_df['time_seconds'] / 3600
    
    # êµ¬ê°„ë³„ ìƒ‰ìƒ
    colors = {'initial': '#FF6B6B', 'middle': '#4ECDC4', 'final': '#45B7D1'}
    
    # 1. ì„±ëŠ¥ ì¶”ì´ì™€ êµ¬ê°„ ë¶„í• 
    ax1.plot(time_hours, stats_df['write_rate'], alpha=0.3, color='gray', label='Raw Performance')
    ax1.plot(time_hours, stats_df['write_rate_smooth'], color='black', linewidth=2, label='Smoothed Performance')
    
    # êµ¬ê°„ë³„ ìƒ‰ì¹ 
    for segment_name, segment_info in segments.items():
        start_idx = segment_info['start_idx']
        end_idx = segment_info['end_idx']
        segment_time = time_hours.iloc[start_idx:end_idx+1]
        segment_perf = stats_df['write_rate_smooth'].iloc[start_idx:end_idx+1]
        
        ax1.fill_between(segment_time, 0, segment_perf, alpha=0.3, color=colors[segment_name], 
                        label=f'{segment_name.title()} Phase')
    
    # êµ¬ê°„ ê²½ê³„ì„ 
    for segment_name, segment_info in segments.items():
        if segment_name != 'final':  # ë§ˆì§€ë§‰ êµ¬ê°„ ì œì™¸
            boundary_time = time_hours.iloc[segment_info['end_idx']]
            ax1.axvline(x=boundary_time, color='red', linestyle='--', alpha=0.7)
    
    ax1.set_xlabel('Time (hours)')
    ax1.set_ylabel('Write Rate (MB/s)')
    ax1.set_title('Performance-based Phase Segmentation')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. ì„±ëŠ¥ ë³€í™”ìœ¨
    ax2.plot(time_hours, stats_df['performance_change_rate'], alpha=0.5, color='blue', label='Change Rate')
    ax2.plot(time_hours, stats_df['change_rate_trend'], color='darkblue', linewidth=2, label='Change Rate Trend')
    ax2.axhline(y=0, color='black', linestyle='-', alpha=0.5)
    ax2.axhline(y=0.02, color='red', linestyle='--', alpha=0.5, label='Change Threshold')
    ax2.axhline(y=-0.02, color='red', linestyle='--', alpha=0.5)
    
    ax2.set_xlabel('Time (hours)')
    ax2.set_ylabel('Performance Change Rate')
    ax2.set_title('Performance Change Rate Analysis')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. êµ¬ê°„ë³„ íŠ¹ì„± ë¹„êµ
    segment_names = list(segment_analysis.keys())
    avg_rates = [segment_analysis[seg]['performance_stats']['avg_write_rate'] for seg in segment_names]
    cv_values = [segment_analysis[seg]['performance_stats']['cv'] for seg in segment_names]
    change_rates = [abs(segment_analysis[seg]['performance_trend']['avg_change_rate']) for seg in segment_names]
    
    x = np.arange(len(segment_names))
    width = 0.25
    
    ax3_twin = ax3.twinx()
    
    bars1 = ax3.bar(x - width, avg_rates, width, label='Avg Performance (MB/s)', color='skyblue', alpha=0.8)
    bars2 = ax3_twin.bar(x, cv_values, width, label='CV', color='lightcoral', alpha=0.8)
    bars3 = ax3_twin.bar(x + width, change_rates, width, label='Avg Change Rate', color='lightgreen', alpha=0.8)
    
    ax3.set_xlabel('Phase')
    ax3.set_ylabel('Average Performance (MB/s)', color='blue')
    ax3_twin.set_ylabel('CV / Change Rate', color='red')
    ax3.set_title('Phase Characteristics Comparison')
    ax3.set_xticks(x)
    ax3.set_xticklabels([seg.title() for seg in segment_names])
    
    # ë²”ë¡€ í†µí•©
    lines1, labels1 = ax3.get_legend_handles_labels()
    lines2, labels2 = ax3_twin.get_legend_handles_labels()
    ax3.legend(lines1 + lines2, labels1 + labels2, loc='upper left')
    
    ax3.grid(True, alpha=0.3)
    
    # 4. êµ¬ê°„ë³„ ìš”ì•½
    ax4.text(0.05, 0.95, 'Performance-based Segmentation Summary', fontsize=16, fontweight='bold', transform=ax4.transAxes)
    
    y_pos = 0.85
    for segment_name, analysis in segment_analysis.items():
        ax4.text(0.05, y_pos, f'{segment_name.title()} Phase:', fontsize=14, fontweight='bold', transform=ax4.transAxes, color=colors[segment_name])
        y_pos -= 0.05
        
        basic = analysis['basic_stats']
        perf = analysis['performance_stats']
        char = analysis['characteristics']
        
        ax4.text(0.05, y_pos, f'  Duration: {basic["duration_hours"]:.1f} hours ({basic["sample_count"]:,} samples)', fontsize=11, transform=ax4.transAxes)
        y_pos -= 0.04
        ax4.text(0.05, y_pos, f'  Avg Performance: {perf["avg_write_rate"]:.1f} MB/s (CV: {perf["cv"]:.3f})', fontsize=11, transform=ax4.transAxes)
        y_pos -= 0.04
        ax4.text(0.05, y_pos, f'  Characteristics: {char["stability"]} stability, {char["trend"]} trend', fontsize=11, transform=ax4.transAxes)
        y_pos -= 0.04
        ax4.text(0.05, y_pos, f'  Description: {analysis["description"]}', fontsize=10, transform=ax4.transAxes, style='italic')
        y_pos -= 0.06
    
    ax4.set_xlim(0, 1)
    ax4.set_ylim(0, 1)
    ax4.axis('off')
    
    plt.tight_layout()
    plt.savefig('performance_based_segmentation_analysis.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print("âœ… ì„±ëŠ¥ ê¸°ë°˜ êµ¬ê°„ ë¶„í•  ì‹œê°í™” ì™„ë£Œ")

def save_results(stats_df, segments, segment_analysis, transitions):
    """ê²°ê³¼ ì €ì¥"""
    print("ğŸ’¾ ì„±ëŠ¥ ê¸°ë°˜ êµ¬ê°„ ë¶„í•  ê²°ê³¼ ì €ì¥ ì¤‘...")
    
    # JSON ê²°ê³¼ ì €ì¥
    results = {
        'segmentation_method': 'performance_based',
        'segments': segments,
        'segment_analysis': segment_analysis,
        'transition_points': {
            'cluster_boundaries': transitions['cluster_boundaries'],
            'performance_levels': transitions['performance_levels'],
            'stability_changes': transitions['stability_changes']
        },
        'analysis_time': datetime.now().isoformat()
    }
    
    with open('performance_based_segmentation_results.json', 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    # Markdown ë³´ê³ ì„œ ìƒì„±
    with open('performance_based_segmentation_report.md', 'w') as f:
        f.write("# Performance-based Segmentation Analysis Report\n\n")
        f.write(f"## Analysis Time\n{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write("## Overview\n")
        f.write("This report presents the results of performance-based segmentation analysis using RocksDB LOG data.\n")
        f.write("Unlike time-based segmentation, this approach identifies phases based on actual performance change patterns.\n\n")
        
        f.write("## Segmentation Method\n")
        f.write("The segmentation is based on:\n")
        f.write("1. **Performance Change Rate Analysis**: Detecting significant performance transitions\n")
        f.write("2. **Performance Level Changes**: Identifying major performance drops\n")
        f.write("3. **Stability Analysis**: Detecting changes in performance variability\n")
        f.write("4. **Clustering Analysis**: Using K-means clustering on performance characteristics\n\n")
        
        f.write("## Phase Definitions\n")
        for segment_name, analysis in segment_analysis.items():
            f.write(f"### {segment_name.title()} Phase\n")
            f.write(f"**Description**: {analysis['description']}\n\n")
            
            basic = analysis['basic_stats']
            f.write(f"**Timing**:\n")
            f.write(f"- Start: {basic['start_time']}\n")
            f.write(f"- End: {basic['end_time']}\n")
            f.write(f"- Duration: {basic['duration_hours']:.1f} hours\n")
            f.write(f"- Sample Count: {basic['sample_count']:,}\n\n")
            
            perf = analysis['performance_stats']
            f.write(f"**Performance Statistics**:\n")
            f.write(f"- Average Rate: {perf['avg_write_rate']:.1f} MB/s\n")
            f.write(f"- Performance Range: {perf['min_write_rate']:.1f} - {perf['max_write_rate']:.1f} MB/s\n")
            f.write(f"- Standard Deviation: {perf['std_write_rate']:.1f} MB/s\n")
            f.write(f"- Coefficient of Variation: {perf['cv']:.3f}\n")
            f.write(f"- Median Rate: {perf['median_write_rate']:.1f} MB/s\n\n")
            
            trend = analysis['performance_trend']
            f.write(f"**Performance Trend**:\n")
            f.write(f"- Trend Slope: {trend['trend_slope']:.6f}\n")
            f.write(f"- RÂ² Score: {trend['trend_r2']:.3f}\n")
            f.write(f"- Overall Change: {trend['performance_change']:.1f}%\n")
            f.write(f"- Average Change Rate: {trend['avg_change_rate']:.6f}\n")
            f.write(f"- Change Rate Volatility: {trend['change_rate_volatility']:.6f}\n\n")
            
            char = analysis['characteristics']
            f.write(f"**Phase Characteristics**:\n")
            f.write(f"- Stability: {char['stability']}\n")
            f.write(f"- Trend: {char['trend']}\n")
            f.write(f"- Performance Level: {char['performance_level']}\n")
            f.write(f"- Change Intensity: {char['change_intensity']}\n\n")
            
            f.write("---\n\n")
        
        f.write("## Key Insights\n\n")
        f.write("### Performance-based vs Time-based Segmentation\n")
        f.write("- **Performance-based**: Segments based on actual performance change patterns\n")
        f.write("- **Advantage**: More meaningful phase boundaries that reflect actual system behavior\n")
        f.write("- **Result**: Variable phase durations based on performance characteristics\n\n")
        
        f.write("### Phase Progression Pattern\n")
        for segment_name, analysis in segment_analysis.items():
            char = analysis['characteristics']
            perf = analysis['performance_stats']
            f.write(f"- **{segment_name.title()}**: {char['performance_level']} performance, {char['stability']} stability, {char['trend']} trend (Avg: {perf['avg_write_rate']:.1f} MB/s)\n")
        
        f.write(f"\n## Analysis Time\n{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
    
    print("âœ… ì„±ëŠ¥ ê¸°ë°˜ êµ¬ê°„ ë¶„í•  ê²°ê³¼ ì €ì¥ ì™„ë£Œ")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ ì„±ëŠ¥ ë³€í™” ê¸°ë°˜ êµ¬ê°„ ë¶„í•  ë¶„ì„ ì‹œì‘...")
    
    # LOG íŒŒì¼ ê²½ë¡œ ì„¤ì •
    main_log = "rocksdb_log_phase_b.log"
    
    if not os.path.exists(main_log):
        print("âŒ LOG íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    print(f"ğŸ“– ë©”ì¸ LOG íŒŒì¼: {main_log}")
    
    # LOG íŒŒì¼ ë¶„ì„
    stats_data = parse_log_file(main_log)
    
    if not stats_data:
        print("âŒ Stats ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë°ì´í„°í”„ë ˆì„ ìƒì„±
    stats_df = pd.DataFrame([s for s in stats_data if s])
    stats_df['datetime'] = pd.to_datetime(stats_df['timestamp'])
    stats_df = stats_df.sort_values('datetime').reset_index(drop=True)
    
    print(f"ğŸ“Š ì´ ë°ì´í„° í¬ì¸íŠ¸: {len(stats_df)}ê°œ")
    
    # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
    stats_df = calculate_performance_metrics(stats_df)
    
    # ì„±ëŠ¥ ì „í™˜ì  íƒì§€
    transitions = detect_performance_transitions(stats_df)
    
    # ìµœì  êµ¬ê°„ ë¶„í•  ê²°ì •
    segments = determine_optimal_segmentation(stats_df, transitions)
    
    # êµ¬ê°„ë³„ íŠ¹ì„± ë¶„ì„
    segment_analysis = analyze_segment_characteristics(stats_df, segments)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\nğŸ“Š ì„±ëŠ¥ ê¸°ë°˜ êµ¬ê°„ ë¶„í•  ê²°ê³¼:")
    for segment_name, analysis in segment_analysis.items():
        print(f"\n  {segment_name.title()} Phase:")
        basic = analysis['basic_stats']
        perf = analysis['performance_stats']
        char = analysis['characteristics']
        
        print(f"    ê¸°ê°„: {basic['start_time']} ~ {basic['end_time']}")
        print(f"    ì§€ì†ì‹œê°„: {basic['duration_hours']:.1f} ì‹œê°„")
        print(f"    ìƒ˜í”Œ ìˆ˜: {basic['sample_count']:,}ê°œ")
        print(f"    í‰ê·  ì„±ëŠ¥: {perf['avg_write_rate']:.1f} MB/s")
        print(f"    ì„±ëŠ¥ ë²”ìœ„: {perf['min_write_rate']:.1f} ~ {perf['max_write_rate']:.1f} MB/s")
        print(f"    ë³€ë™ê³„ìˆ˜: {perf['cv']:.3f}")
        print(f"    ì•ˆì •ì„±: {char['stability']}")
        print(f"    íŠ¸ë Œë“œ: {char['trend']}")
        print(f"    ì„±ëŠ¥ ìˆ˜ì¤€: {char['performance_level']}")
        print(f"    ë³€í™” ê°•ë„: {char['change_intensity']}")
        print(f"    ì„¤ëª…: {analysis['description']}")
    
    # ì‹œê°í™” ìƒì„±
    create_performance_segmentation_visualization(stats_df, segments, segment_analysis, transitions)
    
    # ê²°ê³¼ ì €ì¥
    save_results(stats_df, segments, segment_analysis, transitions)
    
    print("\nâœ… ì„±ëŠ¥ ë³€í™” ê¸°ë°˜ êµ¬ê°„ ë¶„í•  ë¶„ì„ ì™„ë£Œ!")
    print("ğŸ“Š ê²°ê³¼ íŒŒì¼: performance_based_segmentation_analysis.png, performance_based_segmentation_results.json, performance_based_segmentation_report.md")

if __name__ == "__main__":
    main()

