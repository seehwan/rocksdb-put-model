#!/usr/bin/env python3
"""
Phase-B ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
FillRandom LOG íŒŒì¼ ë¶„ì„ ë° ì‹œê°„ë³„ ì„±ëŠ¥ ë³€í™” ì‹œê°í™”
"""

import json
import re
import matplotlib.pyplot as plt
import pandas as pd
from pathlib import Path
from datetime import datetime
import numpy as np

# Liberation Serif í°íŠ¸ ì„¤ì • (Times ìŠ¤íƒ€ì¼)
plt.rcParams['font.family'] = 'Liberation Serif'
plt.rcParams['axes.unicode_minus'] = False

def parse_log_file(log_file):
    """LOG íŒŒì¼ íŒŒì‹±"""
    print(f"ğŸ“– LOG íŒŒì¼ ë¶„ì„ ì¤‘: {log_file}")
    
    stats_data = []
    compaction_data = []
    current_timestamp = None
    
    with open(log_file, 'r') as f:
        for line in f:
            line = line.strip()
            
            # ì‹œê°„ ì •ë³´ ì¶”ì¶œ (DUMPING STATS ë¼ì¸ ë°”ë¡œ ìœ„)
            if "------- DUMPING STATS -------" in line:
                # ì´ì „ ë¼ì¸ì—ì„œ ì‹œê°„ ì •ë³´ ì¶”ì¶œ
                continue
            
            # ì‹œê°„ ì •ë³´ê°€ ìˆëŠ” ë¼ì¸ì—ì„œ timestamp ì¶”ì¶œ
            time_match = re.search(r'(\d{4}/\d{2}/\d{2}-\d{2}:\d{2}:\d{2}\.\d+)', line)
            if time_match:
                current_timestamp = time_match.group(1)
            
            # Stats ë¡œê·¸ íŒŒì‹± (ì‹œê°„ ì •ë³´ì™€ í•¨ê»˜)
            if "Cumulative writes:" in line:
                stats_data.append(parse_stats_line(line, current_timestamp))
            
            # Compaction ë¡œê·¸ íŒŒì‹±
            elif "Compaction start" in line or "Compaction finished" in line:
                compaction_data.append(parse_compaction_line(line))
    
    return stats_data, compaction_data

def parse_stats_line(line, timestamp=None):
    """Stats ë¼ì¸ íŒŒì‹±"""
    try:
        # Cumulative writes ì¶”ì¶œ (K ë‹¨ìœ„ ì²˜ë¦¬)
        writes_match = re.search(r'Cumulative writes: (\d+)K writes', line)
        if writes_match:
            writes = int(writes_match.group(1)) * 1000  # Kë¥¼ 1000ìœ¼ë¡œ ë³€í™˜
        else:
            writes_match = re.search(r'Cumulative writes: (\d+) writes', line)
            writes = int(writes_match.group(1)) if writes_match else 0
        
        # Write rate ì¶”ì¶œ (MB/s ë‹¨ìœ„)
        rate_match = re.search(r'(\d+\.?\d*) MB/s', line)
        write_rate = float(rate_match.group(1)) if rate_match else 0
        
        # Ingest ì¶”ì¶œ (GB ë‹¨ìœ„)
        ingest_match = re.search(r'ingest: (\d+\.?\d*) GB', line)
        ingest_gb = float(ingest_match.group(1)) if ingest_match else 0
        
        return {
            'timestamp': timestamp,
            'cumulative_writes': writes,
            'write_rate': write_rate,
            'ingest_gb': ingest_gb
        }
    except Exception as e:
        print(f"Stats ë¼ì¸ íŒŒì‹± ì˜¤ë¥˜: {e}")
        return None

def parse_compaction_line(line):
    """Compaction ë¼ì¸ íŒŒì‹±"""
    try:
        # ì‹œê°„ ì¶”ì¶œ (ë§ˆì´í¬ë¡œì´ˆ í¬í•¨)
        time_match = re.search(r'(\d{4}/\d{2}/\d{2}-\d{2}:\d{2}:\d{2}\.\d+)', line)
        timestamp = time_match.group(1) if time_match else None
        
        # Compaction íƒ€ì… ì¶”ì¶œ
        compaction_type = "start" if "Compaction start" in line else "finish"
        
        # ë ˆë²¨ ì •ë³´ ì¶”ì¶œ
        level_match = re.search(r'level-(\d+)', line)
        level = int(level_match.group(1)) if level_match else -1
        
        return {
            'timestamp': timestamp,
            'type': compaction_type,
            'level': level
        }
    except Exception as e:
        print(f"Compaction ë¼ì¸ íŒŒì‹± ì˜¤ë¥˜: {e}")
        return None

def analyze_performance_trends(stats_data):
    """ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„"""
    print("\n=== ğŸ“Š ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„ ===")
    
    if not stats_data:
        print("âŒ Stats ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return None
    
    # ë°ì´í„°í”„ë ˆì„ ìƒì„±
    df = pd.DataFrame([d for d in stats_data if d is not None])
    
    if df.empty:
        print("âŒ ìœ íš¨í•œ Stats ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return None
    
    # ì‹œê°„ ë³€í™˜ (ë§ˆì´í¬ë¡œì´ˆ í¬í•¨)
    df['datetime'] = pd.to_datetime(df['timestamp'], format='%Y/%m/%d-%H:%M:%S.%f')
    
    # ì„±ëŠ¥ ë¶„ì„
    initial_rate = df['write_rate'].iloc[0] if len(df) > 0 else 0
    final_rate = df['write_rate'].iloc[-1] if len(df) > 0 else 0
    max_rate = df['write_rate'].max()
    min_rate = df['write_rate'].min()
    
    print(f"ğŸ“ˆ ì„±ëŠ¥ ì§€í‘œ:")
    print(f"  ì´ˆê¸° Put Rate: {initial_rate:.0f} ops/sec")
    print(f"  ìµœëŒ€ Put Rate: {max_rate:.0f} ops/sec")
    print(f"  ìµœì†Œ Put Rate: {min_rate:.0f} ops/sec")
    print(f"  ìµœì¢… Put Rate: {final_rate:.0f} ops/sec")
    print(f"  ì„±ëŠ¥ ì €í•˜ìœ¨: {((initial_rate - final_rate) / initial_rate * 100):.1f}%")
    
    # ì•ˆì •í™” ë¶„ì„
    last_10_percent = df.tail(int(len(df) * 0.1))
    if len(last_10_percent) > 1:
        stability_std = last_10_percent['write_rate'].std()
        stability_mean = last_10_percent['write_rate'].mean()
        stability_cv = stability_std / stability_mean * 100 if stability_mean > 0 else 0
        
        print(f"  ì•ˆì •í™” êµ¬ê°„ ë³€ë™ê³„ìˆ˜: {stability_cv:.1f}%")
        if stability_cv < 10:
            print("  âœ… ì•ˆì •í™” ë‹¬ì„±")
        else:
            print("  âš ï¸ ì•ˆì •í™” ë¯¸ë‹¬ì„±")
    
    return df

def analyze_compaction_patterns(compaction_data):
    """Compaction íŒ¨í„´ ë¶„ì„"""
    print("\n=== ğŸ“Š Compaction íŒ¨í„´ ë¶„ì„ ===")
    
    if not compaction_data:
        print("âŒ Compaction ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return None
    
    # ë°ì´í„°í”„ë ˆì„ ìƒì„±
    df = pd.DataFrame([d for d in compaction_data if d is not None])
    
    if df.empty:
        print("âŒ ìœ íš¨í•œ Compaction ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return None
    
    # ì‹œê°„ ë³€í™˜ (ë§ˆì´í¬ë¡œì´ˆ í¬í•¨)
    df['datetime'] = pd.to_datetime(df['timestamp'], format='%Y/%m/%d-%H:%M:%S.%f')
    
    # ë ˆë²¨ë³„ Compaction í†µê³„
    level_stats = df.groupby('level').size()
    print(f"ğŸ“ˆ ë ˆë²¨ë³„ Compaction íšŸìˆ˜:")
    for level, count in level_stats.items():
        print(f"  Level {level}: {count}íšŒ")
    
    # ì‹œê°„ë³„ Compaction ë¹ˆë„
    df['hour'] = df['datetime'].dt.floor('H')
    hourly_compactions = df.groupby('hour').size()
    
    print(f"ğŸ“ˆ ì‹œê°„ë³„ Compaction ë¹ˆë„:")
    for hour, count in hourly_compactions.items():
        print(f"  {hour.strftime('%H:%M')}: {count}íšŒ")
    
    return df

def create_visualizations(stats_df, compaction_df):
    """ì‹œê°í™” ìƒì„±"""
    print("\n=== ğŸ“Š ì‹œê°í™” ìƒì„± ===")
    
    try:
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. Put Rate ì‹œê°„ë³„ ë³€í™”
        if stats_df is not None and not stats_df.empty:
            ax1.plot(stats_df['datetime'], stats_df['write_rate'], 'b-', linewidth=2)
            ax1.set_title('Put Rate Over Time', fontsize=14, fontweight='bold')
            ax1.set_xlabel('Time')
            ax1.set_ylabel('Put Rate (ops/sec)')
            ax1.grid(True, alpha=0.3)
            ax1.tick_params(axis='x', rotation=45)
            
            # ì´ë™í‰ê·  ì¶”ê°€
            stats_df['ma_10'] = stats_df['write_rate'].rolling(window=10).mean()
            ax1.plot(stats_df['datetime'], stats_df['ma_10'], 'r--', alpha=0.7, label='10-point MA')
            ax1.legend()
        
        # 2. Cumulative Writes
        if stats_df is not None and not stats_df.empty:
            ax2.plot(stats_df['datetime'], stats_df['cumulative_writes'], 'g-', linewidth=2)
            ax2.set_title('Cumulative Writes Over Time', fontsize=14, fontweight='bold')
            ax2.set_xlabel('Time')
            ax2.set_ylabel('Cumulative Writes')
            ax2.grid(True, alpha=0.3)
            ax2.tick_params(axis='x', rotation=45)
        
        # 3. Compaction ë¹ˆë„
        if compaction_df is not None and not compaction_df.empty:
            compaction_df['hour'] = compaction_df['datetime'].dt.floor('H')
            hourly_compactions = compaction_df.groupby('hour').size()
            
            ax3.bar(range(len(hourly_compactions)), hourly_compactions.values, color='orange', alpha=0.7)
            ax3.set_title('Compaction Frequency by Hour', fontsize=14, fontweight='bold')
            ax3.set_xlabel('Hour')
            ax3.set_ylabel('Compaction Count')
            ax3.set_xticks(range(len(hourly_compactions)))
            ax3.set_xticklabels([h.strftime('%H:%M') for h in hourly_compactions.index], rotation=45)
            ax3.grid(True, alpha=0.3)
        
        # 4. ë ˆë²¨ë³„ Compaction ë¶„í¬
        if compaction_df is not None and not compaction_df.empty:
            level_counts = compaction_df['level'].value_counts().sort_index()
            colors = plt.cm.Set3(np.linspace(0, 1, len(level_counts)))
            
            wedges, texts, autotexts = ax4.pie(level_counts.values, labels=[f'Level {l}' for l in level_counts.index], 
                                               autopct='%1.1f%%', colors=colors)
            ax4.set_title('Compaction Distribution by Level', fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig('phase_b_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print("âœ… ì‹œê°í™” ì €ì¥ ì™„ë£Œ: phase_b_analysis.png")
        
    except Exception as e:
        print(f"âŒ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")

def generate_summary_report(stats_df, compaction_df):
    """ì¢…í•© ìš”ì•½ ë³´ê³ ì„œ ìƒì„±"""
    print("\n=== ğŸ“‹ ì¢…í•© ìš”ì•½ ë³´ê³ ì„œ ìƒì„± ===")
    
    summary = {
        "experiment_type": "Phase-B FillRandom Analysis",
        "analysis_date": datetime.now().isoformat(),
        "performance_summary": {},
        "compaction_summary": {},
        "stability_analysis": {}
    }
    
    if stats_df is not None and not stats_df.empty:
        summary["performance_summary"] = {
            "initial_put_rate": float(stats_df['write_rate'].iloc[0]),
            "final_put_rate": float(stats_df['write_rate'].iloc[-1]),
            "max_put_rate": float(stats_df['write_rate'].max()),
            "min_put_rate": float(stats_df['write_rate'].min()),
            "performance_degradation_percent": float(((stats_df['write_rate'].iloc[0] - stats_df['write_rate'].iloc[-1]) / stats_df['write_rate'].iloc[0] * 100)),
            "total_writes": int(stats_df['cumulative_writes'].iloc[-1]),
            "experiment_duration_minutes": float((stats_df['datetime'].iloc[-1] - stats_df['datetime'].iloc[0]).total_seconds() / 60)
        }
        
        # ì•ˆì •í™” ë¶„ì„
        last_10_percent = stats_df.tail(int(len(stats_df) * 0.1))
        if len(last_10_percent) > 1:
            stability_std = last_10_percent['write_rate'].std()
            stability_mean = last_10_percent['write_rate'].mean()
            stability_cv = stability_std / stability_mean * 100 if stability_mean > 0 else 0
            
            summary["stability_analysis"] = {
                "stability_coefficient_of_variation": float(stability_cv),
                "is_stable": stability_cv < 10,
                "final_stability_mean": float(stability_mean),
                "final_stability_std": float(stability_std)
            }
    
    if compaction_df is not None and not compaction_df.empty:
        level_counts = compaction_df['level'].value_counts().to_dict()
        summary["compaction_summary"] = {
            "total_compactions": len(compaction_df),
            "compactions_by_level": {f"level_{k}": v for k, v in level_counts.items()},
            "most_active_level": int(compaction_df['level'].mode().iloc[0]) if len(compaction_df['level'].mode()) > 0 else -1
        }
    
    # ìš”ì•½ ì €ì¥
    # JSON ì €ì¥ (numpy íƒ€ì… ë³€í™˜)
    def convert_numpy_types(obj):
        if isinstance(obj, np.bool_):
            return bool(obj)
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        return obj
    
    # ì¬ê·€ì ìœ¼ë¡œ numpy íƒ€ì… ë³€í™˜
    def clean_dict(d):
        if isinstance(d, dict):
            return {k: clean_dict(v) for k, v in d.items()}
        elif isinstance(d, list):
            return [clean_dict(item) for item in d]
        else:
            return convert_numpy_types(d)
    
    summary_clean = clean_dict(summary)
    
    with open('phase_b_summary.json', 'w') as f:
        json.dump(summary_clean, f, indent=2)
    
    print("âœ… ì¢…í•© ìš”ì•½ ë³´ê³ ì„œ ì €ì¥: phase_b_summary.json")
    
    # ì½˜ì†”ì— ìš”ì•½ ì¶œë ¥
    if stats_df is not None and not stats_df.empty:
        print("\nğŸ¯ **Phase-B FillRandom ë¶„ì„ ìš”ì•½**")
        print(f"ğŸ“… ë¶„ì„ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"\nğŸ“Š ì„±ëŠ¥ ìš”ì•½:")
        print(f"  ì´ˆê¸° Put Rate: {stats_df['write_rate'].iloc[0]:.0f} ops/sec")
        print(f"  ìµœì¢… Put Rate: {stats_df['write_rate'].iloc[-1]:.0f} ops/sec")
        print(f"  ì„±ëŠ¥ ì €í•˜ìœ¨: {((stats_df['write_rate'].iloc[0] - stats_df['write_rate'].iloc[-1]) / stats_df['write_rate'].iloc[0] * 100):.1f}%")
        print(f"  ì´ Write ìˆ˜: {stats_df['cumulative_writes'].iloc[-1]:,}")
        print(f"  ì‹¤í—˜ ì§€ì†ì‹œê°„: {(stats_df['datetime'].iloc[-1] - stats_df['datetime'].iloc[0]).total_seconds() / 60:.1f}ë¶„")
        
        if 'stability_analysis' in summary and summary['stability_analysis']:
            stability = summary['stability_analysis']
            print(f"\nğŸ” ì•ˆì •í™” ë¶„ì„:")
            print(f"  ë³€ë™ê³„ìˆ˜: {stability['stability_coefficient_of_variation']:.1f}%")
            print(f"  ì•ˆì •í™” ë‹¬ì„±: {'âœ… ì˜ˆ' if stability['is_stable'] else 'âŒ ì•„ë‹ˆì˜¤'}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ Phase-B FillRandom ë¶„ì„ ì‹œì‘...")
    
    # LOG íŒŒì¼ ì°¾ê¸° (í˜„ì¬ ë””ë ‰í† ë¦¬ì™€ logs ë””ë ‰í† ë¦¬ì—ì„œ)
    log_files = list(Path('.').glob('LOG*')) + list(Path('logs').glob('LOG*'))
    
    if not log_files:
        print("âŒ LOG íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        print("Phase-B ì‹¤í–‰ í›„ LOG íŒŒì¼ì´ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return
    
    # ê°€ì¥ í° LOG íŒŒì¼ ì„ íƒ (ë©”ì¸ ë¡œê·¸)
    main_log = max(log_files, key=lambda f: f.stat().st_size)
    print(f"ğŸ“– ë©”ì¸ LOG íŒŒì¼: {main_log}")
    
    # LOG íŒŒì¼ ë¶„ì„
    stats_data, compaction_data = parse_log_file(main_log)
    
    # ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„
    stats_df = analyze_performance_trends(stats_data)
    
    # Compaction íŒ¨í„´ ë¶„ì„
    compaction_df = analyze_compaction_patterns(compaction_data)
    
    # ì‹œê°í™” ìƒì„±
    create_visualizations(stats_df, compaction_df)
    
    # ì¢…í•© ìš”ì•½ ë³´ê³ ì„œ ìƒì„±
    generate_summary_report(stats_df, compaction_df)
    
    print("\nâœ… Phase-B FillRandom ë¶„ì„ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
