#!/usr/bin/env python3
"""
Phase-B LOG ê¸°ë°˜ ë¶„ì„
RocksDB LOG íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì •í™•í•œ Phase-B ë¶„ì„
"""

import os
import re
import json
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns

# Liberation Serif í°íŠ¸ ì„¤ì • (Times ìŠ¤íƒ€ì¼)
plt.rcParams['font.family'] = 'Liberation Serif'
plt.rcParams['axes.unicode_minus'] = False

def parse_log_file(log_file):
    """LOG íŒŒì¼ íŒŒì‹±"""
    print(f"ğŸ“– LOG íŒŒì¼ íŒŒì‹± ì¤‘: {log_file}")
    
    stats_data = []
    compaction_data = []
    flush_data = []
    current_timestamp = None
    
    with open(log_file, 'r') as f:
        for line in f:
            line = line.strip()
            
            # ì‹œê°„ ì •ë³´ ì¶”ì¶œ
            time_match = re.search(r'(\d{4}/\d{2}/\d{2}-\d{2}:\d{2}:\d{2}\.\d+)', line)
            if time_match:
                current_timestamp = time_match.group(1)
            
            # Stats ë¡œê·¸ íŒŒì‹±
            if "Cumulative writes:" in line:
                stats_data.append(parse_stats_line(line, current_timestamp))
            
            # Compaction ë¡œê·¸ íŒŒì‹±
            elif "Compaction start" in line:
                compaction_data.append(parse_compaction_start_line(line, current_timestamp))
            elif "compacted to:" in line:
                compaction_data.append(parse_compaction_finish_line(line, current_timestamp))
            
            # Flush ë¡œê·¸ íŒŒì‹±
            elif "Flushing memtable" in line or "Flush lasted" in line:
                flush_data.append(parse_flush_line(line, current_timestamp))
    
    print(f"âœ… íŒŒì‹± ì™„ë£Œ: Stats {len(stats_data)}ê°œ, Compaction {len(compaction_data)}ê°œ, Flush {len(flush_data)}ê°œ")
    return stats_data, compaction_data, flush_data

def parse_stats_line(line, timestamp):
    """Stats ë¼ì¸ íŒŒì‹±"""
    try:
        # cumulative_writes ì¶”ì¶œ (K ë‹¨ìœ„ ì²˜ë¦¬)
        cum_writes_match = re.search(r'Cumulative writes: (\d+)K writes', line)
        if cum_writes_match:
            cum_writes = int(cum_writes_match.group(1)) * 1000
        else:
            cum_writes_match = re.search(r'Cumulative writes: (\d+) writes', line)
            cum_writes = int(cum_writes_match.group(1)) if cum_writes_match else 0
        
        # MB/sì—ì„œ write_rate ê³„ì‚°
        mbps_match = re.search(r'ingest: [\d.]+ GB, ([\d.]+) MB/s', line)
        if mbps_match:
            mbps = float(mbps_match.group(1))
            write_rate = mbps * 1024  # MB/s * 1024 = ops/sec (1KB per op)
        else:
            write_rate = 0
        
        return {
            'timestamp': timestamp,
            'write_rate': write_rate,
            'cumulative_writes': cum_writes
        }
    except Exception as e:
        return None

def parse_compaction_start_line(line, timestamp):
    """Compaction start ë¼ì¸ íŒŒì‹±"""
    try:
        # Base level ì¶”ì¶œ
        base_level_match = re.search(r'Base level (\d+)', line)
        base_level = int(base_level_match.group(1)) if base_level_match else -1
        
        # Input files ì¶”ì¶œ
        inputs_match = re.search(r'inputs: \[([^\]]+)\]', line)
        input_files = inputs_match.group(1) if inputs_match else ""
        
        return {
            'timestamp': timestamp,
            'type': 'start',
            'base_level': base_level,
            'input_files': input_files,
            'output_files': "",
            'compaction_type': f"Level-{base_level}"
        }
    except Exception as e:
        return None

def parse_compaction_finish_line(line, timestamp):
    """Compaction finish ë¼ì¸ íŒŒì‹± (compacted to íŒ¨í„´)"""
    try:
        # files[6 3 0 0 0 0 0] íŒ¨í„´ì—ì„œ ë ˆë²¨ë³„ íŒŒì¼ ìˆ˜ ì¶”ì¶œ
        files_match = re.search(r'files\[([^\]]+)\]', line)
        if files_match:
            files_str = files_match.group(1)
            files_per_level = [int(x) for x in files_str.split()]
            
            # Base level ì¶”ì¶œ (level ì •ë³´ì—ì„œ)
            level_match = re.search(r'level (\d+)', line)
            base_level = int(level_match.group(1)) if level_match else -1
            
            return {
                'timestamp': timestamp,
                'type': 'finished',
                'base_level': base_level,
                'input_files': "",
                'output_files': files_str,
                'compaction_type': f"Level-{base_level}",
                'files_per_level': files_per_level
            }
        return None
    except Exception as e:
        return None

def parse_flush_line(line, timestamp):
    """Flush ë¼ì¸ íŒŒì‹±"""
    try:
        # Flush íƒ€ì… ì¶”ì¶œ
        flush_type = "unknown"
        if "Flushing memtable" in line:
            flush_type = "start"
        elif "Flush lasted" in line:
            flush_type = "finished"
        
        return {
            'timestamp': timestamp,
            'type': flush_type
        }
    except Exception as e:
        return None

def analyze_phase_b_log_based(stats_data, compaction_data, flush_data):
    """Phase-B LOG ê¸°ë°˜ ë¶„ì„"""
    print("ğŸ“Š Phase-B LOG ê¸°ë°˜ ë¶„ì„ ì¤‘...")
    
    try:
        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        stats_df = pd.DataFrame([d for d in stats_data if d is not None])
        compaction_df = pd.DataFrame([d for d in compaction_data if d is not None])
        flush_df = pd.DataFrame([d for d in flush_data if d is not None])
        
        if stats_df.empty:
            print("âŒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
            return
        
        # ì‹œê°„ ë³€í™˜
        stats_df['datetime'] = pd.to_datetime(stats_df['timestamp'], format='%Y/%m/%d-%H:%M:%S.%f')
        if not compaction_df.empty:
            compaction_df['datetime'] = pd.to_datetime(compaction_df['timestamp'], format='%Y/%m/%d-%H:%M:%S.%f')
        if not flush_df.empty:
            flush_df['datetime'] = pd.to_datetime(flush_df['timestamp'], format='%Y/%m/%d-%H:%M:%S.%f')
        
        # ì‹œê°„ë³„ ê·¸ë£¹í™” (ì‹œê°„ ë‹¨ìœ„)
        stats_df['hour'] = stats_df['datetime'].dt.floor('h')
        if not compaction_df.empty:
            compaction_df['hour'] = compaction_df['datetime'].dt.floor('h')
        if not flush_df.empty:
            flush_df['hour'] = flush_df['datetime'].dt.floor('h')
        
        # ì‹œê°í™” ìƒì„±
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))
        
        # 1. ì„±ëŠ¥ ì§€í‘œ ì‹œê°„ë³„ ë³€í™”
        ax1.plot(stats_df['datetime'], stats_df['write_rate'], 'b-', linewidth=2, alpha=0.7, label='Write Rate')
        ax1.set_title('Write Rate Time Series (LOG-based)', fontsize=16, fontweight='bold')
        ax1.set_xlabel('Time')
        ax1.set_ylabel('Write Rate (ops/sec)')
        ax1.grid(True, alpha=0.3)
        ax1.legend()
        ax1.tick_params(axis='x', rotation=45)
        
        # ì´ë™í‰ê·  ì¶”ê°€
        stats_df['ma_10'] = stats_df['write_rate'].rolling(window=10).mean()
        ax1.plot(stats_df['datetime'], stats_df['ma_10'], 'r--', alpha=0.8, linewidth=2, label='10-point MA')
        ax1.legend()
        
        # 2. Compaction Flow ë¶„ì„
        if not compaction_df.empty:
            # Base Levelë³„ Compaction ë¶„í¬
            base_level_counts = compaction_df['base_level'].value_counts().sort_index()
            colors = plt.cm.Set3(np.linspace(0, 1, len(base_level_counts)))
            
            bars = ax2.bar(range(len(base_level_counts)), base_level_counts.values, color=colors, alpha=0.8)
            ax2.set_title('Compaction Distribution by Base Level', fontsize=16, fontweight='bold')
            ax2.set_xlabel('Base Level')
            ax2.set_ylabel('Compaction Count')
            ax2.set_xticks(range(len(base_level_counts)))
            ax2.set_xticklabels([f'Level {l}' for l in base_level_counts.index])
            ax2.grid(True, alpha=0.3)
            
            # ê°’ í‘œì‹œ
            for bar in bars:
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                        f'{int(height)}', ha='center', va='bottom', fontsize=10)
        
        # 3. Flush vs Compaction ë¹„êµ
        if not flush_df.empty and not compaction_df.empty:
            hourly_flush = flush_df.groupby('hour').size()
            hourly_compaction = compaction_df.groupby('hour').size()
            
            ax3.plot(hourly_flush.index, hourly_flush.values, 'b-', linewidth=2, marker='o', label='Flush I/O')
            ax3.plot(hourly_compaction.index, hourly_compaction.values, 'r-', linewidth=2, marker='s', label='Compaction I/O')
            ax3.set_title('Flush vs Compaction I/O Comparison', fontsize=16, fontweight='bold')
            ax3.set_xlabel('Time')
            ax3.set_ylabel('I/O Count per Hour')
            ax3.grid(True, alpha=0.3)
            ax3.legend()
            ax3.tick_params(axis='x', rotation=45)
        
        # 4. ë ˆë²¨ë³„ Compaction Flow íˆíŠ¸ë§µ
        if not compaction_df.empty:
            hourly_base_level = compaction_df.groupby(['hour', 'base_level']).size().unstack(fill_value=0)
            
            if not hourly_base_level.empty:
                # ë ˆë²¨ ìˆœì„œ ì •ë ¬
                level_order = sorted([col for col in hourly_base_level.columns if col != -1])
                hourly_base_level_sorted = hourly_base_level[level_order]
                
                im = ax4.imshow(hourly_base_level_sorted.T.values, cmap='YlOrRd', aspect='auto')
                ax4.set_title('Hourly Compaction Flow Heatmap', fontsize=16, fontweight='bold')
                ax4.set_xlabel('Time (Hours)')
                ax4.set_ylabel('Base Level')
                ax4.set_yticks(range(len(level_order)))
                ax4.set_yticklabels([f'Level {l}' for l in level_order])
                
                # ì‹œê°„ ì¶• ë ˆì´ë¸” (24ì‹œê°„ë§ˆë‹¤)
                time_labels = hourly_base_level_sorted.index[::24]
                time_positions = range(0, len(hourly_base_level_sorted), 24)
                ax4.set_xticks(time_positions)
                ax4.set_xticklabels([t.strftime('%m/%d %H:%M') for t in time_labels], rotation=45)
                
                # ì»¬ëŸ¬ë°” ì¶”ê°€
                cbar = plt.colorbar(im, ax=ax4)
                cbar.set_label('Compaction Count')
        
        plt.tight_layout()
        plt.savefig('phase_b_log_based_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print("âœ… Phase-B LOG ê¸°ë°˜ ë¶„ì„ ì‹œê°í™” ì €ì¥ ì™„ë£Œ: phase_b_log_based_analysis.png")
        
        # ìƒì„¸ ë¶„ì„ ê²°ê³¼ ì¶œë ¥
        print("\nğŸ“Š Phase-B LOG ê¸°ë°˜ ë¶„ì„ ê²°ê³¼:")
        print(f"  ë¶„ì„ ê¸°ê°„: {len(stats_df)} ë°ì´í„° í¬ì¸íŠ¸")
        print(f"  ì´ˆê¸° ì„±ëŠ¥: {stats_df['write_rate'].iloc[0]:.1f} ops/sec")
        print(f"  ìµœì¢… ì„±ëŠ¥: {stats_df['write_rate'].iloc[-1]:.1f} ops/sec")
        print(f"  í‰ê·  ì„±ëŠ¥: {stats_df['write_rate'].mean():.1f} ops/sec")
        print(f"  ìµœëŒ€ ì„±ëŠ¥: {stats_df['write_rate'].max():.1f} ops/sec")
        print(f"  ìµœì†Œ ì„±ëŠ¥: {stats_df['write_rate'].min():.1f} ops/sec")
        
        # Compaction ë¶„ì„
        if not compaction_df.empty:
            print(f"\n  Compaction ë¶„ì„:")
            print(f"    ì´ Compaction: {len(compaction_df)}íšŒ")
            base_level_counts = compaction_df['base_level'].value_counts().sort_index()
            for level, count in base_level_counts.items():
                level_name = f'Level {level}'
                percentage = (count / base_level_counts.sum()) * 100
                print(f"    {level_name}: {count:,}íšŒ ({percentage:.1f}%)")
        
        # Flush ë¶„ì„
        if not flush_df.empty:
            print(f"\n  Flush ë¶„ì„:")
            print(f"    ì´ Flush: {len(flush_df)}íšŒ")
            flush_type_counts = flush_df['type'].value_counts()
            for flush_type, count in flush_type_counts.items():
                print(f"    {flush_type}: {count}íšŒ")
        
        # ì„±ëŠ¥ ì €í•˜ ë¶„ì„
        initial_performance = stats_df['write_rate'].iloc[0]
        final_performance = stats_df['write_rate'].iloc[-1]
        if initial_performance > 0:
            degradation_percent = ((initial_performance - final_performance) / initial_performance) * 100
            print(f"\n  ì„±ëŠ¥ ì €í•˜ ë¶„ì„:")
            print(f"    ì´ˆê¸° ì„±ëŠ¥: {initial_performance:.1f} ops/sec")
            print(f"    ìµœì¢… ì„±ëŠ¥: {final_performance:.1f} ops/sec")
            print(f"    ì„±ëŠ¥ ì €í•˜ìœ¨: {degradation_percent:.1f}%")
        
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ Phase-B LOG ê¸°ë°˜ ë¶„ì„ ì‹œì‘...")
    
    # LOG íŒŒì¼ ê²½ë¡œ ì„¤ì •
    main_log = "rocksdb_log_phase_b.log"
    
    if not os.path.exists(main_log):
        print("âŒ LOG íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        print("Phase-B ì‹¤í–‰ í›„ LOG íŒŒì¼ì´ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return
    
    print(f"ğŸ“– ë©”ì¸ LOG íŒŒì¼: {main_log}")
    
    # LOG íŒŒì¼ ë¶„ì„
    stats_data, compaction_data, flush_data = parse_log_file(main_log)
    
    if not stats_data:
        print("âŒ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    # Phase-B LOG ê¸°ë°˜ ë¶„ì„
    analyze_phase_b_log_based(stats_data, compaction_data, flush_data)
    
    print("\nâœ… Phase-B LOG ê¸°ë°˜ ë¶„ì„ ì™„ë£Œ!")

if __name__ == "__main__":
    main()


