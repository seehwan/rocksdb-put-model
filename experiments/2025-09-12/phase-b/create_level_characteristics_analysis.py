#!/usr/bin/env python3
"""
Phase-B Level Characteristics Analysis
ë ˆë²¨ë³„ íŠ¹ì„± ë¶„ì„ ì‹œê°í™” ìƒì„± (ê¸€ì ê¹¨ì§ ìˆ˜ì •)
"""

import os
import re
import json
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns

# Liberation Serif í°íŠ¸ ì„¤ì • (Times ìŠ¤íƒ€ì¼)
plt.rcParams['font.family'] = 'Liberation Serif'
plt.rcParams['axes.unicode_minus'] = False

def parse_log_file(log_file):
    """LOG íŒŒì¼ íŒŒì‹±"""
    print(f"ğŸ“– LOG íŒŒì¼ íŒŒì‹± ì¤‘: {log_file}")
    
    stats_data = []
    compaction_data = []
    current_timestamp = None
    
    with open(log_file, 'r') as f:
        for line in f:
            line = line.strip()
            
            # ì‹œê°„ ì •ë³´ ì¶”ì¶œ (DUMPING STATS ë¼ì¸ ë°”ë¡œ ìœ„)
            if "------- DUMPING STATS -------" in line:
                # ì´ì „ ë¼ì¸ì—ì„œ ì‹œê°„ ì •ë³´ ì¶”ì¶œ
                continue
            
            # ì‹œê°„ ì •ë³´ê°€ ìˆëŠ” ë¼ì¸ì—ì„œ timestamp ì¶”ì¶œ
            time_match = re.search(r'(\d{4}/\d{2}/\d{2}-\d{2}:\d{2}:\d{2}\.\d+)', line)
            if time_match:
                current_timestamp = time_match.group(1)
            
            # Stats ë¡œê·¸ íŒŒì‹± (ì‹œê°„ ì •ë³´ì™€ í•¨ê»˜)
            if "Cumulative writes:" in line:
                stats_data.append(parse_stats_line(line, current_timestamp))
            
            # Compaction ë¡œê·¸ íŒŒì‹±
            elif "Compaction start" in line or "Compaction finished" in line:
                compaction_data.append(parse_compaction_line(line))
    
    print(f"âœ… íŒŒì‹± ì™„ë£Œ: Stats {len(stats_data)}ê°œ, Compaction {len(compaction_data)}ê°œ")
    return stats_data, compaction_data

def parse_stats_line(line, timestamp):
    """Stats ë¼ì¸ íŒŒì‹±"""
    try:
        # write_rate ì¶”ì¶œ
        write_rate_match = re.search(r'write_rate:(\d+(?:\.\d+)?)', line)
        write_rate = float(write_rate_match.group(1)) if write_rate_match else 0
        
        # cumulative_writes ì¶”ì¶œ
        cum_writes_match = re.search(r'Cumulative writes:(\d+)', line)
        cum_writes = int(cum_writes_match.group(1)) if cum_writes_match else 0
        
        return {
            'timestamp': timestamp,
            'write_rate': write_rate,
            'cumulative_writes': cum_writes
        }
    except Exception as e:
        return None

def parse_compaction_line(line):
    """Compaction ë¼ì¸ íŒŒì‹±"""
    try:
        # level ì¶”ì¶œ
        level_match = re.search(r'level:(\d+)', line)
        level = int(level_match.group(1)) if level_match else -1
        
        # timestamp ì¶”ì¶œ
        timestamp_match = re.search(r'(\d{4}/\d{2}/\d{2}-\d{2}:\d{2}:\d{2}\.\d+)', line)
        timestamp = timestamp_match.group(1) if timestamp_match else None
        
        return {
            'timestamp': timestamp,
            'level': level
        }
    except Exception as e:
        return None

def create_level_characteristics_analysis(stats_data, compaction_data):
    """ë ˆë²¨ë³„ íŠ¹ì„± ë¶„ì„ ì‹œê°í™” ìƒì„±"""
    print("ğŸ“Š ë ˆë²¨ë³„ íŠ¹ì„± ë¶„ì„ ì‹œê°í™” ìƒì„± ì¤‘...")
    
    try:
        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        stats_df = pd.DataFrame(stats_data)
        compaction_df = pd.DataFrame(compaction_data)
        
        if stats_df.empty or compaction_df.empty:
            print("âŒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
            return
        
        # ì‹œê°„ ë³€í™˜
        stats_df['datetime'] = pd.to_datetime(stats_df['timestamp'], format='%Y/%m/%d-%H:%M:%S.%f')
        compaction_df['datetime'] = pd.to_datetime(compaction_df['timestamp'], format='%Y/%m/%d-%H:%M:%S.%f')
        
        # ì‹œê°í™” ìƒì„±
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. ë ˆë²¨ë³„ Compaction ë¹ˆë„
        if not compaction_df.empty:
            level_counts = compaction_df['level'].value_counts().sort_index()
            colors = plt.cm.Set3(np.linspace(0, 1, len(level_counts)))
            
            bars = ax1.bar(range(len(level_counts)), level_counts.values, color=colors, alpha=0.8)
            ax1.set_title('Compaction Frequency by Level', fontsize=14, fontweight='bold')
            ax1.set_xlabel('Level')
            ax1.set_ylabel('Compaction Count')
            ax1.set_xticks(range(len(level_counts)))
            ax1.set_xticklabels([f'Level {l}' for l in level_counts.index])
            ax1.grid(True, alpha=0.3)
            
            # ê°’ í‘œì‹œ
            for i, bar in enumerate(bars):
                height = bar.get_height()
                ax1.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                        f'{int(height)}', ha='center', va='bottom', fontsize=10)
        
        # 2. ì‹œê°„ë³„ ë ˆë²¨ë³„ Compaction íŒ¨í„´
        if not compaction_df.empty:
            compaction_df['hour'] = compaction_df['datetime'].dt.floor('H')
            hourly_level_compactions = compaction_df.groupby(['hour', 'level']).size().unstack(fill_value=0)
            
            if not hourly_level_compactions.empty:
                hourly_level_compactions.plot(kind='bar', stacked=True, ax=ax2, 
                                            color=plt.cm.Set3(np.linspace(0, 1, len(hourly_level_compactions.columns))))
                ax2.set_title('Hourly Compaction Pattern by Level', fontsize=14, fontweight='bold')
                ax2.set_xlabel('Hour')
                ax2.set_ylabel('Compaction Count')
                ax2.legend(title='Level', bbox_to_anchor=(1.05, 1), loc='upper left')
                ax2.tick_params(axis='x', rotation=45)
                ax2.grid(True, alpha=0.3)
        
        # 3. ë ˆë²¨ë³„ Compaction ë¶„í¬ (íŒŒì´ ì°¨íŠ¸)
        if not compaction_df.empty:
            level_counts = compaction_df['level'].value_counts().sort_index()
            colors = plt.cm.Set3(np.linspace(0, 1, len(level_counts)))
            
            wedges, texts, autotexts = ax3.pie(level_counts.values, 
                                             labels=[f'Level {l}' for l in level_counts.index], 
                                             autopct='%1.1f%%', 
                                             colors=colors,
                                             startangle=90)
            ax3.set_title('Compaction Distribution by Level', fontsize=14, fontweight='bold')
            
            # í…ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼ ê°œì„ 
            for autotext in autotexts:
                autotext.set_color('white')
                autotext.set_fontweight('bold')
                autotext.set_fontsize(10)
        
        # 4. ì„±ëŠ¥ vs Compaction ìƒê´€ê´€ê³„
        if not stats_df.empty and not compaction_df.empty:
            # ì‹œê°„ë³„ ì„±ëŠ¥ ë°ì´í„°
            stats_df['hour'] = stats_df['datetime'].dt.floor('H')
            hourly_performance = stats_df.groupby('hour')['write_rate'].mean()
            
            # ì‹œê°„ë³„ Compaction ë°ì´í„°
            compaction_df['hour'] = compaction_df['datetime'].dt.floor('H')
            hourly_compactions = compaction_df.groupby('hour').size()
            
            # ê³µí†µ ì‹œê°„ëŒ€ë§Œ ì„ íƒ
            common_hours = hourly_performance.index.intersection(hourly_compactions.index)
            if len(common_hours) > 0:
                performance_values = [hourly_performance[h] for h in common_hours]
                compaction_values = [hourly_compactions[h] for h in common_hours]
                
                scatter = ax4.scatter(compaction_values, performance_values, 
                                    c=range(len(common_hours)), cmap='viridis', 
                                    alpha=0.7, s=100)
                ax4.set_title('Performance vs Compaction Correlation', fontsize=14, fontweight='bold')
                ax4.set_xlabel('Compaction Count per Hour')
                ax4.set_ylabel('Write Rate (ops/sec)')
                ax4.grid(True, alpha=0.3)
                
                # ì»¬ëŸ¬ë°” ì¶”ê°€
                cbar = plt.colorbar(scatter, ax=ax4)
                cbar.set_label('Time Progression')
        
        plt.tight_layout()
        plt.savefig('level_characteristics_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print("âœ… ë ˆë²¨ë³„ íŠ¹ì„± ë¶„ì„ ì‹œê°í™” ì €ì¥ ì™„ë£Œ: level_characteristics_analysis.png")
        
    except Exception as e:
        print(f"âŒ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ Phase-B Level Characteristics Analysis ì‹œì‘...")
    
    # LOG íŒŒì¼ ì°¾ê¸° (í˜„ì¬ ë””ë ‰í† ë¦¬ì™€ logs ë””ë ‰í† ë¦¬ì—ì„œ)
    log_files = list(Path('.').glob('LOG*')) + list(Path('logs').glob('LOG*'))
    
    if not log_files:
        print("âŒ LOG íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        print("Phase-B ì‹¤í–‰ í›„ LOG íŒŒì¼ì´ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return
    
    # ê°€ì¥ í° LOG íŒŒì¼ ì„ íƒ (ë©”ì¸ ë¡œê·¸)
    main_log = max(log_files, key=lambda f: f.stat().st_size)
    print(f"ğŸ“– ë©”ì¸ LOG íŒŒì¼: {main_log}")
    
    # LOG íŒŒì¼ ë¶„ì„
    stats_data, compaction_data = parse_log_file(main_log)
    
    if not stats_data or not compaction_data:
        print("âŒ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    # ë ˆë²¨ë³„ íŠ¹ì„± ë¶„ì„ ì‹œê°í™” ìƒì„±
    create_level_characteristics_analysis(stats_data, compaction_data)
    
    print("\nâœ… Phase-B Level Characteristics Analysis ì™„ë£Œ!")

if __name__ == "__main__":
    main()
