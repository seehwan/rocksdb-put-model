#!/usr/bin/env python3
"""
ì„±ëŠ¥ ë³€í™” ê¸°ë°˜ êµ¬ê°„ ë¶„í• ì„ í™œìš©í•œ v4.2 ëª¨ë¸ í‰ê°€
"""

import os
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

# Liberation Serif í°íŠ¸ ì„¤ì • (Times ìŠ¤íƒ€ì¼)
plt.rcParams['font.family'] = 'Liberation Serif'
plt.rcParams['axes.unicode_minus'] = False

def load_performance_based_segmentation():
    """ì„±ëŠ¥ ê¸°ë°˜ êµ¬ê°„ ë¶„í•  ê²°ê³¼ ë¡œë“œ"""
    print("ğŸ“Š ì„±ëŠ¥ ê¸°ë°˜ êµ¬ê°„ ë¶„í•  ê²°ê³¼ ë¡œë“œ ì¤‘...")
    
    try:
        with open('performance_based_segmentation_results.json', 'r') as f:
            segmentation_results = json.load(f)
        print("âœ… ì„±ëŠ¥ ê¸°ë°˜ êµ¬ê°„ ë¶„í•  ê²°ê³¼ ë¡œë“œ ì™„ë£Œ")
        return segmentation_results
    except FileNotFoundError:
        print("âŒ ì„±ëŠ¥ ê¸°ë°˜ êµ¬ê°„ ë¶„í•  ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

def load_v4_2_model_results():
    """v4.2 ëª¨ë¸ ê²°ê³¼ ë¡œë“œ"""
    print("ğŸ“Š v4.2 ëª¨ë¸ ê²°ê³¼ ë¡œë“œ ì¤‘...")
    
    try:
        model_file = '/home/sslab/rocksdb-put-model/experiments/2025-09-12/phase-c/results/v4_2_model_results.json'
        if os.path.exists(model_file):
            with open(model_file, 'r') as f:
                model_results = json.load(f)
            print("âœ… v4.2 ëª¨ë¸ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ")
            return model_results
        else:
            print("âŒ v4.2 ëª¨ë¸ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
    except Exception as e:
        print(f"âŒ v4.2 ëª¨ë¸ ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def evaluate_v4_2_with_performance_phases(segmentation_results, model_results):
    """ì„±ëŠ¥ ê¸°ë°˜ êµ¬ê°„ì„ í™œìš©í•œ v4.2 ëª¨ë¸ í‰ê°€"""
    print("ğŸ“Š ì„±ëŠ¥ ê¸°ë°˜ êµ¬ê°„ì„ í™œìš©í•œ v4.2 ëª¨ë¸ í‰ê°€ ì¤‘...")
    
    if not segmentation_results or not model_results:
        print("âŒ í•„ìš”í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # ì„±ëŠ¥ ê¸°ë°˜ êµ¬ê°„ íŠ¹ì„± ì¶”ì¶œ
    performance_phase_characteristics = {}
    for phase_name, phase_data in segmentation_results['segment_analysis'].items():
        performance_phase_characteristics[phase_name] = {
            'avg_performance': phase_data['performance_stats']['avg_write_rate'],
            'stability': phase_data['characteristics']['stability'],
            'performance_level': phase_data['characteristics']['performance_level'],
            'cv': phase_data['performance_stats']['cv'],
            'duration_hours': phase_data['basic_stats']['duration_hours'],
            'trend': phase_data['characteristics']['trend'],
            'change_intensity': phase_data['characteristics']['change_intensity']
        }
    
    # v4.2 ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ ì¶”ì¶œ
    model_predictions = {}
    if 'temporal_predictions' in model_results:
        device_envelope = model_results['temporal_predictions'].get('device_envelope_temporal', {})
        for phase_name in ['initial_phase', 'middle_phase', 'final_phase']:
            if phase_name in device_envelope:
                model_predictions[phase_name] = device_envelope[phase_name]
    
    # ëª¨ë¸ í‰ê°€ ìˆ˜í–‰
    evaluation_results = {}
    
    # êµ¬ê°„ ë§¤í•‘ (ì„±ëŠ¥ ê¸°ë°˜ êµ¬ê°„ â†’ ëª¨ë¸ êµ¬ê°„)
    phase_mapping = {
        'initial': 'initial_phase',
        'middle': 'middle_phase', 
        'final': 'final_phase'
    }
    
    for perf_phase, perf_char in performance_phase_characteristics.items():
        model_phase = phase_mapping.get(perf_phase)
        
        if model_phase and model_phase in model_predictions:
            model_pred = model_predictions[model_phase]
            
            # ì„±ëŠ¥ ì •í™•ë„ í‰ê°€
            performance_accuracy = evaluate_performance_accuracy(perf_char, model_pred)
            
            # ì•ˆì •ì„± ë§¤ì¹­ í‰ê°€
            stability_match = evaluate_stability_match_enhanced(perf_char, model_pred)
            
            # êµ¬ê°„ íŠ¹ì„± í‰ê°€
            phase_characteristics_evaluation = evaluate_phase_characteristics_enhanced(perf_char, model_pred)
            
            # íŠ¸ë Œë“œ ë§¤ì¹­ í‰ê°€
            trend_match = evaluate_trend_match(perf_char, model_pred)
            
            # ì „ì²´ ì ìˆ˜ ê³„ì‚°
            overall_score = calculate_enhanced_overall_score(
                performance_accuracy, stability_match, 
                phase_characteristics_evaluation, trend_match
            )
            
            evaluation_results[perf_phase] = {
                'performance_characteristics': perf_char,
                'model_predictions': model_pred,
                'performance_accuracy': performance_accuracy,
                'stability_match': stability_match,
                'phase_characteristics_evaluation': phase_characteristics_evaluation,
                'trend_match': trend_match,
                'overall_score': overall_score
            }
    
    return evaluation_results

def evaluate_performance_accuracy(perf_char, model_pred):
    """ì„±ëŠ¥ ì •í™•ë„ í‰ê°€ (ê°œì„ ëœ ë²„ì „)"""
    actual_performance = perf_char['avg_performance']  # MB/s
    predicted_write_bw = model_pred.get('adjusted_write_bw', 0)  # MB/s
    
    if predicted_write_bw > 0 and actual_performance > 0:
        # ì§ì ‘ MB/s ë‹¨ìœ„ë¡œ ë¹„êµ
        error_ratio = abs(predicted_write_bw - actual_performance) / actual_performance
        accuracy = max(0, 1 - error_ratio)  # 0~1 ì‚¬ì´ ê°’
        return accuracy
    else:
        return 0.0

def evaluate_stability_match_enhanced(perf_char, model_pred):
    """ì•ˆì •ì„± ë§¤ì¹­ í‰ê°€ (ê°œì„ ëœ ë²„ì „)"""
    # ì„±ëŠ¥ ê¸°ë°˜ êµ¬ê°„ì˜ ì‹¤ì œ ì•ˆì •ì„±
    actual_stability = perf_char['stability']  # 'high', 'medium', 'low'
    actual_cv = perf_char['cv']
    
    # ëª¨ë¸ ì˜ˆì¸¡ì˜ ì•ˆì •ì„± ì§€í‘œë“¤
    model_stability_indicators = []
    
    if 'stability_factor' in model_pred:
        model_stability_indicators.append(model_pred['stability_factor'])
    
    if 'io_contention' in model_pred:
        model_stability_indicators.append(1 - model_pred['io_contention'])
    
    if 'device_degradation' in model_pred:
        # ë‚®ì€ degradation = ë†’ì€ ì•ˆì •ì„±
        model_stability_indicators.append(1 - model_pred['device_degradation'])
    
    if model_stability_indicators:
        avg_model_stability = np.mean(model_stability_indicators)
        
        # ì‹¤ì œ CVë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì•ˆì •ì„± ì ìˆ˜
        if actual_cv < 0.1:
            actual_stability_score = 0.9
        elif actual_cv < 0.3:
            actual_stability_score = 0.6
        else:
            actual_stability_score = 0.3
        
        # ì•ˆì •ì„± ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
        stability_match = 1 - abs(avg_model_stability - actual_stability_score)
        return max(0, min(1, stability_match))
    else:
        return 0.5

def evaluate_phase_characteristics_enhanced(perf_char, model_pred):
    """êµ¬ê°„ë³„ íŠ¹ì„± í‰ê°€ (ê°œì„ ëœ ë²„ì „)"""
    characteristics_score = 0
    total_checks = 0
    
    # ì„±ëŠ¥ ìˆ˜ì¤€ ë§¤ì¹­
    actual_perf_level = perf_char['performance_level']
    predicted_s_max = model_pred.get('s_max', 0)
    
    # S_maxë¥¼ MB/së¡œ ë³€í™˜ (ê°€ì •: 1KB ë ˆì½”ë“œ)
    predicted_mbps = (predicted_s_max * 1024) / (1024 * 1024)  # ops/sec to MB/s (1KB ë ˆì½”ë“œ)
    
    if predicted_mbps > 50:
        model_perf_level = 'high'
    elif predicted_mbps > 15:
        model_perf_level = 'medium'
    else:
        model_perf_level = 'low'
    
    if actual_perf_level == model_perf_level:
        characteristics_score += 1
    total_checks += 1
    
    # ë³€ë™ì„± ë§¤ì¹­
    actual_cv = perf_char['cv']
    model_degradation = model_pred.get('device_degradation', 0)
    
    # degradationì´ ë†’ì„ìˆ˜ë¡ ë³€ë™ì„±ë„ ë†’ì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒ
    expected_cv_from_degradation = model_degradation * 0.8
    cv_match = 1 - abs(actual_cv - expected_cv_from_degradation) / max(actual_cv, expected_cv_from_degradation, 0.1)
    characteristics_score += max(0, cv_match)
    total_checks += 1
    
    # ë³€í™” ê°•ë„ ë§¤ì¹­
    actual_change_intensity = perf_char['change_intensity']
    model_workload_impact = model_pred.get('workload_impact', 0)
    
    # workload_impactê°€ ë†’ì„ìˆ˜ë¡ ë³€í™” ê°•ë„ë„ ë†’ì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒ
    if model_workload_impact > 0.6:
        expected_change_intensity = 'high'
    elif model_workload_impact > 0.4:
        expected_change_intensity = 'medium'
    else:
        expected_change_intensity = 'low'
    
    if actual_change_intensity == expected_change_intensity:
        characteristics_score += 1
    total_checks += 1
    
    return characteristics_score / total_checks if total_checks > 0 else 0

def evaluate_trend_match(perf_char, model_pred):
    """íŠ¸ë Œë“œ ë§¤ì¹­ í‰ê°€"""
    actual_trend = perf_char['trend']  # 'increasing', 'stable', 'decreasing'
    
    # ëª¨ë¸ì—ì„œ íŠ¸ë Œë“œ ì˜ˆì¸¡ (degradation ê¸°ë°˜)
    device_degradation = model_pred.get('device_degradation', 0)
    
    if device_degradation > 0.5:
        expected_trend = 'decreasing'
    elif device_degradation > 0.1:
        expected_trend = 'stable'
    else:
        expected_trend = 'stable'  # ì´ˆê¸°ì—ëŠ” ì•ˆì •ì 
    
    # íŠ¸ë Œë“œ ë§¤ì¹­ ì ìˆ˜
    if actual_trend == expected_trend:
        return 1.0
    elif (actual_trend == 'stable' and expected_trend in ['increasing', 'decreasing']) or \
         (expected_trend == 'stable' and actual_trend in ['increasing', 'decreasing']):
        return 0.5  # ë¶€ë¶„ ë§¤ì¹­
    else:
        return 0.0

def calculate_enhanced_overall_score(performance_accuracy, stability_match, characteristics_evaluation, trend_match):
    """ì „ì²´ ì ìˆ˜ ê³„ì‚° (ê°œì„ ëœ ë²„ì „)"""
    weights = {
        'performance': 0.4,
        'stability': 0.25,
        'characteristics': 0.25,
        'trend': 0.1
    }
    
    overall_score = (
        performance_accuracy * weights['performance'] +
        stability_match * weights['stability'] +
        characteristics_evaluation * weights['characteristics'] +
        trend_match * weights['trend']
    )
    
    return overall_score

def create_enhanced_evaluation_visualization(evaluation_results):
    """ê°œì„ ëœ ëª¨ë¸ í‰ê°€ ì‹œê°í™”"""
    print("ğŸ“Š ì„±ëŠ¥ ê¸°ë°˜ êµ¬ê°„ì„ í™œìš©í•œ ëª¨ë¸ í‰ê°€ ì‹œê°í™” ìƒì„± ì¤‘...")
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))
    fig.suptitle('V4.2 Model Evaluation with Performance-based Phase Segmentation', fontsize=18, fontweight='bold')
    
    phase_names = list(evaluation_results.keys())
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
    
    # 1. êµ¬ê°„ë³„ ì¢…í•© í‰ê°€ ì ìˆ˜
    overall_scores = [evaluation_results[phase]['overall_score'] for phase in phase_names]
    performance_scores = [evaluation_results[phase]['performance_accuracy'] for phase in phase_names]
    stability_scores = [evaluation_results[phase]['stability_match'] for phase in phase_names]
    characteristics_scores = [evaluation_results[phase]['phase_characteristics_evaluation'] for phase in phase_names]
    trend_scores = [evaluation_results[phase]['trend_match'] for phase in phase_names]
    
    x = np.arange(len(phase_names))
    width = 0.15
    
    bars1 = ax1.bar(x - 2*width, overall_scores, width, label='Overall Score', color='darkblue', alpha=0.8)
    bars2 = ax1.bar(x - width, performance_scores, width, label='Performance', color='skyblue', alpha=0.8)
    bars3 = ax1.bar(x, stability_scores, width, label='Stability', color='lightcoral', alpha=0.8)
    bars4 = ax1.bar(x + width, characteristics_scores, width, label='Characteristics', color='lightgreen', alpha=0.8)
    bars5 = ax1.bar(x + 2*width, trend_scores, width, label='Trend', color='orange', alpha=0.8)
    
    ax1.set_ylabel('Score')
    ax1.set_title('Enhanced Model Evaluation Scores by Phase')
    ax1.set_xticks(x)
    ax1.set_xticklabels([p.title() for p in phase_names])
    ax1.legend()
    ax1.set_ylim(0, 1.2)
    ax1.grid(True, alpha=0.3)
    
    # 2. ì‹¤ì œ vs ì˜ˆì¸¡ ì„±ëŠ¥ ë¹„êµ
    actual_performances = [evaluation_results[phase]['performance_characteristics']['avg_performance'] for phase in phase_names]
    predicted_performances = [evaluation_results[phase]['model_predictions'].get('adjusted_write_bw', 0) for phase in phase_names]
    
    ax2.scatter(actual_performances, predicted_performances, c=colors, s=150, alpha=0.7, edgecolors='black')
    
    # ì™„ë²½í•œ ì˜ˆì¸¡ì„ 
    min_perf = min(min(actual_performances), min(predicted_performances))
    max_perf = max(max(actual_performances), max(predicted_performances))
    ax2.plot([min_perf, max_perf], [min_perf, max_perf], 'r--', alpha=0.5, label='Perfect Prediction')
    
    # ê° ì ì— ë¼ë²¨ ì¶”ê°€
    for i, phase in enumerate(phase_names):
        ax2.annotate(phase.title(), (actual_performances[i], predicted_performances[i]), 
                    xytext=(5, 5), textcoords='offset points', fontsize=10)
    
    ax2.set_xlabel('Actual Performance (MB/s)')
    ax2.set_ylabel('Predicted Performance (MB/s)')
    ax2.set_title('Actual vs Predicted Performance (Performance-based Phases)')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. êµ¬ê°„ë³„ íŠ¹ì„± ë¹„êµ (ë ˆì´ë” ì°¨íŠ¸ ìŠ¤íƒ€ì¼)
    metrics = ['Performance', 'Stability', 'Characteristics', 'Trend']
    
    for i, phase in enumerate(phase_names):
        scores = [
            evaluation_results[phase]['performance_accuracy'],
            evaluation_results[phase]['stability_match'],
            evaluation_results[phase]['phase_characteristics_evaluation'],
            evaluation_results[phase]['trend_match']
        ]
        ax3.plot(metrics, scores, 'o-', label=f'{phase.title()} Phase', 
                color=colors[i], linewidth=2, markersize=8)
    
    ax3.set_ylim(0, 1)
    ax3.set_title('Phase-wise Evaluation Metrics')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    ax3.set_ylabel('Score')
    
    # 4. í‰ê°€ ìš”ì•½ ë° ê°œì„ ì‚¬í•­
    ax4.text(0.05, 0.95, 'Performance-based Evaluation Summary', fontsize=16, fontweight='bold', transform=ax4.transAxes)
    
    y_pos = 0.85
    avg_overall = np.mean(overall_scores)
    ax4.text(0.05, y_pos, f'Average Overall Score: {avg_overall:.3f}', fontsize=14, fontweight='bold', transform=ax4.transAxes)
    y_pos -= 0.08
    
    for phase_name, results in evaluation_results.items():
        perf_char = results['performance_characteristics']
        
        ax4.text(0.05, y_pos, f'{phase_name.title()} Phase:', fontsize=13, fontweight='bold', transform=ax4.transAxes)
        y_pos -= 0.05
        
        ax4.text(0.05, y_pos, f'  Overall Score: {results["overall_score"]:.3f}', fontsize=11, transform=ax4.transAxes)
        y_pos -= 0.04
        ax4.text(0.05, y_pos, f'  Duration: {perf_char["duration_hours"]:.1f}h, Avg Perf: {perf_char["avg_performance"]:.1f} MB/s', fontsize=11, transform=ax4.transAxes)
        y_pos -= 0.04
        ax4.text(0.05, y_pos, f'  Stability: {perf_char["stability"]}, Trend: {perf_char["trend"]}', fontsize=11, transform=ax4.transAxes)
        y_pos -= 0.06
    
    # ê°œì„ ì‚¬í•­
    y_pos -= 0.02
    ax4.text(0.05, y_pos, 'Key Improvements:', fontsize=12, fontweight='bold', transform=ax4.transAxes)
    y_pos -= 0.05
    ax4.text(0.05, y_pos, 'â€¢ Performance-based segmentation reflects actual behavior', fontsize=10, transform=ax4.transAxes)
    y_pos -= 0.04
    ax4.text(0.05, y_pos, 'â€¢ Variable phase durations based on performance changes', fontsize=10, transform=ax4.transAxes)
    y_pos -= 0.04
    ax4.text(0.05, y_pos, 'â€¢ Enhanced evaluation metrics include trend matching', fontsize=10, transform=ax4.transAxes)
    y_pos -= 0.04
    ax4.text(0.05, y_pos, 'â€¢ Direct MB/s comparison without unit conversion issues', fontsize=10, transform=ax4.transAxes)
    
    ax4.set_xlim(0, 1)
    ax4.set_ylim(0, 1)
    ax4.axis('off')
    
    plt.tight_layout()
    plt.savefig('v4_2_evaluation_with_performance_based_phases.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print("âœ… ì„±ëŠ¥ ê¸°ë°˜ êµ¬ê°„ì„ í™œìš©í•œ ëª¨ë¸ í‰ê°€ ì‹œê°í™” ì™„ë£Œ")

def save_enhanced_evaluation_results(evaluation_results):
    """ê°œì„ ëœ í‰ê°€ ê²°ê³¼ ì €ì¥"""
    print("ğŸ’¾ ì„±ëŠ¥ ê¸°ë°˜ êµ¬ê°„ì„ í™œìš©í•œ ëª¨ë¸ í‰ê°€ ê²°ê³¼ ì €ì¥ ì¤‘...")
    
    # JSON ê²°ê³¼ ì €ì¥
    results = {
        'evaluation_method': 'performance_based_segmentation',
        'model_version': 'v4.2',
        'evaluation_results': evaluation_results,
        'analysis_time': datetime.now().isoformat()
    }
    
    with open('v4_2_evaluation_with_performance_based_phases_results.json', 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    # Markdown ë³´ê³ ì„œ ìƒì„±
    with open('v4_2_evaluation_with_performance_based_phases_report.md', 'w') as f:
        f.write("# V4.2 Model Evaluation with Performance-based Phase Segmentation\n\n")
        f.write(f"## Analysis Time\n{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write("## Overview\n")
        f.write("This report evaluates the v4.2 model using performance-based phase segmentation.\n")
        f.write("Unlike time-based segmentation, this approach uses actual performance change patterns to define phases.\n\n")
        
        f.write("## Evaluation Method\n")
        f.write("### Performance-based Segmentation Features:\n")
        f.write("1. **Variable Phase Durations**: Based on actual performance transitions\n")
        f.write("2. **Enhanced Metrics**: Performance, stability, characteristics, and trend matching\n")
        f.write("3. **Direct Comparison**: MB/s to MB/s comparison without unit conversion\n")
        f.write("4. **Trend Analysis**: Includes performance trend matching evaluation\n\n")
        
        f.write("## Evaluation Results\n\n")
        
        overall_scores = [results['overall_score'] for results in evaluation_results.values()]
        avg_overall = np.mean(overall_scores)
        f.write(f"**Average Overall Score: {avg_overall:.3f}**\n\n")
        
        for phase_name, results in evaluation_results.items():
            f.write(f"### {phase_name.title()} Phase\n\n")
            
            # ì„±ëŠ¥ ê¸°ë°˜ êµ¬ê°„ íŠ¹ì„±
            perf_char = results['performance_characteristics']
            f.write("**Performance-based Phase Characteristics:**\n")
            f.write(f"- Duration: {perf_char['duration_hours']:.1f} hours\n")
            f.write(f"- Average Performance: {perf_char['avg_performance']:.1f} MB/s\n")
            f.write(f"- Stability: {perf_char['stability']} (CV: {perf_char['cv']:.3f})\n")
            f.write(f"- Performance Level: {perf_char['performance_level']}\n")
            f.write(f"- Trend: {perf_char['trend']}\n")
            f.write(f"- Change Intensity: {perf_char['change_intensity']}\n\n")
            
            # ëª¨ë¸ ì˜ˆì¸¡
            model_pred = results['model_predictions']
            f.write("**V4.2 Model Predictions:**\n")
            f.write(f"- S_max: {model_pred.get('s_max', 0):.1f} ops/sec\n")
            f.write(f"- Adjusted Write BW: {model_pred.get('adjusted_write_bw', 0):.1f} MB/s\n")
            f.write(f"- Device Degradation: {model_pred.get('device_degradation', 0):.3f}\n")
            f.write(f"- Workload Impact: {model_pred.get('workload_impact', 0):.3f}\n")
            f.write(f"- Stability Factor: {model_pred.get('stability_factor', 0):.3f}\n\n")
            
            # í‰ê°€ ì ìˆ˜
            f.write("**Evaluation Scores:**\n")
            f.write(f"- Overall Score: {results['overall_score']:.3f}\n")
            f.write(f"- Performance Accuracy: {results['performance_accuracy']:.3f}\n")
            f.write(f"- Stability Match: {results['stability_match']:.3f}\n")
            f.write(f"- Characteristics Evaluation: {results['phase_characteristics_evaluation']:.3f}\n")
            f.write(f"- Trend Match: {results['trend_match']:.3f}\n\n")
            
            f.write("---\n\n")
        
        f.write("## Key Improvements over Time-based Segmentation\n\n")
        f.write("### 1. Meaningful Phase Boundaries\n")
        f.write("- **Initial Phase**: Very short (0.1h) but captures actual rapid change period\n")
        f.write("- **Middle Phase**: Medium duration (31.8h) reflecting stabilization process\n")
        f.write("- **Final Phase**: Long duration (64.7h) representing stable operation\n\n")
        
        f.write("### 2. Enhanced Evaluation Metrics\n")
        f.write("- **Direct Performance Comparison**: MB/s to MB/s without unit conversion issues\n")
        f.write("- **Trend Matching**: Evaluates whether model predictions match actual trends\n")
        f.write("- **Stability Assessment**: Uses actual CV values for precise stability evaluation\n")
        f.write("- **Change Intensity**: Considers the rate and magnitude of performance changes\n\n")
        
        f.write("### 3. Model Insights\n")
        for phase_name, results in evaluation_results.items():
            score = results['overall_score']
            if score >= 0.7:
                rating = "Good"
            elif score >= 0.5:
                rating = "Moderate"
            else:
                rating = "Poor"
            f.write(f"- **{phase_name.title()} Phase**: {rating} match ({score:.3f})\n")
        
        f.write(f"\n## Conclusion\n")
        f.write("Performance-based segmentation provides more meaningful evaluation by:\n")
        f.write("- Reflecting actual system behavior patterns\n")
        f.write("- Using variable phase durations based on performance changes\n")
        f.write("- Enabling direct performance comparisons\n")
        f.write("- Providing comprehensive evaluation metrics\n\n")
        
        f.write(f"## Analysis Time\n{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
    
    print("âœ… ì„±ëŠ¥ ê¸°ë°˜ êµ¬ê°„ì„ í™œìš©í•œ ëª¨ë¸ í‰ê°€ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ ì„±ëŠ¥ ë³€í™” ê¸°ë°˜ êµ¬ê°„ì„ í™œìš©í•œ v4.2 ëª¨ë¸ í‰ê°€ ì‹œì‘...")
    
    # ë°ì´í„° ë¡œë“œ
    segmentation_results = load_performance_based_segmentation()
    model_results = load_v4_2_model_results()
    
    if not segmentation_results or not model_results:
        print("âŒ í•„ìš”í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ëª¨ë¸ í‰ê°€ ìˆ˜í–‰
    evaluation_results = evaluate_v4_2_with_performance_phases(segmentation_results, model_results)
    
    if not evaluation_results:
        print("âŒ ëª¨ë¸ í‰ê°€ ì‹¤íŒ¨")
        return
    
    # ê²°ê³¼ ì¶œë ¥
    print("\nğŸ“Š ì„±ëŠ¥ ê¸°ë°˜ êµ¬ê°„ì„ í™œìš©í•œ v4.2 ëª¨ë¸ í‰ê°€ ê²°ê³¼:")
    for phase_name, results in evaluation_results.items():
        perf_char = results['performance_characteristics']
        print(f"\n  {phase_name.title()} Phase:")
        print(f"    ì§€ì†ì‹œê°„: {perf_char['duration_hours']:.1f} ì‹œê°„")
        print(f"    ì‹¤ì œ ì„±ëŠ¥: {perf_char['avg_performance']:.1f} MB/s")
        print(f"    ì•ˆì •ì„±: {perf_char['stability']} (CV: {perf_char['cv']:.3f})")
        print(f"    íŠ¸ë Œë“œ: {perf_char['trend']}")
        print(f"    ì „ì²´ ì ìˆ˜: {results['overall_score']:.3f}")
        print(f"    ì„±ëŠ¥ ì •í™•ë„: {results['performance_accuracy']:.3f}")
        print(f"    ì•ˆì •ì„± ë§¤ì¹­: {results['stability_match']:.3f}")
        print(f"    íŠ¹ì„± í‰ê°€: {results['phase_characteristics_evaluation']:.3f}")
        print(f"    íŠ¸ë Œë“œ ë§¤ì¹­: {results['trend_match']:.3f}")
    
    # ì „ì²´ í‰ê·  ì ìˆ˜
    overall_scores = [results['overall_score'] for results in evaluation_results.values()]
    avg_overall = np.mean(overall_scores)
    print(f"\nğŸ“Š ì „ì²´ í‰ê·  ì ìˆ˜: {avg_overall:.3f}")
    
    # ì‹œê°í™” ìƒì„±
    create_enhanced_evaluation_visualization(evaluation_results)
    
    # ê²°ê³¼ ì €ì¥
    save_enhanced_evaluation_results(evaluation_results)
    
    print("\nâœ… ì„±ëŠ¥ ê¸°ë°˜ êµ¬ê°„ì„ í™œìš©í•œ v4.2 ëª¨ë¸ í‰ê°€ ì™„ë£Œ!")
    print("ğŸ“Š ê²°ê³¼ íŒŒì¼: v4_2_evaluation_with_performance_based_phases.png, v4_2_evaluation_with_performance_based_phases_results.json, v4_2_evaluation_with_performance_based_phases_report.md")

if __name__ == "__main__":
    main()

