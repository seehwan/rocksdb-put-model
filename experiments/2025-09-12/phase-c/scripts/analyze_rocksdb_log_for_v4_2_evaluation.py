#!/usr/bin/env python3
"""
RocksDB Log Analysis for V4.2 Model Evaluation
v4.2 ëª¨ë¸ í‰ê°€ë¥¼ ìœ„í•œ RocksDB ë¡œê·¸ ë¶„ì„
"""

import os
import sys
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from datetime import datetime, timedelta
import re
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Liberation Serif'
plt.rcParams['axes.unicode_minus'] = False

class RocksDB_Log_Analyzer_V4_2:
    def __init__(self, results_dir="results"):
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(exist_ok=True)
        
        # RocksDB ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
        self.log_file_path = '/home/sslab/rocksdb-put-model/experiments/2025-09-12/phase-b/rocksdb_log_phase_b.log'
        
        # ë¶„ì„ ê²°ê³¼
        self.log_analysis = {}
        self.performance_data = {}
        
        print("ğŸš€ RocksDB Log Analysis for V4.2 Model Evaluation ì‹œì‘")
        print("=" * 70)
    
    def parse_rocksdb_log(self):
        """RocksDB ë¡œê·¸ íŒŒì‹±"""
        print("ğŸ“Š RocksDB ë¡œê·¸ íŒŒì‹± ì¤‘...")
        
        if not os.path.exists(self.log_file_path):
            print(f"âš ï¸ ë¡œê·¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.log_file_path}")
            return None
        
        log_data = {
            'timestamps': [],
            'put_rates': [],
            'flush_events': [],
            'compaction_events': [],
            'level_stats': [],
            'performance_metrics': []
        }
        
        try:
            with open(self.log_file_path, 'r') as f:
                for line_num, line in enumerate(f, 1):
                    if line_num % 10000 == 0:
                        print(f"   ì²˜ë¦¬ ì¤‘: {line_num:,} ë¼ì¸")
                    
                    # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ
                    timestamp_match = re.search(r'(\d{4}/\d{2}/\d{2}-\d{2}:\d{2}:\d{2}\.\d{6})', line)
                    if timestamp_match:
                        timestamp = timestamp_match.group(1)
                        log_data['timestamps'].append(timestamp)
                    
                    # Put rate ì¶”ì¶œ
                    put_rate_match = re.search(r'put_rate: ([\d.]+)', line)
                    if put_rate_match:
                        put_rate = float(put_rate_match.group(1))
                        log_data['put_rates'].append({
                            'timestamp': timestamp if 'timestamp' in locals() else None,
                            'put_rate': put_rate
                        })
                    
                    # Flush ì´ë²¤íŠ¸ ì¶”ì¶œ
                    if 'flush_started' in line or 'flush_finished' in line:
                        flush_data = self._parse_flush_event(line, timestamp if 'timestamp' in locals() else None)
                        if flush_data:
                            log_data['flush_events'].append(flush_data)
                    
                    # Compaction ì´ë²¤íŠ¸ ì¶”ì¶œ
                    if 'compaction_started' in line or 'compaction_finished' in line:
                        compaction_data = self._parse_compaction_event(line, timestamp if 'timestamp' in locals() else None)
                        if compaction_data:
                            log_data['compaction_events'].append(compaction_data)
                    
                    # Level í†µê³„ ì¶”ì¶œ
                    if 'level' in line and 'files' in line:
                        level_data = self._parse_level_stats(line, timestamp if 'timestamp' in locals() else None)
                        if level_data:
                            log_data['level_stats'].append(level_data)
            
            print(f"âœ… RocksDB ë¡œê·¸ íŒŒì‹± ì™„ë£Œ:")
            print(f"   - íƒ€ì„ìŠ¤íƒ¬í”„: {len(log_data['timestamps']):,}ê°œ")
            print(f"   - Put rates: {len(log_data['put_rates']):,}ê°œ")
            print(f"   - Flush ì´ë²¤íŠ¸: {len(log_data['flush_events']):,}ê°œ")
            print(f"   - Compaction ì´ë²¤íŠ¸: {len(log_data['compaction_events']):,}ê°œ")
            print(f"   - Level í†µê³„: {len(log_data['level_stats']):,}ê°œ")
            
            return log_data
            
        except Exception as e:
            print(f"âš ï¸ ë¡œê·¸ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
    
    def _parse_flush_event(self, line, timestamp):
        """Flush ì´ë²¤íŠ¸ íŒŒì‹±"""
        try:
            if 'flush_started' in line:
                # flush_started ì´ë²¤íŠ¸ íŒŒì‹±
                match = re.search(r'flush_started: (\d+)', line)
                if match:
                    return {
                        'event_type': 'flush_started',
                        'timestamp': timestamp,
                        'memtable_id': int(match.group(1))
                    }
            elif 'flush_finished' in line:
                # flush_finished ì´ë²¤íŠ¸ íŒŒì‹±
                match = re.search(r'flush_finished: (\d+)', line)
                if match:
                    return {
                        'event_type': 'flush_finished',
                        'timestamp': timestamp,
                        'memtable_id': int(match.group(1))
                    }
        except Exception as e:
            pass
        return None
    
    def _parse_compaction_event(self, line, timestamp):
        """Compaction ì´ë²¤íŠ¸ íŒŒì‹±"""
        try:
            if 'compaction_started' in line:
                # compaction_started ì´ë²¤íŠ¸ íŒŒì‹±
                match = re.search(r'compaction_started: level=(\d+)', line)
                if match:
                    return {
                        'event_type': 'compaction_started',
                        'timestamp': timestamp,
                        'level': int(match.group(1))
                    }
            elif 'compaction_finished' in line:
                # compaction_finished ì´ë²¤íŠ¸ íŒŒì‹±
                match = re.search(r'compaction_finished: level=(\d+)', line)
                if match:
                    return {
                        'event_type': 'compaction_finished',
                        'timestamp': timestamp,
                        'level': int(match.group(1))
                    }
        except Exception as e:
            pass
        return None
    
    def _parse_level_stats(self, line, timestamp):
        """Level í†µê³„ íŒŒì‹±"""
        try:
            # Level í†µê³„ íŒŒì‹± (ê°„ë‹¨í•œ ì˜ˆì‹œ)
            match = re.search(r'level (\d+): (\d+) files', line)
            if match:
                return {
                    'timestamp': timestamp,
                    'level': int(match.group(1)),
                    'files': int(match.group(2))
                }
        except Exception as e:
            pass
        return None
    
    def analyze_temporal_performance(self, log_data):
        """ì‹œê¸°ë³„ ì„±ëŠ¥ ë¶„ì„"""
        print("ğŸ“Š ì‹œê¸°ë³„ ì„±ëŠ¥ ë¶„ì„ ì¤‘...")
        
        if not log_data or not log_data['put_rates']:
            print("âš ï¸ Put rate ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # Put rate ë°ì´í„°ë¥¼ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬
        put_rates = sorted(log_data['put_rates'], key=lambda x: x['timestamp'] if x['timestamp'] else '')
        
        # ì „ì²´ ê¸°ê°„ì„ 3ê°œ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
        total_periods = len(put_rates)
        initial_period = total_periods // 3
        middle_period = (total_periods * 2) // 3
        
        # ì‹œê¸°ë³„ ì„±ëŠ¥ ë¶„ì„
        temporal_performance = {
            'initial_phase': {
                'period': '0-33%',
                'data_points': put_rates[:initial_period],
                'avg_put_rate': 0,
                'max_put_rate': 0,
                'min_put_rate': 0,
                'std_put_rate': 0
            },
            'middle_phase': {
                'period': '33-66%',
                'data_points': put_rates[initial_period:middle_period],
                'avg_put_rate': 0,
                'max_put_rate': 0,
                'min_put_rate': 0,
                'std_put_rate': 0
            },
            'final_phase': {
                'period': '66-100%',
                'data_points': put_rates[middle_period:],
                'avg_put_rate': 0,
                'min_put_rate': 0,
                'std_put_rate': 0
            }
        }
        
        # ê° ì‹œê¸°ë³„ í†µê³„ ê³„ì‚°
        for phase_name, phase_data in temporal_performance.items():
            if phase_data['data_points']:
                put_rates_values = [point['put_rate'] for point in phase_data['data_points']]
                phase_data['avg_put_rate'] = np.mean(put_rates_values)
                phase_data['max_put_rate'] = np.max(put_rates_values)
                phase_data['min_put_rate'] = np.min(put_rates_values)
                phase_data['std_put_rate'] = np.std(put_rates_values)
                
                print(f"   {phase_name}: {phase_data['avg_put_rate']:.2f} ops/sec (avg)")
        
        return temporal_performance
    
    def analyze_compaction_performance(self, log_data):
        """Compaction ì„±ëŠ¥ ë¶„ì„"""
        print("ğŸ“Š Compaction ì„±ëŠ¥ ë¶„ì„ ì¤‘...")
        
        if not log_data or not log_data['compaction_events']:
            print("âš ï¸ Compaction ì´ë²¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # Compaction ì´ë²¤íŠ¸ ë¶„ì„
        compaction_analysis = {
            'total_compactions': len(log_data['compaction_events']),
            'compaction_by_level': {},
            'compaction_timeline': [],
            'compaction_efficiency': {}
        }
        
        # Levelë³„ Compaction í†µê³„
        for event in log_data['compaction_events']:
            if 'level' in event:
                level = event['level']
                if level not in compaction_analysis['compaction_by_level']:
                    compaction_analysis['compaction_by_level'][level] = 0
                compaction_analysis['compaction_by_level'][level] += 1
        
        print(f"   ì´ Compaction ì´ë²¤íŠ¸: {compaction_analysis['total_compactions']:,}ê°œ")
        for level, count in compaction_analysis['compaction_by_level'].items():
            print(f"   Level {level}: {count:,}ê°œ")
        
        return compaction_analysis
    
    def analyze_flush_performance(self, log_data):
        """Flush ì„±ëŠ¥ ë¶„ì„"""
        print("ğŸ“Š Flush ì„±ëŠ¥ ë¶„ì„ ì¤‘...")
        
        if not log_data or not log_data['flush_events']:
            print("âš ï¸ Flush ì´ë²¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # Flush ì´ë²¤íŠ¸ ë¶„ì„
        flush_analysis = {
            'total_flushes': len(log_data['flush_events']),
            'flush_timeline': [],
            'flush_efficiency': {}
        }
        
        print(f"   ì´ Flush ì´ë²¤íŠ¸: {flush_analysis['total_flushes']:,}ê°œ")
        
        return flush_analysis
    
    def evaluate_v4_2_model_with_log_data(self, temporal_performance):
        """ë¡œê·¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ v4.2 ëª¨ë¸ í‰ê°€"""
        print("ğŸ“Š ë¡œê·¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ v4.2 ëª¨ë¸ í‰ê°€ ì¤‘...")
        
        # v4.2 ëª¨ë¸ ê²°ê³¼ ë¡œë“œ
        v4_2_file = '/home/sslab/rocksdb-put-model/experiments/2025-09-12/phase-c/scripts/results/v4_2_fillrandom_enhanced_model_results.json'
        
        if not os.path.exists(v4_2_file):
            print("âš ï¸ v4.2 ëª¨ë¸ ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        try:
            with open(v4_2_file, 'r') as f:
                v4_2_results = json.load(f)
        except Exception as e:
            print(f"âš ï¸ v4.2 ëª¨ë¸ ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
        
        # v4.2 ëª¨ë¸ ì˜ˆì¸¡ê°’
        v4_2_predictions = v4_2_results.get('v4_2_predictions', {})
        device_envelope = v4_2_predictions.get('device_envelope_temporal', {})
        
        # ë¡œê·¸ ë°ì´í„°ì™€ ëª¨ë¸ ì˜ˆì¸¡ ë¹„êµ
        model_evaluation = {
            'phase_comparison': {},
            'accuracy_analysis': {},
            'performance_trends': {}
        }
        
        for phase_name, phase_data in temporal_performance.items():
            actual_avg_rate = phase_data['avg_put_rate']
            model_prediction = device_envelope.get(phase_name, {}).get('s_max', 0)
            
            # ì •í™•ë„ ê³„ì‚°
            if actual_avg_rate > 0 and model_prediction > 0:
                accuracy = min(100.0, (1.0 - abs(model_prediction - actual_avg_rate) / actual_avg_rate) * 100)
            else:
                accuracy = 0.0
            
            model_evaluation['phase_comparison'][phase_name] = {
                'actual_avg_rate': actual_avg_rate,
                'model_prediction': model_prediction,
                'accuracy': accuracy,
                'difference': abs(model_prediction - actual_avg_rate),
                'relative_error': abs(model_prediction - actual_avg_rate) / actual_avg_rate * 100 if actual_avg_rate > 0 else 0
            }
            
            print(f"   {phase_name}:")
            print(f"     ì‹¤ì œ í‰ê· : {actual_avg_rate:.2f} ops/sec")
            print(f"     ëª¨ë¸ ì˜ˆì¸¡: {model_prediction:.2f} ops/sec")
            print(f"     ì •í™•ë„: {accuracy:.1f}%")
        
        # ì „ì²´ ì •í™•ë„ ê³„ì‚°
        accuracies = [data['accuracy'] for data in model_evaluation['phase_comparison'].values()]
        model_evaluation['accuracy_analysis'] = {
            'overall_accuracy': np.mean(accuracies) if accuracies else 0.0,
            'accuracy_by_phase': {phase: data['accuracy'] for phase, data in model_evaluation['phase_comparison'].items()},
            'best_phase': max(model_evaluation['phase_comparison'].items(), key=lambda x: x[1]['accuracy'])[0] if model_evaluation['phase_comparison'] else None,
            'worst_phase': min(model_evaluation['phase_comparison'].items(), key=lambda x: x[1]['accuracy'])[0] if model_evaluation['phase_comparison'] else None
        }
        
        print(f"âœ… ë¡œê·¸ ë°ì´í„° ê¸°ë°˜ v4.2 ëª¨ë¸ í‰ê°€ ì™„ë£Œ:")
        print(f"   ì „ì²´ ì •í™•ë„: {model_evaluation['accuracy_analysis']['overall_accuracy']:.1f}%")
        
        return model_evaluation
    
    def create_log_analysis_visualization(self, temporal_performance, model_evaluation):
        """ë¡œê·¸ ë¶„ì„ ì‹œê°í™” ìƒì„±"""
        print("ğŸ“Š ë¡œê·¸ ë¶„ì„ ì‹œê°í™” ìƒì„± ì¤‘...")
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 14))
        fig.suptitle('RocksDB Log Analysis for V4.2 Model Evaluation', fontsize=16, fontweight='bold')
        
        # 1. ì‹œê¸°ë³„ ì‹¤ì œ ì„±ëŠ¥
        if temporal_performance:
            phases = list(temporal_performance.keys())
            avg_rates = [phase_data['avg_put_rate'] for phase_data in temporal_performance.values()]
            max_rates = [phase_data['max_put_rate'] for phase_data in temporal_performance.values()]
            min_rates = [phase_data['min_put_rate'] for phase_data in temporal_performance.values()]
            
            x = np.arange(len(phases))
            width = 0.25
            
            ax1.bar(x - width, avg_rates, width, label='Average', color='skyblue', alpha=0.7)
            ax1.bar(x, max_rates, width, label='Maximum', color='lightgreen', alpha=0.7)
            ax1.bar(x + width, min_rates, width, label='Minimum', color='lightcoral', alpha=0.7)
            
            ax1.set_ylabel('Put Rate (ops/sec)')
            ax1.set_title('Actual Performance by Phase (from RocksDB Log)')
            ax1.set_xticks(x)
            ax1.set_xticklabels([p.replace('_phase', '').title() for p in phases])
            ax1.legend()
            ax1.grid(True, alpha=0.3)
        
        # 2. ëª¨ë¸ ì˜ˆì¸¡ vs ì‹¤ì œ ì„±ëŠ¥
        if model_evaluation and 'phase_comparison' in model_evaluation:
            phases = list(model_evaluation['phase_comparison'].keys())
            actual_rates = [data['actual_avg_rate'] for data in model_evaluation['phase_comparison'].values()]
            predicted_rates = [data['model_prediction'] for data in model_evaluation['phase_comparison'].values()]
            
            x = np.arange(len(phases))
            width = 0.35
            
            bars1 = ax2.bar(x - width/2, actual_rates, width, label='Actual (Log)', color='lightblue', alpha=0.7)
            bars2 = ax2.bar(x + width/2, predicted_rates, width, label='Predicted (V4.2)', color='lightcoral', alpha=0.7)
            
            ax2.set_ylabel('Put Rate (ops/sec)')
            ax2.set_title('V4.2 Model Prediction vs Actual Performance')
            ax2.set_xticks(x)
            ax2.set_xticklabels([p.replace('_phase', '').title() for p in phases])
            ax2.legend()
            ax2.grid(True, alpha=0.3)
            
            # ì •í™•ë„ í‘œì‹œ
            for i, (actual, predicted) in enumerate(zip(actual_rates, predicted_rates)):
                accuracy = model_evaluation['phase_comparison'][phases[i]]['accuracy']
                ax2.text(i, max(actual, predicted) + 0.1 * max(actual, predicted),
                        f'{accuracy:.1f}%', ha='center', va='bottom', fontsize=9)
        
        # 3. ì •í™•ë„ ë¶„ì„
        if model_evaluation and 'accuracy_analysis' in model_evaluation:
            accuracy_data = model_evaluation['accuracy_analysis']
            phase_accuracies = accuracy_data.get('accuracy_by_phase', {})
            
            if phase_accuracies:
                phases = list(phase_accuracies.keys())
                accuracies = list(phase_accuracies.values())
                
                colors = ['green' if acc > 80 else 'orange' if acc > 60 else 'red' for acc in accuracies]
                bars = ax3.bar([p.replace('_phase', '').title() for p in phases], accuracies, color=colors, alpha=0.7)
                ax3.set_ylabel('Accuracy (%)')
                ax3.set_title('V4.2 Model Accuracy by Phase (Log-based)')
                ax3.set_ylim(0, 100)
                ax3.grid(True, alpha=0.3)
                
                for bar, accuracy in zip(bars, accuracies):
                    height = bar.get_height()
                    ax3.text(bar.get_x() + bar.get_width()/2., height,
                            f'{accuracy:.1f}%', ha='center', va='bottom', fontweight='bold')
        
        # 4. í‰ê°€ ìš”ì•½
        if model_evaluation and 'accuracy_analysis' in model_evaluation:
            accuracy_data = model_evaluation['accuracy_analysis']
            
            ax4.text(0.1, 0.9, 'V4.2 Model Evaluation Summary (Log-based):', fontsize=14, fontweight='bold', transform=ax4.transAxes)
            
            overall_accuracy = accuracy_data.get('overall_accuracy', 0)
            ax4.text(0.1, 0.8, f'Overall Accuracy: {overall_accuracy:.1f}%', fontsize=12, transform=ax4.transAxes)
            
            best_phase = accuracy_data.get('best_phase', 'N/A')
            worst_phase = accuracy_data.get('worst_phase', 'N/A')
            ax4.text(0.1, 0.7, f'Best Phase: {best_phase.replace("_", " ").title()}', fontsize=10, transform=ax4.transAxes)
            ax4.text(0.1, 0.65, f'Worst Phase: {worst_phase.replace("_", " ").title()}', fontsize=10, transform=ax4.transAxes)
            
            # ì‹œê¸°ë³„ ì •í™•ë„
            phase_accuracies = accuracy_data.get('accuracy_by_phase', {})
            y_pos = 0.55
            for phase, accuracy in phase_accuracies.items():
                ax4.text(0.1, y_pos, f'{phase.replace("_", " ").title()}: {accuracy:.1f}%', fontsize=10, transform=ax4.transAxes)
                y_pos -= 0.05
            
            ax4.set_xlim(0, 1)
            ax4.set_ylim(0, 1)
            ax4.axis('off')
            ax4.set_title('Evaluation Summary')
        
        plt.tight_layout()
        plt.savefig(f"{self.results_dir}/rocksdb_log_analysis_v4_2_evaluation.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        print("âœ… ë¡œê·¸ ë¶„ì„ ì‹œê°í™” ì™„ë£Œ")
    
    def save_results(self, log_data, temporal_performance, model_evaluation):
        """ê²°ê³¼ ì €ì¥"""
        print("ğŸ’¾ ë¡œê·¸ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘...")
        
        # JSON ê²°ê³¼ ì €ì¥
        try:
            results = {
                'log_analysis': log_data,
                'temporal_performance': temporal_performance,
                'model_evaluation': model_evaluation,
                'analysis_time': datetime.now().isoformat()
            }
            
            with open(f"{self.results_dir}/rocksdb_log_analysis_v4_2_evaluation_results.json", 'w') as f:
                json.dump(results, f, indent=2, default=str)
            print("âœ… JSON ê²°ê³¼ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ JSON ì €ì¥ ì‹¤íŒ¨: {e}")
        
        # Markdown ë³´ê³ ì„œ ìƒì„±
        try:
            report_content = self._generate_log_analysis_report(temporal_performance, model_evaluation)
            with open(f"{self.results_dir}/rocksdb_log_analysis_v4_2_evaluation_report.md", 'w') as f:
                f.write(report_content)
            print("âœ… Markdown ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ Markdown ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def _generate_log_analysis_report(self, temporal_performance, model_evaluation):
        """ë¡œê·¸ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        report = f"""# RocksDB Log Analysis for V4.2 Model Evaluation

## Overview
This report presents the analysis of RocksDB log data to evaluate the V4.2 FillRandom Enhanced model with actual performance data from the log.

## Analysis Time
{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## RocksDB Log Analysis Results

### Temporal Performance Analysis (from Log Data)
"""
        
        if temporal_performance:
            for phase_name, phase_data in temporal_performance.items():
                report += f"""
#### {phase_name.replace('_', ' ').title()} Phase
- **Period**: {phase_data['period']}
- **Data Points**: {len(phase_data['data_points']):,}
- **Average Put Rate**: {phase_data['avg_put_rate']:.2f} ops/sec
- **Maximum Put Rate**: {phase_data['max_put_rate']:.2f} ops/sec
- **Minimum Put Rate**: {phase_data['min_put_rate']:.2f} ops/sec
- **Standard Deviation**: {phase_data['std_put_rate']:.2f} ops/sec
"""
        
        if model_evaluation:
            report += f"""
### V4.2 Model Evaluation (Log-based)

#### Overall Accuracy
- **Overall Accuracy**: {model_evaluation.get('accuracy_analysis', {}).get('overall_accuracy', 0):.1f}%
- **Best Phase**: {model_evaluation.get('accuracy_analysis', {}).get('best_phase', 'N/A').replace('_', ' ').title()}
- **Worst Phase**: {model_evaluation.get('accuracy_analysis', {}).get('worst_phase', 'N/A').replace('_', ' ').title()}

#### Phase-specific Comparison
"""
            for phase_name, phase_data in model_evaluation.get('phase_comparison', {}).items():
                report += f"""
##### {phase_name.replace('_', ' ').title()} Phase
- **Actual Average Rate**: {phase_data['actual_avg_rate']:.2f} ops/sec
- **Model Prediction**: {phase_data['model_prediction']:.2f} ops/sec
- **Accuracy**: {phase_data['accuracy']:.1f}%
- **Difference**: {phase_data['difference']:.2f} ops/sec
- **Relative Error**: {phase_data['relative_error']:.1f}%
"""
        
        report += f"""
## Key Insights

### 1. Log-based Performance Analysis
- **Data Source**: RocksDB log file analysis
- **Temporal Phases**: Initial, Middle, Final phases identified
- **Performance Metrics**: Put rates, Flush events, Compaction events
- **Accuracy**: Based on actual log data rather than summary statistics

### 2. V4.2 Model Evaluation
- **Evaluation Method**: Log-based performance comparison
- **Accuracy Calculation**: Direct comparison with actual log data
- **Phase Analysis**: Phase-specific accuracy assessment
- **Performance Trends**: Temporal performance trend analysis

### 3. Model Accuracy Insights
- **Log-based Accuracy**: More accurate than summary-based evaluation
- **Phase-specific Performance**: Detailed phase-by-phase analysis
- **Real Performance Data**: Actual RocksDB performance from logs
- **Temporal Trends**: Performance changes over time

## Visualization
![RocksDB Log Analysis for V4.2 Evaluation](rocksdb_log_analysis_v4_2_evaluation.png)

## Analysis Time
{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
        
        return report
    
    def run_analysis(self):
        """ì „ì²´ ë¶„ì„ ì‹¤í–‰"""
        print("ğŸš€ RocksDB ë¡œê·¸ ë¶„ì„ ì‹œì‘")
        print("=" * 70)
        
        # 1. RocksDB ë¡œê·¸ íŒŒì‹±
        log_data = self.parse_rocksdb_log()
        if not log_data:
            print("âš ï¸ ë¡œê·¸ íŒŒì‹± ì‹¤íŒ¨")
            return
        
        # 2. ì‹œê¸°ë³„ ì„±ëŠ¥ ë¶„ì„
        temporal_performance = self.analyze_temporal_performance(log_data)
        if not temporal_performance:
            print("âš ï¸ ì‹œê¸°ë³„ ì„±ëŠ¥ ë¶„ì„ ì‹¤íŒ¨")
            return
        
        # 3. Compaction ì„±ëŠ¥ ë¶„ì„
        compaction_analysis = self.analyze_compaction_performance(log_data)
        
        # 4. Flush ì„±ëŠ¥ ë¶„ì„
        flush_analysis = self.analyze_flush_performance(log_data)
        
        # 5. v4.2 ëª¨ë¸ í‰ê°€
        model_evaluation = self.evaluate_v4_2_model_with_log_data(temporal_performance)
        if not model_evaluation:
            print("âš ï¸ v4.2 ëª¨ë¸ í‰ê°€ ì‹¤íŒ¨")
            return
        
        # 6. ì‹œê°í™” ìƒì„±
        self.create_log_analysis_visualization(temporal_performance, model_evaluation)
        
        # 7. ê²°ê³¼ ì €ì¥
        self.save_results(log_data, temporal_performance, model_evaluation)
        
        print("=" * 70)
        print("âœ… RocksDB ë¡œê·¸ ë¶„ì„ ì™„ë£Œ!")
        print(f"ğŸ“Š ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {self.results_dir}")

if __name__ == "__main__":
    analyzer = RocksDB_Log_Analyzer_V4_2()
    analyzer.run_analysis()

