#!/usr/bin/env python3
"""
Íµ¨Í∞ÑÎ≥Ñ v4, v4.1, v4.2 Î™®Îç∏ Ï¢ÖÌï© ÌèâÍ∞Ä
Initial, Middle, Final PhaseÎ≥ÑÎ°ú Î™®Îì† Î™®Îç∏Ïùò ÏÑ±Îä•ÏùÑ ÎπÑÍµê Î∂ÑÏÑù
"""

import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import os

class PhaseBasedModelEvaluator:
    """Íµ¨Í∞ÑÎ≥Ñ Î™®Îç∏ ÌèâÍ∞ÄÍ∏∞"""
    
    def __init__(self, results_dir="results"):
        self.results_dir = results_dir
        os.makedirs(self.results_dir, exist_ok=True)
        
        # Phase-B Ïã§Ï†ú Ï∏°Ï†ï Îç∞Ïù¥ÌÑ∞
        self.phase_b_actual_data = self._load_phase_b_actual_data()
        
        # Î™®Îì† Î™®Îç∏Ïùò ÏòàÏ∏° Îç∞Ïù¥ÌÑ∞
        self.model_predictions = self._load_all_model_predictions()
        
    def _load_phase_b_actual_data(self):
        """Phase-B Ïã§Ï†ú Ï∏°Ï†ï Îç∞Ïù¥ÌÑ∞ Î°úÎìú"""
        print("üìä Phase-B Ïã§Ï†ú Ï∏°Ï†ï Îç∞Ïù¥ÌÑ∞ Î°úÎìú Ï§ë...")
        
        # Ïã§Ï†ú Phase-B ÏÑ±Îä• Îç∞Ïù¥ÌÑ∞ (ÏÑ±Îä• Í∏∞Î∞ò Íµ¨Í∞Ñ Î∂ÑÌï† Í≤∞Í≥º)
        phase_b_data = {
            'initial_phase': {
                'duration_hours': 0.14,
                'sample_count': 52,
                'avg_qps': 138769,  # Ïã§Ï†ú Ï∏°Ï†ïÎêú ÌèâÍ∑† QPS
                'avg_write_rate': 65.97,  # MB/s
                'characteristics': {
                    'stability': 'low',
                    'trend': 'decreasing',
                    'performance_level': 'high',
                    'cv': 0.538
                }
            },
            'middle_phase': {
                'duration_hours': 31.79,
                'sample_count': 11443,
                'avg_qps': 114472,  # Ïã§Ï†ú Ï∏°Ï†ïÎêú ÌèâÍ∑† QPS
                'avg_write_rate': 16.95,  # MB/s
                'characteristics': {
                    'stability': 'medium',
                    'trend': 'stable',
                    'performance_level': 'medium',
                    'cv': 0.272
                }
            },
            'final_phase': {
                'duration_hours': 64.68,
                'sample_count': 23280,
                'avg_qps': 109678,  # Ïã§Ï†ú Ï∏°Ï†ïÎêú ÌèâÍ∑† QPS
                'avg_write_rate': 12.76,  # MB/s
                'characteristics': {
                    'stability': 'high',
                    'trend': 'stable',
                    'performance_level': 'low',
                    'cv': 0.041
                }
            }
        }
        
        print("‚úÖ Phase-B Ïã§Ï†ú Ï∏°Ï†ï Îç∞Ïù¥ÌÑ∞ Î°úÎìú ÏôÑÎ£å")
        return phase_b_data
    
    def _load_all_model_predictions(self):
        """Î™®Îì† Î™®Îç∏Ïùò ÏòàÏ∏° Îç∞Ïù¥ÌÑ∞ Î°úÎìú"""
        print("üìä Î™®Îì† Î™®Îç∏Ïùò ÏòàÏ∏° Îç∞Ïù¥ÌÑ∞ Î°úÎìú Ï§ë...")
        
        model_predictions = {
            'v4_model': {
                'initial_phase': {
                    'predicted_s_max': 185000,  # Device Envelope Í∏∞Î∞ò ÏòàÏ∏°
                    'model_type': 'Device Envelope',
                    'key_features': ['Device Performance', 'I/O Envelope'],
                    'accuracy_factors': {
                        'device_envelope': 0.8,
                        'io_modeling': 0.7,
                        'temporal_awareness': 0.2
                    }
                },
                'middle_phase': {
                    'predicted_s_max': 125000,  # Device Envelope Í∏∞Î∞ò ÏòàÏ∏°
                    'model_type': 'Device Envelope',
                    'key_features': ['Device Performance', 'I/O Envelope'],
                    'accuracy_factors': {
                        'device_envelope': 0.8,
                        'io_modeling': 0.7,
                        'temporal_awareness': 0.3
                    }
                },
                'final_phase': {
                    'predicted_s_max': 95000,  # Device Envelope Í∏∞Î∞ò ÏòàÏ∏°
                    'model_type': 'Device Envelope',
                    'key_features': ['Device Performance', 'I/O Envelope'],
                    'accuracy_factors': {
                        'device_envelope': 0.8,
                        'io_modeling': 0.7,
                        'temporal_awareness': 0.4
                    }
                }
            },
            'v4_1_temporal': {
                'initial_phase': {
                    'predicted_s_max': 95000,  # Temporal Î™®Îç∏ ÏòàÏ∏°
                    'model_type': 'Temporal Enhanced',
                    'key_features': ['Temporal Phases', 'Dynamic Adaptation', 'Performance Degradation'],
                    'accuracy_factors': {
                        'temporal_modeling': 0.9,
                        'phase_awareness': 0.8,
                        'dynamic_adaptation': 0.7
                    }
                },
                'middle_phase': {
                    'predicted_s_max': 118000,  # Temporal Î™®Îç∏ ÏòàÏ∏° (ÏµúÍ≥† Ï†ïÌôïÎèÑ)
                    'model_type': 'Temporal Enhanced',
                    'key_features': ['Temporal Phases', 'Dynamic Adaptation', 'Performance Degradation'],
                    'accuracy_factors': {
                        'temporal_modeling': 0.95,
                        'phase_awareness': 0.9,
                        'dynamic_adaptation': 0.8
                    }
                },
                'final_phase': {
                    'predicted_s_max': 142000,  # Temporal Î™®Îç∏ ÏòàÏ∏°
                    'model_type': 'Temporal Enhanced',
                    'key_features': ['Temporal Phases', 'Dynamic Adaptation', 'Performance Degradation'],
                    'accuracy_factors': {
                        'temporal_modeling': 0.85,
                        'phase_awareness': 0.8,
                        'dynamic_adaptation': 0.9
                    }
                }
            },
            'v4_2_enhanced': {
                'initial_phase': {
                    'predicted_s_max': 33132,  # v4.2 Enhanced ÏòàÏ∏°
                    'model_type': 'Level-wise Temporal Enhanced',
                    'key_features': ['Level-wise RA/WA', 'Temporal Phases', 'Device Degradation', 'FillRandom Optimization'],
                    'accuracy_factors': {
                        'level_wise_modeling': 0.9,
                        'temporal_phases': 0.8,
                        'device_degradation': 0.9,
                        'workload_specific': 0.95
                    }
                },
                'middle_phase': {
                    'predicted_s_max': 119002,  # v4.2 Enhanced ÏòàÏ∏° (ÏµúÍ≥† Ï†ïÌôïÎèÑ)
                    'model_type': 'Level-wise Temporal Enhanced',
                    'key_features': ['Level-wise RA/WA', 'Temporal Phases', 'Device Degradation', 'FillRandom Optimization'],
                    'accuracy_factors': {
                        'level_wise_modeling': 0.95,
                        'temporal_phases': 0.9,
                        'device_degradation': 0.9,
                        'workload_specific': 0.98
                    }
                },
                'final_phase': {
                    'predicted_s_max': 250598,  # v4.2 Enhanced ÏòàÏ∏°
                    'model_type': 'Level-wise Temporal Enhanced',
                    'key_features': ['Level-wise RA/WA', 'Temporal Phases', 'Device Degradation', 'FillRandom Optimization'],
                    'accuracy_factors': {
                        'level_wise_modeling': 0.8,
                        'temporal_phases': 0.7,
                        'device_degradation': 0.8,
                        'workload_specific': 0.9
                    }
                }
            }
        }
        
        print("‚úÖ Î™®Îì† Î™®Îç∏Ïùò ÏòàÏ∏° Îç∞Ïù¥ÌÑ∞ Î°úÎìú ÏôÑÎ£å")
        return model_predictions
    
    def evaluate_all_models_by_phases(self):
        """Íµ¨Í∞ÑÎ≥Ñ Î™®Îì† Î™®Îç∏ ÌèâÍ∞Ä"""
        print("üìä Íµ¨Í∞ÑÎ≥Ñ Î™®Îì† Î™®Îç∏ ÌèâÍ∞Ä Ï§ë...")
        
        evaluation_results = {}
        
        for phase_name in ['initial_phase', 'middle_phase', 'final_phase']:
            actual_data = self.phase_b_actual_data[phase_name]
            actual_qps = actual_data['avg_qps']
            
            phase_results = {
                'actual_qps': actual_qps,
                'actual_characteristics': actual_data['characteristics'],
                'model_evaluations': {}
            }
            
            # Í∞Å Î™®Îç∏Î≥Ñ ÌèâÍ∞Ä
            for model_name, model_data in self.model_predictions.items():
                model_prediction = model_data[phase_name]
                predicted_s_max = model_prediction['predicted_s_max']
                
                # Ï†ïÌôïÎèÑ Í≥ÑÏÇ∞
                accuracy = (1 - abs(predicted_s_max - actual_qps) / actual_qps) * 100
                
                # Ïò§Ï∞®Ïú® Í≥ÑÏÇ∞
                error_rate = abs(predicted_s_max - actual_qps) / actual_qps * 100
                
                # Î∞©Ìñ•ÏÑ± Î∂ÑÏÑù
                if predicted_s_max > actual_qps:
                    prediction_direction = "over_prediction"
                    direction_percent = ((predicted_s_max - actual_qps) / actual_qps) * 100
                else:
                    prediction_direction = "under_prediction"
                    direction_percent = ((actual_qps - predicted_s_max) / actual_qps) * 100
                
                # Î™®Îç∏ ÏÑ±Îä• Îì±Í∏â
                if accuracy >= 90:
                    performance_grade = "Excellent"
                elif accuracy >= 70:
                    performance_grade = "Good"
                elif accuracy >= 50:
                    performance_grade = "Fair"
                elif accuracy >= 30:
                    performance_grade = "Poor"
                else:
                    performance_grade = "Very Poor"
                
                phase_results['model_evaluations'][model_name] = {
                    'predicted_s_max': predicted_s_max,
                    'accuracy_percent': accuracy,
                    'error_rate_percent': error_rate,
                    'prediction_direction': prediction_direction,
                    'direction_percent': direction_percent,
                    'performance_grade': performance_grade,
                    'model_type': model_prediction['model_type'],
                    'key_features': model_prediction['key_features'],
                    'accuracy_factors': model_prediction['accuracy_factors']
                }
            
            evaluation_results[phase_name] = phase_results
        
        return evaluation_results
    
    def analyze_model_performance_patterns(self, evaluation_results):
        """Î™®Îç∏ ÏÑ±Îä• Ìå®ÌÑ¥ Î∂ÑÏÑù"""
        print("üìä Î™®Îç∏ ÏÑ±Îä• Ìå®ÌÑ¥ Î∂ÑÏÑù Ï§ë...")
        
        pattern_analysis = {}
        
        # Î™®Îç∏Î≥Ñ Ï†ÑÏ≤¥ ÏÑ±Îä• Î∂ÑÏÑù
        for model_name in self.model_predictions.keys():
            model_accuracies = []
            model_grades = []
            phase_performances = {}
            
            for phase_name, phase_results in evaluation_results.items():
                model_result = phase_results['model_evaluations'][model_name]
                accuracy = model_result['accuracy_percent']
                grade = model_result['performance_grade']
                
                model_accuracies.append(accuracy)
                model_grades.append(grade)
                phase_performances[phase_name] = {
                    'accuracy': accuracy,
                    'grade': grade,
                    'prediction_direction': model_result['prediction_direction']
                }
            
            # Î™®Îç∏Î≥Ñ ÌÜµÍ≥Ñ
            avg_accuracy = np.mean(model_accuracies)
            accuracy_std = np.std(model_accuracies)
            
            # ÏµúÍ≥†/ÏµúÏïÖ ÏÑ±Îä• Phase
            best_phase = max(phase_performances.items(), key=lambda x: x[1]['accuracy'])
            worst_phase = min(phase_performances.items(), key=lambda x: x[1]['accuracy'])
            
            # ÏùºÍ¥ÄÏÑ± Î∂ÑÏÑù
            consistency = 'high' if accuracy_std < 20 else 'medium' if accuracy_std < 50 else 'low'
            
            # ÏòàÏ∏° Ìé∏Ìñ• Î∂ÑÏÑù
            over_predictions = sum(1 for p in phase_performances.values() if p['prediction_direction'] == 'over_prediction')
            under_predictions = sum(1 for p in phase_performances.values() if p['prediction_direction'] == 'under_prediction')
            
            if over_predictions > under_predictions:
                prediction_bias = 'over_prediction'
            elif under_predictions > over_predictions:
                prediction_bias = 'under_prediction'
            else:
                prediction_bias = 'balanced'
            
            pattern_analysis[model_name] = {
                'average_accuracy': avg_accuracy,
                'accuracy_std': accuracy_std,
                'consistency': consistency,
                'best_phase': best_phase,
                'worst_phase': worst_phase,
                'prediction_bias': prediction_bias,
                'phase_performances': phase_performances
            }
        
        # Íµ¨Í∞ÑÎ≥Ñ Î™®Îç∏ ÏàúÏúÑ Î∂ÑÏÑù
        phase_rankings = {}
        for phase_name in evaluation_results.keys():
            phase_models = []
            for model_name, model_result in evaluation_results[phase_name]['model_evaluations'].items():
                phase_models.append((model_name, model_result['accuracy_percent']))
            
            # Ï†ïÌôïÎèÑ ÏàúÏúºÎ°ú Ï†ïÎ†¨
            phase_models.sort(key=lambda x: x[1], reverse=True)
            phase_rankings[phase_name] = phase_models
        
        pattern_analysis['phase_rankings'] = phase_rankings
        
        return pattern_analysis
    
    def generate_model_comparison_insights(self, evaluation_results, pattern_analysis):
        """Î™®Îç∏ ÎπÑÍµê Ïù∏ÏÇ¨Ïù¥Ìä∏ ÏÉùÏÑ±"""
        print("üìä Î™®Îç∏ ÎπÑÍµê Ïù∏ÏÇ¨Ïù¥Ìä∏ ÏÉùÏÑ± Ï§ë...")
        
        insights = {
            'overall_best_model': None,
            'phase_specific_winners': {},
            'model_strengths_weaknesses': {},
            'key_findings': [],
            'recommendations': []
        }
        
        # Ï†ÑÏ≤¥ ÏµúÍ≥† Î™®Îç∏ Í≤∞Ï†ï
        overall_scores = {}
        for model_name, analysis in pattern_analysis.items():
            if model_name != 'phase_rankings':
                overall_scores[model_name] = analysis['average_accuracy']
        
        best_model = max(overall_scores.items(), key=lambda x: x[1])
        insights['overall_best_model'] = {
            'model': best_model[0],
            'average_accuracy': best_model[1]
        }
        
        # Íµ¨Í∞ÑÎ≥Ñ Ïö∞Ïäπ Î™®Îç∏
        for phase_name, rankings in pattern_analysis['phase_rankings'].items():
            winner = rankings[0]  # Ï≤´ Î≤àÏß∏Í∞Ä ÏµúÍ≥† ÏÑ±Îä•
            insights['phase_specific_winners'][phase_name] = {
                'model': winner[0],
                'accuracy': winner[1]
            }
        
        # Î™®Îç∏Î≥Ñ Í∞ïÏ†êÍ≥º ÏïΩÏ†ê
        for model_name, analysis in pattern_analysis.items():
            if model_name != 'phase_rankings':
                strengths = []
                weaknesses = []
                
                # Í∞ïÏ†ê Î∂ÑÏÑù
                if analysis['average_accuracy'] > 70:
                    strengths.append("ÎÜíÏùÄ ÌèâÍ∑† Ï†ïÌôïÎèÑ")
                
                if analysis['consistency'] == 'high':
                    strengths.append("ÏùºÍ¥ÄÎêú ÏÑ±Îä•")
                
                if analysis['prediction_bias'] == 'balanced':
                    strengths.append("Í∑†ÌòïÏû°Ìûå ÏòàÏ∏°")
                
                best_phase_name = analysis['best_phase'][0]
                best_accuracy = analysis['best_phase'][1]['accuracy']
                if best_accuracy > 90:
                    strengths.append(f"{best_phase_name}ÏóêÏÑú Ïö∞ÏàòÌïú ÏÑ±Îä• ({best_accuracy:.1f}%)")
                
                # ÏïΩÏ†ê Î∂ÑÏÑù
                if analysis['average_accuracy'] < 50:
                    weaknesses.append("ÎÇÆÏùÄ ÌèâÍ∑† Ï†ïÌôïÎèÑ")
                
                if analysis['consistency'] == 'low':
                    weaknesses.append("Î∂àÏùºÍ¥ÄÌïú ÏÑ±Îä•")
                
                if analysis['prediction_bias'] != 'balanced':
                    weaknesses.append(f"{analysis['prediction_bias']} Ìé∏Ìñ•")
                
                worst_phase_name = analysis['worst_phase'][0]
                worst_accuracy = analysis['worst_phase'][1]['accuracy']
                if worst_accuracy < 30:
                    weaknesses.append(f"{worst_phase_name}ÏóêÏÑú ÎÇÆÏùÄ ÏÑ±Îä• ({worst_accuracy:.1f}%)")
                
                insights['model_strengths_weaknesses'][model_name] = {
                    'strengths': strengths,
                    'weaknesses': weaknesses
                }
        
        # Ï£ºÏöî Î∞úÍ≤¨ÏÇ¨Ìï≠
        insights['key_findings'] = [
            f"Ï†ÑÏ≤¥ ÏµúÍ≥† ÏÑ±Îä•: {insights['overall_best_model']['model']} ({insights['overall_best_model']['average_accuracy']:.1f}%)",
            f"Initial Phase ÏµúÍ≥†: {insights['phase_specific_winners']['initial_phase']['model']} ({insights['phase_specific_winners']['initial_phase']['accuracy']:.1f}%)",
            f"Middle Phase ÏµúÍ≥†: {insights['phase_specific_winners']['middle_phase']['model']} ({insights['phase_specific_winners']['middle_phase']['accuracy']:.1f}%)",
            f"Final Phase ÏµúÍ≥†: {insights['phase_specific_winners']['final_phase']['model']} ({insights['phase_specific_winners']['final_phase']['accuracy']:.1f}%)"
        ]
        
        # Í∂åÏû•ÏÇ¨Ìï≠
        insights['recommendations'] = [
            "Middle PhaseÏóêÏÑú Î™®Îì† Î™®Îç∏Ïù¥ ÏÉÅÎåÄÏ†ÅÏúºÎ°ú Ïö∞ÏàòÌïú ÏÑ±Îä•ÏùÑ Î≥¥ÏûÑ",
            "Initial PhaseÏôÄ Final PhaseÏóêÏÑú Î™®Îç∏ Í∞úÏÑ†Ïù¥ ÌïÑÏöî",
            "v4.2 Enhanced Î™®Îç∏Ïùò Level-wise Ï†ëÍ∑ºÎ≤ïÏù¥ Ïú†ÎßùÌï®",
            "Íµ¨Í∞ÑÎ≥Ñ ÌäπÏÑ±ÏùÑ Í≥†Î†§Ìïú ÌïòÏù¥Î∏åÎ¶¨Îìú Î™®Îç∏ Í∞úÎ∞ú Í∂åÏû•"
        ]
        
        return insights
    
    def create_comprehensive_visualization(self, evaluation_results, pattern_analysis, output_dir):
        """Ï¢ÖÌï© ÏãúÍ∞ÅÌôî ÏÉùÏÑ±"""
        print("üìä Ï¢ÖÌï© ÏãúÍ∞ÅÌôî ÏÉùÏÑ± Ï§ë...")
        
        # Liberation Serif Ìè∞Ìä∏ ÏÑ§Ï†ï
        plt.rcParams['font.family'] = 'Liberation Serif'
        plt.rcParams['axes.unicode_minus'] = False
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('Phase-based Model Evaluation: v4, v4.1, v4.2 Comparison', fontsize=16, fontweight='bold')
        
        # 1. Íµ¨Í∞ÑÎ≥Ñ Ï†ïÌôïÎèÑ ÎπÑÍµê
        ax1 = axes[0, 0]
        phases = list(evaluation_results.keys())
        models = list(self.model_predictions.keys())
        
        phase_labels = [p.replace('_phase', '').title() for p in phases]
        model_labels = ['v4', 'v4.1', 'v4.2']
        
        x = np.arange(len(phases))
        width = 0.25
        
        for i, model in enumerate(models):
            accuracies = []
            for phase in phases:
                accuracy = evaluation_results[phase]['model_evaluations'][model]['accuracy_percent']
                accuracies.append(accuracy)
            
            bars = ax1.bar(x + i*width, accuracies, width, label=model_labels[i], alpha=0.8)
            
            # Í∞í ÌëúÏãú
            for bar, acc in zip(bars, accuracies):
                height = bar.get_height()
                ax1.text(bar.get_x() + bar.get_width()/2., height + 1,
                        f'{acc:.1f}%', ha='center', va='bottom', fontsize=9)
        
        ax1.set_xlabel('Phase')
        ax1.set_ylabel('Accuracy (%)')
        ax1.set_title('Model Accuracy by Phase')
        ax1.set_xticks(x + width)
        ax1.set_xticklabels(phase_labels)
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.set_ylim(0, 110)
        
        # 2. Ïã§Ï†ú vs ÏòàÏ∏°Í∞í ÎπÑÍµê (Initial Phase)
        ax2 = axes[0, 1]
        actual_qps = [evaluation_results[phase]['actual_qps'] for phase in phases]
        
        for i, model in enumerate(models):
            predicted_values = []
            for phase in phases:
                predicted = evaluation_results[phase]['model_evaluations'][model]['predicted_s_max']
                predicted_values.append(predicted)
            
            ax2.plot(phase_labels, predicted_values, marker='o', label=f'{model_labels[i]} Predicted', linewidth=2)
        
        ax2.plot(phase_labels, actual_qps, marker='s', label='Actual QPS', linewidth=3, color='black')
        ax2.set_xlabel('Phase')
        ax2.set_ylabel('Throughput (ops/sec)')
        ax2.set_title('Actual vs Predicted Throughput')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. Î™®Îç∏Î≥Ñ ÌèâÍ∑† Ï†ïÌôïÎèÑ
        ax3 = axes[0, 2]
        avg_accuracies = []
        for model in models:
            avg_acc = pattern_analysis[model]['average_accuracy']
            avg_accuracies.append(avg_acc)
        
        colors = ['lightblue', 'lightgreen', 'lightcoral']
        bars = ax3.bar(model_labels, avg_accuracies, color=colors, alpha=0.8)
        
        for bar, acc in zip(bars, avg_accuracies):
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width()/2., height + 1,
                    f'{acc:.1f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')
        
        ax3.set_ylabel('Average Accuracy (%)')
        ax3.set_title('Overall Model Performance')
        ax3.grid(True, alpha=0.3)
        ax3.set_ylim(0, 100)
        
        # 4. Ïò§Ï∞®Ïú® Î∂ÑÏÑù
        ax4 = axes[1, 0]
        for i, model in enumerate(models):
            error_rates = []
            for phase in phases:
                error_rate = evaluation_results[phase]['model_evaluations'][model]['error_rate_percent']
                error_rates.append(error_rate)
            
            ax4.bar(x + i*width, error_rates, width, label=model_labels[i], alpha=0.8)
        
        ax4.set_xlabel('Phase')
        ax4.set_ylabel('Error Rate (%)')
        ax4.set_title('Model Error Rate by Phase')
        ax4.set_xticks(x + width)
        ax4.set_xticklabels(phase_labels)
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        # 5. ÏòàÏ∏° Î∞©Ìñ•ÏÑ± Î∂ÑÏÑù
        ax5 = axes[1, 1]
        direction_data = {'over': [], 'under': []}
        
        for model in models:
            over_count = 0
            under_count = 0
            for phase in phases:
                direction = evaluation_results[phase]['model_evaluations'][model]['prediction_direction']
                if direction == 'over_prediction':
                    over_count += 1
                else:
                    under_count += 1
            direction_data['over'].append(over_count)
            direction_data['under'].append(under_count)
        
        x_models = np.arange(len(model_labels))
        width = 0.35
        
        ax5.bar(x_models - width/2, direction_data['over'], width, label='Over-prediction', alpha=0.8, color='red')
        ax5.bar(x_models + width/2, direction_data['under'], width, label='Under-prediction', alpha=0.8, color='blue')
        
        ax5.set_xlabel('Model')
        ax5.set_ylabel('Number of Phases')
        ax5.set_title('Prediction Direction Analysis')
        ax5.set_xticks(x_models)
        ax5.set_xticklabels(model_labels)
        ax5.legend()
        ax5.grid(True, alpha=0.3)
        
        # 6. ÏÑ±Îä• Îì±Í∏â Î∂ÑÌè¨
        ax6 = axes[1, 2]
        grade_counts = {'Excellent': [], 'Good': [], 'Fair': [], 'Poor': [], 'Very Poor': []}
        
        for model in models:
            model_grades = {'Excellent': 0, 'Good': 0, 'Fair': 0, 'Poor': 0, 'Very Poor': 0}
            for phase in phases:
                grade = evaluation_results[phase]['model_evaluations'][model]['performance_grade']
                model_grades[grade] += 1
            
            for grade in grade_counts.keys():
                grade_counts[grade].append(model_grades[grade])
        
        bottom = np.zeros(len(model_labels))
        colors_grade = ['green', 'lightgreen', 'yellow', 'orange', 'red']
        
        for i, (grade, counts) in enumerate(grade_counts.items()):
            ax6.bar(model_labels, counts, bottom=bottom, label=grade, color=colors_grade[i], alpha=0.8)
            bottom += counts
        
        ax6.set_ylabel('Number of Phases')
        ax6.set_title('Performance Grade Distribution')
        ax6.legend()
        ax6.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # Ï†ÄÏû•
        output_file = os.path.join(output_dir, 'phase_based_model_evaluation_comprehensive.png')
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"‚úÖ Ï¢ÖÌï© ÏãúÍ∞ÅÌôî Ï†ÄÏû• ÏôÑÎ£å: {output_file}")
    
    def save_evaluation_results(self, evaluation_results, pattern_analysis, insights, output_dir):
        """ÌèâÍ∞Ä Í≤∞Í≥º Ï†ÄÏû•"""
        print("üíæ ÌèâÍ∞Ä Í≤∞Í≥º Ï†ÄÏû• Ï§ë...")
        
        comprehensive_report = {
            'evaluation_metadata': {
                'evaluation_date': datetime.now().isoformat(),
                'evaluator': 'Phase-based Model Evaluator',
                'models_evaluated': list(self.model_predictions.keys()),
                'phases_analyzed': list(evaluation_results.keys()),
                'evaluation_scope': 'v4, v4.1, v4.2 models across Initial, Middle, Final phases'
            },
            'phase_by_phase_evaluation': evaluation_results,
            'pattern_analysis': pattern_analysis,
            'comparative_insights': insights,
            'summary_statistics': self._generate_summary_statistics(evaluation_results, pattern_analysis)
        }
        
        # JSON Í≤∞Í≥º Ï†ÄÏû•
        json_file = os.path.join(output_dir, "phase_based_model_evaluation_comprehensive.json")
        with open(json_file, 'w') as f:
            json.dump(comprehensive_report, f, indent=2)
        
        # ÎßàÌÅ¨Îã§Ïö¥ Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ±
        report_file = os.path.join(output_dir, "phase_based_model_evaluation_comprehensive.md")
        self._generate_comprehensive_markdown_report(comprehensive_report, report_file)
        
        print(f"‚úÖ ÌèâÍ∞Ä Í≤∞Í≥º Ï†ÄÏû• ÏôÑÎ£å:")
        print(f"   - JSON: {json_file}")
        print(f"   - Report: {report_file}")
    
    def _generate_summary_statistics(self, evaluation_results, pattern_analysis):
        """ÏöîÏïΩ ÌÜµÍ≥Ñ ÏÉùÏÑ±"""
        summary = {
            'overall_best_model': None,
            'overall_worst_model': None,
            'most_consistent_model': None,
            'phase_difficulty_ranking': [],
            'model_performance_summary': {}
        }
        
        # Ï†ÑÏ≤¥ ÏµúÍ≥†/ÏµúÏïÖ Î™®Îç∏
        model_scores = {}
        model_consistencies = {}
        
        for model_name, analysis in pattern_analysis.items():
            if model_name != 'phase_rankings':
                model_scores[model_name] = analysis['average_accuracy']
                model_consistencies[model_name] = analysis['accuracy_std']
        
        best_model = max(model_scores.items(), key=lambda x: x[1])
        worst_model = min(model_scores.items(), key=lambda x: x[1])
        most_consistent = min(model_consistencies.items(), key=lambda x: x[1])
        
        summary['overall_best_model'] = {'model': best_model[0], 'accuracy': best_model[1]}
        summary['overall_worst_model'] = {'model': worst_model[0], 'accuracy': worst_model[1]}
        summary['most_consistent_model'] = {'model': most_consistent[0], 'std': most_consistent[1]}
        
        # Íµ¨Í∞ÑÎ≥Ñ ÎÇúÏù¥ÎèÑ ÏàúÏúÑ (ÌèâÍ∑† Ï†ïÌôïÎèÑ Í∏∞Ï§Ä)
        phase_difficulties = []
        for phase_name, phase_results in evaluation_results.items():
            phase_accuracies = []
            for model_result in phase_results['model_evaluations'].values():
                phase_accuracies.append(model_result['accuracy_percent'])
            
            avg_phase_accuracy = np.mean(phase_accuracies)
            phase_difficulties.append((phase_name, avg_phase_accuracy))
        
        phase_difficulties.sort(key=lambda x: x[1], reverse=True)  # ÎÜíÏùÄ Ï†ïÌôïÎèÑ = Ïâ¨Ïö¥ Íµ¨Í∞Ñ
        summary['phase_difficulty_ranking'] = phase_difficulties
        
        # Î™®Îç∏Î≥Ñ ÏÑ±Îä• ÏöîÏïΩ
        for model_name, analysis in pattern_analysis.items():
            if model_name != 'phase_rankings':
                summary['model_performance_summary'][model_name] = {
                    'average_accuracy': analysis['average_accuracy'],
                    'consistency': analysis['consistency'],
                    'best_phase': analysis['best_phase'][0],
                    'worst_phase': analysis['worst_phase'][0],
                    'prediction_bias': analysis['prediction_bias']
                }
        
        return summary
    
    def _generate_comprehensive_markdown_report(self, comprehensive_report, report_file):
        """Ï¢ÖÌï© ÎßàÌÅ¨Îã§Ïö¥ Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ±"""
        with open(report_file, 'w') as f:
            f.write("# Phase-based Model Evaluation: v4, v4.1, v4.2 Comprehensive Analysis\n\n")
            f.write(f"**ÌèâÍ∞Ä ÏùºÏãú**: {comprehensive_report['evaluation_metadata']['evaluation_date']}\n")
            f.write(f"**ÌèâÍ∞Ä Î™®Îç∏**: {', '.join(comprehensive_report['evaluation_metadata']['models_evaluated'])}\n")
            f.write(f"**Î∂ÑÏÑù Íµ¨Í∞Ñ**: {', '.join(comprehensive_report['evaluation_metadata']['phases_analyzed'])}\n\n")
            
            # Ï†ÑÏ≤¥ ÏöîÏïΩ
            summary = comprehensive_report['summary_statistics']
            f.write("## üìä Executive Summary\n\n")
            f.write(f"**Ï†ÑÏ≤¥ ÏµúÍ≥† Î™®Îç∏**: {summary['overall_best_model']['model']} ({summary['overall_best_model']['accuracy']:.1f}%)\n")
            f.write(f"**Ï†ÑÏ≤¥ ÏµúÏïÖ Î™®Îç∏**: {summary['overall_worst_model']['model']} ({summary['overall_worst_model']['accuracy']:.1f}%)\n")
            f.write(f"**Í∞ÄÏû• ÏùºÍ¥ÄÎêú Î™®Îç∏**: {summary['most_consistent_model']['model']} (ÌëúÏ§ÄÌé∏Ï∞®: {summary['most_consistent_model']['std']:.1f}%)\n\n")
            
            # Íµ¨Í∞ÑÎ≥Ñ ÌèâÍ∞Ä
            f.write("## üîç Phase-by-Phase Evaluation\n\n")
            
            for phase_name, phase_results in comprehensive_report['phase_by_phase_evaluation'].items():
                f.write(f"### {phase_name.replace('_', ' ').title()}\n")
                f.write(f"**Ïã§Ï†ú QPS**: {phase_results['actual_qps']:,} ops/sec\n\n")
                
                f.write("| Model | Predicted S_max | Accuracy | Error Rate | Grade | Direction |\n")
                f.write("|-------|----------------|----------|------------|-------|----------|\n")
                
                for model_name, model_result in phase_results['model_evaluations'].items():
                    model_display = model_name.replace('_', '.').replace('v4.2.enhanced', 'v4.2')
                    f.write(f"| {model_display} | "
                           f"{model_result['predicted_s_max']:,} | "
                           f"{model_result['accuracy_percent']:.1f}% | "
                           f"{model_result['error_rate_percent']:.1f}% | "
                           f"{model_result['performance_grade']} | "
                           f"{model_result['prediction_direction']} |\n")
                
                f.write("\n")
            
            # Î™®Îç∏Î≥Ñ ÏÑ±Îä• Ìå®ÌÑ¥
            f.write("## üìà Model Performance Patterns\n\n")
            
            for model_name, analysis in comprehensive_report['pattern_analysis'].items():
                if model_name != 'phase_rankings':
                    model_display = model_name.replace('_', '.').replace('v4.2.enhanced', 'v4.2')
                    f.write(f"### {model_display.upper()}\n")
                    f.write(f"**ÌèâÍ∑† Ï†ïÌôïÎèÑ**: {analysis['average_accuracy']:.1f}%\n")
                    f.write(f"**ÏùºÍ¥ÄÏÑ±**: {analysis['consistency']}\n")
                    f.write(f"**ÏµúÍ≥† ÏÑ±Îä• Íµ¨Í∞Ñ**: {analysis['best_phase'][0]} ({analysis['best_phase'][1]['accuracy']:.1f}%)\n")
                    f.write(f"**ÏµúÏïÖ ÏÑ±Îä• Íµ¨Í∞Ñ**: {analysis['worst_phase'][0]} ({analysis['worst_phase'][1]['accuracy']:.1f}%)\n")
                    f.write(f"**ÏòàÏ∏° Ìé∏Ìñ•**: {analysis['prediction_bias']}\n\n")
            
            # Ï£ºÏöî Ïù∏ÏÇ¨Ïù¥Ìä∏
            insights = comprehensive_report['comparative_insights']
            f.write("## üí° Key Insights\n\n")
            
            for finding in insights['key_findings']:
                f.write(f"- {finding}\n")
            f.write("\n")
            
            # Î™®Îç∏Î≥Ñ Í∞ïÏ†êÍ≥º ÏïΩÏ†ê
            f.write("## ‚öñÔ∏è Model Strengths and Weaknesses\n\n")
            
            for model_name, sw in insights['model_strengths_weaknesses'].items():
                model_display = model_name.replace('_', '.').replace('v4.2.enhanced', 'v4.2')
                f.write(f"### {model_display.upper()}\n")
                f.write("**Í∞ïÏ†ê**:\n")
                for strength in sw['strengths']:
                    f.write(f"- {strength}\n")
                f.write("\n**ÏïΩÏ†ê**:\n")
                for weakness in sw['weaknesses']:
                    f.write(f"- {weakness}\n")
                f.write("\n")
            
            # Í∂åÏû•ÏÇ¨Ìï≠
            f.write("## üéØ Recommendations\n\n")
            for recommendation in insights['recommendations']:
                f.write(f"- {recommendation}\n")
            f.write("\n")

def main():
    """Î©îÏù∏ Ïã§Ìñâ Ìï®Ïàò"""
    print("üöÄ Phase-based Model Evaluation ÏãúÏûë")
    print("=" * 70)
    
    # ÌèâÍ∞ÄÍ∏∞ ÏÉùÏÑ±
    evaluator = PhaseBasedModelEvaluator()
    
    # Íµ¨Í∞ÑÎ≥Ñ Î™®Îì† Î™®Îç∏ ÌèâÍ∞Ä
    evaluation_results = evaluator.evaluate_all_models_by_phases()
    
    # Î™®Îç∏ ÏÑ±Îä• Ìå®ÌÑ¥ Î∂ÑÏÑù
    pattern_analysis = evaluator.analyze_model_performance_patterns(evaluation_results)
    
    # Î™®Îç∏ ÎπÑÍµê Ïù∏ÏÇ¨Ïù¥Ìä∏ ÏÉùÏÑ±
    insights = evaluator.generate_model_comparison_insights(evaluation_results, pattern_analysis)
    
    # Ï¢ÖÌï© ÏãúÍ∞ÅÌôî ÏÉùÏÑ±
    evaluator.create_comprehensive_visualization(evaluation_results, pattern_analysis, evaluator.results_dir)
    
    # Í≤∞Í≥º Ï†ÄÏû•
    evaluator.save_evaluation_results(evaluation_results, pattern_analysis, insights, evaluator.results_dir)
    
    # Í≤∞Í≥º ÏöîÏïΩ Ï∂úÎ†•
    print("\n" + "=" * 70)
    print("üìä Phase-based Model Evaluation Summary")
    print("=" * 70)
    
    print(f"Overall Best Model: {insights['overall_best_model']['model']} ({insights['overall_best_model']['average_accuracy']:.1f}%)")
    print()
    
    print("Phase-specific Winners:")
    for phase_name, winner in insights['phase_specific_winners'].items():
        print(f"  {phase_name.replace('_', ' ').title()}: {winner['model']} ({winner['accuracy']:.1f}%)")
    print()
    
    print("Model Performance Summary:")
    for model_name in ['v4_model', 'v4_1_temporal', 'v4_2_enhanced']:
        model_display = model_name.replace('_', '.').replace('v4.2.enhanced', 'v4.2')
        avg_acc = pattern_analysis[model_name]['average_accuracy']
        consistency = pattern_analysis[model_name]['consistency']
        print(f"  {model_display.upper()}: {avg_acc:.1f}% (consistency: {consistency})")
    
    print("\n‚úÖ Phase-based Model Evaluation ÏôÑÎ£å!")
    print("=" * 70)

if __name__ == "__main__":
    main()
