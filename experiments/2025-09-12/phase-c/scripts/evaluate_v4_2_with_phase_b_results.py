#!/usr/bin/env python3
"""
Phase-B ì‹¤í—˜ ê²°ê³¼ë¥¼ ì´ìš©í•œ v4.2 ëª¨ë¸ í‰ê°€
ì‹¤ì œ ì¸¡ì • ë°ì´í„°ì™€ ëª¨ë¸ ì˜ˆì¸¡ê°’ì˜ ê°ê´€ì  ë¹„êµ ë¶„ì„
"""

import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import os

class V4_2PhaseBEvaluator:
    """Phase-B ì‹¤í—˜ ê²°ê³¼ ê¸°ë°˜ v4.2 ëª¨ë¸ í‰ê°€ê¸°"""
    
    def __init__(self, results_dir="results"):
        self.results_dir = results_dir
        os.makedirs(self.results_dir, exist_ok=True)
        
        # Phase-B ì‹¤ì œ ì¸¡ì • ë°ì´í„°
        self.phase_b_actual_data = self._load_phase_b_actual_data()
        
        # v4.2 ëª¨ë¸ ì˜ˆì¸¡ ë°ì´í„°
        self.v4_2_predictions = self._load_v4_2_predictions()
        
    def _load_phase_b_actual_data(self):
        """Phase-B ì‹¤ì œ ì¸¡ì • ë°ì´í„° ë¡œë“œ"""
        print("ğŸ“Š Phase-B ì‹¤ì œ ì¸¡ì • ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        # ì‹¤ì œ Phase-B ì„±ëŠ¥ ë°ì´í„° (ì„±ëŠ¥ ê¸°ë°˜ êµ¬ê°„ ë¶„í•  ê²°ê³¼)
        phase_b_data = {
            'initial_phase': {
                'duration_hours': 0.14,
                'sample_count': 52,
                'avg_qps': 138769,  # ì‹¤ì œ ì¸¡ì •ëœ í‰ê·  QPS
                'performance_stats': {
                    'avg_write_rate': 65.97,  # MB/s
                    'max_write_rate': 280.18,
                    'min_write_rate': 46.74,
                    'std_write_rate': 35.49,
                    'cv': 0.538  # ë³€ë™ê³„ìˆ˜
                },
                'characteristics': {
                    'stability': 'low',
                    'trend': 'decreasing',
                    'performance_level': 'high',
                    'change_intensity': 'low'
                }
            },
            'middle_phase': {
                'duration_hours': 31.79,
                'sample_count': 11443,
                'avg_qps': 114472,  # ì‹¤ì œ ì¸¡ì •ëœ í‰ê·  QPS
                'performance_stats': {
                    'avg_write_rate': 16.95,  # MB/s
                    'max_write_rate': 47.05,
                    'min_write_rate': 13.84,
                    'std_write_rate': 4.61,
                    'cv': 0.272  # ë³€ë™ê³„ìˆ˜
                },
                'characteristics': {
                    'stability': 'medium',
                    'trend': 'stable',
                    'performance_level': 'medium',
                    'change_intensity': 'low'
                }
            },
            'final_phase': {
                'duration_hours': 64.68,
                'sample_count': 23280,
                'avg_qps': 109678,  # ì‹¤ì œ ì¸¡ì •ëœ í‰ê·  QPS
                'performance_stats': {
                    'avg_write_rate': 12.76,  # MB/s
                    'max_write_rate': 13.84,
                    'min_write_rate': 12.76,
                    'std_write_rate': 0.0,
                    'cv': 0.0  # ë³€ë™ê³„ìˆ˜ (ì•ˆì •í™”)
                },
                'characteristics': {
                    'stability': 'high',
                    'trend': 'stable',
                    'performance_level': 'low',
                    'change_intensity': 'none'
                }
            }
        }
        
        print("âœ… Phase-B ì‹¤ì œ ì¸¡ì • ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        return phase_b_data
    
    def _load_v4_2_predictions(self):
        """v4.2 ëª¨ë¸ ì˜ˆì¸¡ ë°ì´í„° ë¡œë“œ"""
        print("ğŸ“Š v4.2 ëª¨ë¸ ì˜ˆì¸¡ ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        # v4.2 Enhanced ëª¨ë¸ ì˜ˆì¸¡ê°’
        v4_2_predictions = {
            'initial_phase': {
                'predicted_s_max': 33132,  # ops/sec
                'level_wise_analysis': {
                    'L0': {'wa': 1.0, 'ra': 0.0, 'io_impact': 0.10},
                    'L1': {'wa': 1.1, 'ra': 0.1, 'io_impact': 0.20},
                    'L2': {'wa': 1.3, 'ra': 0.2, 'io_impact': 0.30},
                    'L3': {'wa': 1.5, 'ra': 0.3, 'io_impact': 0.20},
                    'L4': {'wa': 2.0, 'ra': 0.5, 'io_impact': 0.10},
                    'L5': {'wa': 2.5, 'ra': 0.8, 'io_impact': 0.05},
                    'L6': {'wa': 3.0, 'ra': 1.0, 'io_impact': 0.05}
                },
                'performance_factors': {
                    'performance_factor': 0.3,
                    'stability_factor': 0.2,
                    'io_contention': 0.6,
                    'avg_wa': 1.3,
                    'avg_ra': 0.2
                }
            },
            'middle_phase': {
                'predicted_s_max': 119002,  # ops/sec
                'level_wise_analysis': {
                    'L0': {'wa': 1.0, 'ra': 0.0, 'io_impact': 0.10},
                    'L1': {'wa': 1.2, 'ra': 0.2, 'io_impact': 0.20},
                    'L2': {'wa': 2.5, 'ra': 0.8, 'io_impact': 0.40},
                    'L3': {'wa': 3.5, 'ra': 1.2, 'io_impact': 0.20},
                    'L4': {'wa': 4.0, 'ra': 1.5, 'io_impact': 0.10},
                    'L5': {'wa': 4.5, 'ra': 1.8, 'io_impact': 0.05},
                    'L6': {'wa': 5.0, 'ra': 2.0, 'io_impact': 0.05}
                },
                'performance_factors': {
                    'performance_factor': 0.6,
                    'stability_factor': 0.5,
                    'io_contention': 0.8,
                    'avg_wa': 2.4,
                    'avg_ra': 0.8
                }
            },
            'final_phase': {
                'predicted_s_max': 250598,  # ops/sec
                'level_wise_analysis': {
                    'L0': {'wa': 1.0, 'ra': 0.0, 'io_impact': 0.10},
                    'L1': {'wa': 1.3, 'ra': 0.3, 'io_impact': 0.20},
                    'L2': {'wa': 3.0, 'ra': 1.0, 'io_impact': 0.40},
                    'L3': {'wa': 4.0, 'ra': 1.5, 'io_impact': 0.20},
                    'L4': {'wa': 5.0, 'ra': 2.0, 'io_impact': 0.10},
                    'L5': {'wa': 5.5, 'ra': 2.2, 'io_impact': 0.05},
                    'L6': {'wa': 6.0, 'ra': 2.5, 'io_impact': 0.05}
                },
                'performance_factors': {
                    'performance_factor': 0.9,
                    'stability_factor': 0.8,
                    'io_contention': 0.9,
                    'avg_wa': 3.2,
                    'avg_ra': 1.1
                }
            }
        }
        
        print("âœ… v4.2 ëª¨ë¸ ì˜ˆì¸¡ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        return v4_2_predictions
    
    def evaluate_model_accuracy(self):
        """ëª¨ë¸ ì •í™•ë„ í‰ê°€"""
        print("ğŸ“Š v4.2 ëª¨ë¸ ì •í™•ë„ í‰ê°€ ì¤‘...")
        
        evaluation_results = {}
        
        for phase_name in ['initial_phase', 'middle_phase', 'final_phase']:
            actual_data = self.phase_b_actual_data[phase_name]
            predicted_data = self.v4_2_predictions[phase_name]
            
            actual_qps = actual_data['avg_qps']
            predicted_s_max = predicted_data['predicted_s_max']
            
            # ì •í™•ë„ ê³„ì‚°
            accuracy = (1 - abs(predicted_s_max - actual_qps) / actual_qps) * 100
            
            # ì˜¤ì°¨ìœ¨ ê³„ì‚°
            error_rate = abs(predicted_s_max - actual_qps) / actual_qps * 100
            
            # ë°©í–¥ì„± ë¶„ì„ (ê³¼ëŒ€/ê³¼ì†Œ ì˜ˆì¸¡)
            if predicted_s_max > actual_qps:
                prediction_direction = "over_prediction"
                direction_percent = ((predicted_s_max - actual_qps) / actual_qps) * 100
            else:
                prediction_direction = "under_prediction"
                direction_percent = ((actual_qps - predicted_s_max) / actual_qps) * 100
            
            evaluation_results[phase_name] = {
                'actual_qps': actual_qps,
                'predicted_s_max': predicted_s_max,
                'accuracy_percent': accuracy,
                'error_rate_percent': error_rate,
                'prediction_direction': prediction_direction,
                'direction_percent': direction_percent,
                'performance_factors': predicted_data['performance_factors']
            }
        
        return evaluation_results
    
    def analyze_prediction_patterns(self, evaluation_results):
        """ì˜ˆì¸¡ íŒ¨í„´ ë¶„ì„"""
        print("ğŸ“Š ì˜ˆì¸¡ íŒ¨í„´ ë¶„ì„ ì¤‘...")
        
        pattern_analysis = {}
        
        # ì‹œê¸°ë³„ ì •í™•ë„ íŠ¸ë Œë“œ
        accuracies = [evaluation_results[phase]['accuracy_percent'] for phase in evaluation_results.keys()]
        phases = list(evaluation_results.keys())
        
        # ì •í™•ë„ ë³€í™” íŒ¨í„´
        accuracy_trend = 'increasing' if accuracies[1] > accuracies[0] and accuracies[2] > accuracies[1] else 'variable'
        
        # ì˜ˆì¸¡ ë°©í–¥ì„± ë¶„ì„
        over_predictions = sum(1 for result in evaluation_results.values() if result['prediction_direction'] == 'over_prediction')
        under_predictions = sum(1 for result in evaluation_results.values() if result['prediction_direction'] == 'under_prediction')
        
        # í‰ê·  ì •í™•ë„
        avg_accuracy = np.mean(accuracies)
        
        # í‘œì¤€ í¸ì°¨
        accuracy_std = np.std(accuracies)
        
        pattern_analysis = {
            'accuracy_trend': accuracy_trend,
            'over_predictions': over_predictions,
            'under_predictions': under_predictions,
            'average_accuracy': avg_accuracy,
            'accuracy_std': accuracy_std,
            'accuracy_consistency': 'high' if accuracy_std < 20 else 'medium' if accuracy_std < 50 else 'low',
            'prediction_bias': 'over_prediction' if over_predictions > under_predictions else 'under_prediction' if under_predictions > over_predictions else 'balanced'
        }
        
        return pattern_analysis
    
    def evaluate_model_strengths_weaknesses(self, evaluation_results):
        """ëª¨ë¸ ê°•ì ê³¼ ì•½ì  í‰ê°€"""
        print("ğŸ“Š ëª¨ë¸ ê°•ì ê³¼ ì•½ì  í‰ê°€ ì¤‘...")
        
        strengths = []
        weaknesses = []
        
        # ì •í™•ë„ ê¸°ë°˜ í‰ê°€
        accuracies = [result['accuracy_percent'] for result in evaluation_results.values()]
        
        if any(acc > 90 for acc in accuracies):
            strengths.append("ì¼ë¶€ ì‹œê¸°ì—ì„œ ë†’ì€ ì •í™•ë„ ë‹¬ì„± (90% ì´ìƒ)")
        
        if any(acc < 50 for acc in accuracies):
            weaknesses.append("ì¼ë¶€ ì‹œê¸°ì—ì„œ ë‚®ì€ ì •í™•ë„ (50% ë¯¸ë§Œ)")
        
        # ì˜ˆì¸¡ ë°©í–¥ì„± ë¶„ì„
        over_predictions = sum(1 for result in evaluation_results.values() if result['prediction_direction'] == 'over_prediction')
        under_predictions = sum(1 for result in evaluation_results.values() if result['prediction_direction'] == 'under_prediction')
        
        if over_predictions > under_predictions:
            weaknesses.append("ê³¼ëŒ€ ì˜ˆì¸¡ ê²½í–¥ (ì‹¤ì œë³´ë‹¤ ë†’ê²Œ ì˜ˆì¸¡)")
        elif under_predictions > over_predictions:
            weaknesses.append("ê³¼ì†Œ ì˜ˆì¸¡ ê²½í–¥ (ì‹¤ì œë³´ë‹¤ ë‚®ê²Œ ì˜ˆì¸¡)")
        else:
            strengths.append("ê· í˜•ì¡íŒ ì˜ˆì¸¡ ë°©í–¥ì„±")
        
        # ì‹œê¸°ë³„ ì„±ëŠ¥ ë¶„ì„
        phase_performances = [(phase, result['accuracy_percent']) for phase, result in evaluation_results.items()]
        best_phase = max(phase_performances, key=lambda x: x[1])
        worst_phase = min(phase_performances, key=lambda x: x[1])
        
        strengths.append(f"{best_phase[0]}ì—ì„œ ìµœê³  ì„±ëŠ¥ ({best_phase[1]:.1f}% ì •í™•ë„)")
        weaknesses.append(f"{worst_phase[0]}ì—ì„œ ê°œì„  í•„ìš” ({worst_phase[1]:.1f}% ì •í™•ë„)")
        
        return {
            'strengths': strengths,
            'weaknesses': weaknesses
        }
    
    def generate_evaluation_report(self, evaluation_results, pattern_analysis, strengths_weaknesses):
        """í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("ğŸ“Š í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        report = {
            'evaluation_metadata': {
                'evaluation_date': datetime.now().isoformat(),
                'evaluator': 'Phase-B Experimental Results',
                'model_version': 'v4.2_enhanced_level_wise_temporal',
                'evaluation_scope': 'Phase-B experimental data comparison'
            },
            'phase_by_phase_evaluation': evaluation_results,
            'pattern_analysis': pattern_analysis,
            'strengths_weaknesses': strengths_weaknesses,
            'overall_assessment': self._generate_overall_assessment(evaluation_results, pattern_analysis)
        }
        
        return report
    
    def _generate_overall_assessment(self, evaluation_results, pattern_analysis):
        """ì „ì²´ í‰ê°€ ìƒì„±"""
        avg_accuracy = pattern_analysis['average_accuracy']
        
        if avg_accuracy > 80:
            assessment_level = "Excellent"
            assessment_description = "ëª¨ë¸ì´ ë§¤ìš° ë†’ì€ ì •í™•ë„ë¥¼ ë³´ì´ë©° ì‹¤ìš©ì ìœ¼ë¡œ í™œìš© ê°€ëŠ¥"
        elif avg_accuracy > 60:
            assessment_level = "Good"
            assessment_description = "ëª¨ë¸ì´ ì–‘í˜¸í•œ ì •í™•ë„ë¥¼ ë³´ì´ë©° ì¼ë¶€ ê°œì„  í›„ í™œìš© ê°€ëŠ¥"
        elif avg_accuracy > 40:
            assessment_level = "Fair"
            assessment_description = "ëª¨ë¸ì´ ë³´í†µ ìˆ˜ì¤€ì˜ ì •í™•ë„ë¥¼ ë³´ì´ë©° ìƒë‹¹í•œ ê°œì„  í•„ìš”"
        else:
            assessment_level = "Poor"
            assessment_description = "ëª¨ë¸ì˜ ì •í™•ë„ê°€ ë‚®ìœ¼ë©° ëŒ€í­ì ì¸ ê°œì„  í•„ìš”"
        
        return {
            'assessment_level': assessment_level,
            'assessment_description': assessment_description,
            'average_accuracy': avg_accuracy,
            'recommendation': self._generate_recommendation(avg_accuracy, pattern_analysis)
        }
    
    def _generate_recommendation(self, avg_accuracy, pattern_analysis):
        """ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if avg_accuracy < 70:
            recommendations.append("ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¬ì¡°ì • í•„ìš”")
        
        if pattern_analysis['prediction_bias'] == 'over_prediction':
            recommendations.append("ê³¼ëŒ€ ì˜ˆì¸¡ ê²½í–¥ ë³´ì • í•„ìš”")
        elif pattern_analysis['prediction_bias'] == 'under_prediction':
            recommendations.append("ê³¼ì†Œ ì˜ˆì¸¡ ê²½í–¥ ë³´ì • í•„ìš”")
        
        if pattern_analysis['accuracy_consistency'] == 'low':
            recommendations.append("ì‹œê¸°ë³„ ì •í™•ë„ ì¼ê´€ì„± ê°œì„  í•„ìš”")
        
        if not recommendations:
            recommendations.append("ëª¨ë¸ì´ ì–‘í˜¸í•œ ì„±ëŠ¥ì„ ë³´ì´ë©° í˜„ì¬ ìƒíƒœ ìœ ì§€ ê¶Œì¥")
        
        return recommendations
    
    def create_visualization(self, evaluation_results, output_dir):
        """ì‹œê°í™” ìƒì„±"""
        print("ğŸ“Š í‰ê°€ ê²°ê³¼ ì‹œê°í™” ìƒì„± ì¤‘...")
        
        # Liberation Serif í°íŠ¸ ì„¤ì •
        plt.rcParams['font.family'] = 'Liberation Serif'
        plt.rcParams['axes.unicode_minus'] = False
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('V4.2 Model Evaluation with Phase-B Results', fontsize=16, fontweight='bold')
        
        # 1. ì‹¤ì œ vs ì˜ˆì¸¡ê°’ ë¹„êµ
        ax1 = axes[0, 0]
        phases = list(evaluation_results.keys())
        actual_values = [evaluation_results[phase]['actual_qps'] for phase in phases]
        predicted_values = [evaluation_results[phase]['predicted_s_max'] for phase in phases]
        
        x = np.arange(len(phases))
        width = 0.35
        
        bars1 = ax1.bar(x - width/2, actual_values, width, label='Actual QPS', alpha=0.8, color='skyblue')
        bars2 = ax1.bar(x + width/2, predicted_values, width, label='Predicted S_max', alpha=0.8, color='lightcoral')
        
        ax1.set_xlabel('Phase')
        ax1.set_ylabel('Throughput (ops/sec)')
        ax1.set_title('Actual vs Predicted Throughput')
        ax1.set_xticks(x)
        ax1.set_xticklabels([p.replace('_', ' ').title() for p in phases])
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # ê°’ í‘œì‹œ
        for bar in bars1:
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                    f'{height:,.0f}', ha='center', va='bottom', fontsize=9)
        for bar in bars2:
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                    f'{height:,.0f}', ha='center', va='bottom', fontsize=9)
        
        # 2. ì •í™•ë„ ë§‰ëŒ€ ê·¸ë˜í”„
        ax2 = axes[0, 1]
        accuracies = [evaluation_results[phase]['accuracy_percent'] for phase in phases]
        colors = ['green' if acc > 70 else 'orange' if acc > 40 else 'red' for acc in accuracies]
        
        bars = ax2.bar([p.replace('_', ' ').title() for p in phases], accuracies, color=colors, alpha=0.7)
        ax2.set_ylabel('Accuracy (%)')
        ax2.set_title('Model Accuracy by Phase')
        ax2.set_ylim(0, 100)
        ax2.grid(True, alpha=0.3)
        
        # ì •í™•ë„ ê°’ í‘œì‹œ
        for bar, acc in zip(bars, accuracies):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + 1,
                    f'{acc:.1f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')
        
        # 3. ì˜¤ì°¨ìœ¨ ë¶„ì„
        ax3 = axes[1, 0]
        error_rates = [evaluation_results[phase]['error_rate_percent'] for phase in phases]
        direction_colors = ['red' if evaluation_results[phase]['prediction_direction'] == 'over_prediction' else 'blue' for phase in phases]
        
        bars = ax3.bar([p.replace('_', ' ').title() for p in phases], error_rates, color=direction_colors, alpha=0.7)
        ax3.set_ylabel('Error Rate (%)')
        ax3.set_title('Prediction Error Rate by Phase')
        ax3.grid(True, alpha=0.3)
        
        # ì˜¤ì°¨ìœ¨ ê°’ í‘œì‹œ
        for bar, err in zip(bars, error_rates):
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width()/2., height + height*0.02,
                    f'{err:.1f}%', ha='center', va='bottom', fontsize=10)
        
        # 4. ì„±ëŠ¥ ì¸ì ë¶„ì„
        ax4 = axes[1, 1]
        performance_factors = ['Performance', 'Stability', 'IO Contention']
        
        # ê° ì‹œê¸°ë³„ ì„±ëŠ¥ ì¸ì ë°ì´í„°
        initial_factors = [
            evaluation_results['initial_phase']['performance_factors']['performance_factor'],
            evaluation_results['initial_phase']['performance_factors']['stability_factor'],
            evaluation_results['initial_phase']['performance_factors']['io_contention']
        ]
        
        middle_factors = [
            evaluation_results['middle_phase']['performance_factors']['performance_factor'],
            evaluation_results['middle_phase']['performance_factors']['stability_factor'],
            evaluation_results['middle_phase']['performance_factors']['io_contention']
        ]
        
        final_factors = [
            evaluation_results['final_phase']['performance_factors']['performance_factor'],
            evaluation_results['final_phase']['performance_factors']['stability_factor'],
            evaluation_results['final_phase']['performance_factors']['io_contention']
        ]
        
        x = np.arange(len(performance_factors))
        width = 0.25
        
        ax4.bar(x - width, initial_factors, width, label='Initial', alpha=0.8, color='lightblue')
        ax4.bar(x, middle_factors, width, label='Middle', alpha=0.8, color='lightgreen')
        ax4.bar(x + width, final_factors, width, label='Final', alpha=0.8, color='lightcoral')
        
        ax4.set_xlabel('Performance Factors')
        ax4.set_ylabel('Factor Value')
        ax4.set_title('Performance Factors by Phase')
        ax4.set_xticks(x)
        ax4.set_xticklabels(performance_factors)
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ì €ì¥
        output_file = os.path.join(output_dir, 'v4_2_phase_b_evaluation.png')
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… ì‹œê°í™” ì €ì¥ ì™„ë£Œ: {output_file}")
    
    def save_evaluation_results(self, evaluation_report, output_dir):
        """í‰ê°€ ê²°ê³¼ ì €ì¥"""
        print("ğŸ’¾ í‰ê°€ ê²°ê³¼ ì €ì¥ ì¤‘...")
        
        # JSON ê²°ê³¼ ì €ì¥
        json_file = os.path.join(output_dir, "v4_2_phase_b_evaluation_results.json")
        with open(json_file, 'w') as f:
            json.dump(evaluation_report, f, indent=2)
        
        # ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±
        report_file = os.path.join(output_dir, "v4_2_phase_b_evaluation_report.md")
        self._generate_markdown_report(evaluation_report, report_file)
        
        print(f"âœ… í‰ê°€ ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
        print(f"   - JSON: {json_file}")
        print(f"   - Report: {report_file}")
    
    def _generate_markdown_report(self, evaluation_report, report_file):
        """ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±"""
        with open(report_file, 'w') as f:
            f.write("# V4.2 Model Evaluation with Phase-B Results\n\n")
            f.write(f"**í‰ê°€ ì¼ì‹œ**: {evaluation_report['evaluation_metadata']['evaluation_date']}\n")
            f.write(f"**ëª¨ë¸ ë²„ì „**: {evaluation_report['evaluation_metadata']['model_version']}\n\n")
            
            # ì „ì²´ í‰ê°€
            overall = evaluation_report['overall_assessment']
            f.write("## Overall Assessment\n\n")
            f.write(f"**í‰ê°€ ë“±ê¸‰**: {overall['assessment_level']}\n")
            f.write(f"**í‰ê°€ ì„¤ëª…**: {overall['assessment_description']}\n")
            f.write(f"**í‰ê·  ì •í™•ë„**: {overall['average_accuracy']:.1f}%\n\n")
            
            # ì‹œê¸°ë³„ í‰ê°€
            f.write("## Phase-by-Phase Evaluation\n\n")
            f.write("| Phase | Actual QPS | Predicted S_max | Accuracy | Error Rate | Direction |\n")
            f.write("|-------|------------|-----------------|----------|------------|----------|\n")
            
            for phase, result in evaluation_report['phase_by_phase_evaluation'].items():
                f.write(f"| {phase.replace('_', ' ').title()} | "
                       f"{result['actual_qps']:,.0f} | "
                       f"{result['predicted_s_max']:,.0f} | "
                       f"{result['accuracy_percent']:.1f}% | "
                       f"{result['error_rate_percent']:.1f}% | "
                       f"{result['prediction_direction']} |\n")
            
            f.write("\n")
            
            # íŒ¨í„´ ë¶„ì„
            pattern = evaluation_report['pattern_analysis']
            f.write("## Pattern Analysis\n\n")
            f.write(f"- **ì •í™•ë„ íŠ¸ë Œë“œ**: {pattern['accuracy_trend']}\n")
            f.write(f"- **ê³¼ëŒ€ ì˜ˆì¸¡**: {pattern['over_predictions']}ê°œ ì‹œê¸°\n")
            f.write(f"- **ê³¼ì†Œ ì˜ˆì¸¡**: {pattern['under_predictions']}ê°œ ì‹œê¸°\n")
            f.write(f"- **í‰ê·  ì •í™•ë„**: {pattern['average_accuracy']:.1f}%\n")
            f.write(f"- **ì •í™•ë„ í‘œì¤€í¸ì°¨**: {pattern['accuracy_std']:.1f}%\n")
            f.write(f"- **ì •í™•ë„ ì¼ê´€ì„±**: {pattern['accuracy_consistency']}\n")
            f.write(f"- **ì˜ˆì¸¡ í¸í–¥**: {pattern['prediction_bias']}\n\n")
            
            # ê°•ì ê³¼ ì•½ì 
            sw = evaluation_report['strengths_weaknesses']
            f.write("## Strengths and Weaknesses\n\n")
            f.write("### Strengths\n")
            for strength in sw['strengths']:
                f.write(f"- {strength}\n")
            f.write("\n### Weaknesses\n")
            for weakness in sw['weaknesses']:
                f.write(f"- {weakness}\n")
            f.write("\n")
            
            # ê¶Œì¥ì‚¬í•­
            f.write("## Recommendations\n\n")
            for recommendation in overall['recommendation']:
                f.write(f"- {recommendation}\n")
            f.write("\n")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ V4.2 Model Evaluation with Phase-B Results ì‹œì‘")
    print("=" * 60)
    
    # í‰ê°€ê¸° ìƒì„±
    evaluator = V4_2PhaseBEvaluator()
    
    # ëª¨ë¸ ì •í™•ë„ í‰ê°€
    evaluation_results = evaluator.evaluate_model_accuracy()
    
    # ì˜ˆì¸¡ íŒ¨í„´ ë¶„ì„
    pattern_analysis = evaluator.analyze_prediction_patterns(evaluation_results)
    
    # ê°•ì ê³¼ ì•½ì  í‰ê°€
    strengths_weaknesses = evaluator.evaluate_model_strengths_weaknesses(evaluation_results)
    
    # í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±
    evaluation_report = evaluator.generate_evaluation_report(evaluation_results, pattern_analysis, strengths_weaknesses)
    
    # ì‹œê°í™” ìƒì„±
    evaluator.create_visualization(evaluation_results, evaluator.results_dir)
    
    # ê²°ê³¼ ì €ì¥
    evaluator.save_evaluation_results(evaluation_report, evaluator.results_dir)
    
    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ“Š V4.2 Model Evaluation Summary")
    print("=" * 60)
    
    overall = evaluation_report['overall_assessment']
    print(f"Overall Assessment: {overall['assessment_level']}")
    print(f"Average Accuracy: {overall['average_accuracy']:.1f}%")
    print()
    
    for phase, result in evaluation_results.items():
        print(f"{phase.replace('_', ' ').title()}:")
        print(f"  - Actual QPS: {result['actual_qps']:,.0f}")
        print(f"  - Predicted S_max: {result['predicted_s_max']:,.0f}")
        print(f"  - Accuracy: {result['accuracy_percent']:.1f}%")
        print(f"  - Error Rate: {result['error_rate_percent']:.1f}%")
        print(f"  - Direction: {result['prediction_direction']}")
        print()
    
    print("âœ… V4.2 Model Evaluation ì™„ë£Œ!")
    print("=" * 60)

if __name__ == "__main__":
    main()
