#!/usr/bin/env python3
"""
Phase-B ë¡œê·¸ì—ì„œ ë ˆë²¨ë³„ ì»´íŒ©ì…˜ í†µê³„ ì¶”ì¶œ
ì‹œê¸°ë³„ ë ˆë²¨ë³„ RA/WA ë³€í™” íŒ¨í„´ ë¶„ì„
"""

import re
import json
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from collections import defaultdict
import os

class LevelWiseCompactionExtractor:
    """ë ˆë²¨ë³„ ì»´íŒ©ì…˜ í†µê³„ ì¶”ì¶œê¸°"""
    
    def __init__(self, log_file_path):
        self.log_file_path = log_file_path
        self.compaction_events = []
        self.level_stats = defaultdict(lambda: defaultdict(int))
        self.temporal_stats = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))
        
    def extract_compaction_events(self):
        """ë¡œê·¸ì—ì„œ ì»´íŒ©ì…˜ ì´ë²¤íŠ¸ ì¶”ì¶œ"""
        print(f"ğŸ“Š ë¡œê·¸ íŒŒì¼ ë¶„ì„ ì¤‘: {self.log_file_path}")
        
        # ì»´íŒ©ì…˜ íŒ¨í„´ë“¤ (ì‹¤ì œ ë¡œê·¸ í˜•ì‹ì— ë§ê²Œ ìˆ˜ì •)
        patterns = {
            'flush': r'(\d{4}/\d{2}/\d{2}-\d{2}:\d{2}:\d{2}\.\d{6})\s+(\d+)\s+.*Level-0 flush table #(\d+): (\d+) bytes OK',
            'compaction_start': r'(\d{4}/\d{2}/\d{2}-\d{2}:\d{2}:\d{2}\.\d{6})\s+(\d+)\s+.*Started (\w+) compaction from level-(\d+) to level-(\d+)',
            'compaction_finish': r'(\d{4}/\d{2}/\d{2}-\d{2}:\d{2}:\d{2}\.\d{6})\s+(\d+)\s+.*(\w+) compaction from level-(\d+) to level-(\d+) finished: (\d+) bytes',
            'level_summary': r'(\d{4}/\d{2}/\d{2}-\d{2}:\d{2}:\d{2}\.\d{6})\s+(\d+)\s+.*Level summary: files\[([^\]]+)\]'
        }
        
        with open(self.log_file_path, 'r') as f:
            for line_num, line in enumerate(f):
                # Flush ì´ë²¤íŠ¸
                flush_match = re.search(patterns['flush'], line)
                if flush_match:
                    timestamp, thread_id, table_id, bytes_str = flush_match.groups()
                    self.compaction_events.append({
                        'timestamp': timestamp,
                        'thread_id': int(thread_id),
                        'level': 0,
                        'type': 'flush',
                        'table_id': int(table_id),
                        'bytes': int(bytes_str),
                        'line_num': line_num
                    })
                
                # Compaction ì™„ë£Œ ì´ë²¤íŠ¸
                compaction_match = re.search(patterns['compaction_finish'], line)
                if compaction_match:
                    timestamp, thread_id, compaction_type, from_level, to_level, bytes_str = compaction_match.groups()
                    self.compaction_events.append({
                        'timestamp': timestamp,
                        'thread_id': int(thread_id),
                        'level': int(to_level),
                        'type': 'compaction',
                        'compaction_type': compaction_type,
                        'from_level': int(from_level),
                        'to_level': int(to_level),
                        'bytes': int(bytes_str),
                        'line_num': line_num
                    })
        
        print(f"âœ… ì´ {len(self.compaction_events)}ê°œì˜ ì»´íŒ©ì…˜ ì´ë²¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ")
        return self.compaction_events
    
    def analyze_level_wise_stats(self):
        """ë ˆë²¨ë³„ í†µê³„ ë¶„ì„"""
        print("ğŸ“Š ë ˆë²¨ë³„ ì»´íŒ©ì…˜ í†µê³„ ë¶„ì„ ì¤‘...")
        
        # ë ˆë²¨ë³„ ê¸°ë³¸ í†µê³„
        for event in self.compaction_events:
            level = event['level']
            event_type = event['type']
            bytes_size = event['bytes']
            
            self.level_stats[level][f'{event_type}_count'] += 1
            self.level_stats[level][f'{event_type}_bytes'] += bytes_size
            
            # ì‹œê°„ëŒ€ë³„ ë¶„ì„ (ì‹œê°„ëŒ€ë³„ë¡œ ê·¸ë£¹í™”)
            timestamp = event['timestamp']
            hour_key = timestamp[:13]  # YYYY/MM/DD-HH
            
            self.temporal_stats[hour_key][level][f'{event_type}_count'] += 1
            self.temporal_stats[hour_key][level][f'{event_type}_bytes'] += bytes_size
        
        # ë ˆë²¨ë³„ RA/WA ê³„ì‚°
        level_amplification = {}
        for level in sorted(self.level_stats.keys()):
            stats = self.level_stats[level]
            
            # Write Amplification (WA)
            flush_bytes = stats.get('flush_bytes', 0)
            compaction_bytes = stats.get('compaction_bytes', 0)
            total_write_bytes = flush_bytes + compaction_bytes
            
            wa = total_write_bytes / flush_bytes if flush_bytes > 0 else 0
            
            # Read Amplification (RA) - compaction read bytes / user write bytes
            # FillRandomì—ì„œëŠ” user write â‰ˆ flush bytes
            ra = compaction_bytes / flush_bytes if flush_bytes > 0 else 0
            
            level_amplification[level] = {
                'write_amplification': wa,
                'read_amplification': ra,
                'flush_count': stats.get('flush_count', 0),
                'compaction_count': stats.get('compaction_count', 0),
                'flush_bytes': flush_bytes,
                'compaction_bytes': compaction_bytes,
                'total_bytes': total_write_bytes
            }
        
        return level_amplification
    
    def analyze_temporal_amplification(self):
        """ì‹œê¸°ë³„ ë ˆë²¨ë³„ RA/WA ë¶„ì„"""
        print("ğŸ“Š ì‹œê¸°ë³„ ë ˆë²¨ë³„ RA/WA ë¶„ì„ ì¤‘...")
        
        # ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_hours = sorted(self.temporal_stats.keys())
        
        # ì‹œê¸°ë³„ ë¶„í•  (ì‹¤ì œ ì„±ëŠ¥ ê¸°ë°˜)
        total_hours = len(sorted_hours)
        initial_hours = int(total_hours * 0.1)  # ì²˜ìŒ 10%
        middle_hours = int(total_hours * 0.3)   # ì¤‘ê°„ 30%
        final_hours = total_hours - initial_hours - middle_hours  # ë‚˜ë¨¸ì§€ 60%
        
        phases = {
            'initial': sorted_hours[:initial_hours],
            'middle': sorted_hours[initial_hours:initial_hours + middle_hours],
            'final': sorted_hours[initial_hours + middle_hours:]
        }
        
        temporal_amplification = {}
        
        for phase_name, hour_list in phases.items():
            phase_amplification = {}
            
            for level in [0, 1, 2, 3, 4, 5, 6]:  # L0-L6
                phase_flush_bytes = 0
                phase_compaction_bytes = 0
                phase_flush_count = 0
                phase_compaction_count = 0
                
                for hour in hour_list:
                    if hour in self.temporal_stats and level in self.temporal_stats[hour]:
                        hour_stats = self.temporal_stats[hour][level]
                        phase_flush_bytes += hour_stats.get('flush_bytes', 0)
                        phase_compaction_bytes += hour_stats.get('compaction_bytes', 0)
                        phase_flush_count += hour_stats.get('flush_count', 0)
                        phase_compaction_count += hour_stats.get('compaction_count', 0)
                
                # RA/WA ê³„ì‚°
                wa = (phase_flush_bytes + phase_compaction_bytes) / phase_flush_bytes if phase_flush_bytes > 0 else 0
                ra = phase_compaction_bytes / phase_flush_bytes if phase_flush_bytes > 0 else 0
                
                phase_amplification[level] = {
                    'write_amplification': wa,
                    'read_amplification': ra,
                    'flush_count': phase_flush_count,
                    'compaction_count': phase_compaction_count,
                    'flush_bytes': phase_flush_bytes,
                    'compaction_bytes': phase_compaction_bytes,
                    'io_intensity': phase_compaction_count / max(1, len(hour_list))  # ì‹œê°„ë‹¹ ì»´íŒ©ì…˜ ìˆ˜
                }
            
            temporal_amplification[phase_name] = phase_amplification
        
        return temporal_amplification
    
    def generate_enhanced_v4_2_model(self, level_amplification, temporal_amplification):
        """ê°œì„ ëœ v4.2 ëª¨ë¸ ìƒì„±"""
        print("ğŸš€ ì‹œê¸°ë³„ ë ˆë²¨ë³„ RA/WAë¥¼ ë°˜ì˜í•œ v4.2 ëª¨ë¸ ìƒì„± ì¤‘...")
        
        enhanced_model = {
            'model_version': 'v4.2_enhanced_level_wise',
            'creation_time': datetime.now().isoformat(),
            'level_wise_amplification': level_amplification,
            'temporal_level_amplification': temporal_amplification,
            'enhanced_predictions': {}
        }
        
        # ì‹œê¸°ë³„ ì˜ˆì¸¡ ëª¨ë¸ ìƒì„±
        for phase_name, phase_data in temporal_amplification.items():
            phase_predictions = {}
            
            # ë ˆë²¨ë³„ ì„±ëŠ¥ ì˜í–¥ë„ ê³„ì‚°
            level_io_impact = {}
            total_wa = 0
            total_ra = 0
            
            for level, level_data in phase_data.items():
                wa = level_data['write_amplification']
                ra = level_data['read_amplification']
                io_intensity = level_data['io_intensity']
                
                # ë ˆë²¨ë³„ I/O ì˜í–¥ë„ (ë ˆë²¨ì´ ê¹Šì„ìˆ˜ë¡ ì˜í–¥ ì¦ê°€)
                impact_factor = 1.0 + (level * 0.2)  # ë ˆë²¨ë³„ ì˜í–¥ ì¦ê°€
                io_impact = (wa + ra) * io_intensity * impact_factor
                
                level_io_impact[level] = {
                    'write_amplification': wa,
                    'read_amplification': ra,
                    'io_impact': io_impact,
                    'impact_factor': impact_factor,
                    'io_intensity': io_intensity
                }
                
                total_wa += wa
                total_ra += ra
            
            # ì‹œê¸°ë³„ ì „ì²´ ì„±ëŠ¥ ì˜ˆì¸¡
            avg_wa = total_wa / len(phase_data) if phase_data else 1.0
            avg_ra = total_ra / len(phase_data) if phase_data else 0.0
            
            # ì‹œê¸°ë³„ ì„±ëŠ¥ ì¸ì
            if phase_name == 'initial':
                performance_factor = 0.3  # ì´ˆê¸°: ë‚®ì€ ì„±ëŠ¥
                stability_factor = 0.2
                io_contention = 0.6
            elif phase_name == 'middle':
                performance_factor = 0.6  # ì¤‘ê¸°: ì¤‘ê°„ ì„±ëŠ¥
                stability_factor = 0.5
                io_contention = 0.8
            else:  # final
                performance_factor = 0.9  # í›„ê¸°: ë†’ì€ ì„±ëŠ¥
                stability_factor = 0.8
                io_contention = 0.9
            
            phase_predictions = {
                'level_wise_impact': level_io_impact,
                'overall_amplification': {
                    'avg_write_amplification': avg_wa,
                    'avg_read_amplification': avg_ra,
                    'performance_factor': performance_factor,
                    'stability_factor': stability_factor,
                    'io_contention': io_contention
                },
                'predicted_s_max': self._calculate_enhanced_s_max(
                    avg_wa, avg_ra, performance_factor, stability_factor
                )
            }
            
            enhanced_model['enhanced_predictions'][phase_name] = phase_predictions
        
        return enhanced_model
    
    def _calculate_enhanced_s_max(self, wa, ra, performance_factor, stability_factor):
        """ê°œì„ ëœ S_max ê³„ì‚°"""
        # ê¸°ë³¸ ëŒ€ì—­í­ (Phase-A ë°ì´í„° ê¸°ë°˜)
        base_write_bw = 1074.8  # MB/s (degraded state)
        base_read_bw = 1166.1   # MB/s
        
        # RA/WAë¥¼ ê³ ë ¤í•œ ì¡°ì •
        adjusted_write_bw = base_write_bw / (1 + wa * 0.1)  # WA ì˜í–¥
        adjusted_read_bw = base_read_bw / (1 + ra * 0.05)   # RA ì˜í–¥
        
        # ì„±ëŠ¥ ì¸ì ì ìš©
        effective_write_bw = adjusted_write_bw * performance_factor * stability_factor
        
        # S_max ê³„ì‚° (16KB key + 1KB value)
        s_max = (effective_write_bw * 1024 * 1024) / (16 + 1024)  # ops/sec
        
        return s_max
    
    def save_results(self, enhanced_model, output_dir="results"):
        """ê²°ê³¼ ì €ì¥"""
        os.makedirs(output_dir, exist_ok=True)
        
        # JSON ê²°ê³¼ ì €ì¥
        json_file = os.path.join(output_dir, "v4_2_enhanced_level_wise_model.json")
        with open(json_file, 'w') as f:
            json.dump(enhanced_model, f, indent=2)
        
        # ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±
        report_file = os.path.join(output_dir, "v4_2_enhanced_level_wise_report.md")
        self._generate_report(enhanced_model, report_file)
        
        print(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
        print(f"   - JSON: {json_file}")
        print(f"   - Report: {report_file}")
    
    def _generate_report(self, enhanced_model, report_file):
        """ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±"""
        with open(report_file, 'w') as f:
            f.write("# V4.2 Enhanced Level-Wise Model Report\n\n")
            f.write(f"**ìƒì„± ì‹œê°„**: {enhanced_model['creation_time']}\n\n")
            
            # ë ˆë²¨ë³„ ê¸°ë³¸ í†µê³„
            f.write("## ë ˆë²¨ë³„ ê¸°ë³¸ í†µê³„\n\n")
            for level, data in enhanced_model['level_wise_amplification'].items():
                f.write(f"### Level {level}\n")
                f.write(f"- **Write Amplification**: {data['write_amplification']:.3f}\n")
                f.write(f"- **Read Amplification**: {data['read_amplification']:.3f}\n")
                f.write(f"- **Flush Count**: {data['flush_count']:,}\n")
                f.write(f"- **Compaction Count**: {data['compaction_count']:,}\n")
                f.write(f"- **Total Bytes**: {data['total_bytes']:,} bytes\n\n")
            
            # ì‹œê¸°ë³„ ë¶„ì„
            f.write("## ì‹œê¸°ë³„ ë ˆë²¨ë³„ RA/WA ë¶„ì„\n\n")
            for phase_name, phase_data in enhanced_model['temporal_level_amplification'].items():
                f.write(f"### {phase_name.title()} Phase\n")
                for level, data in phase_data.items():
                    f.write(f"**Level {level}**:\n")
                    f.write(f"- WA: {data['write_amplification']:.3f}\n")
                    f.write(f"- RA: {data['read_amplification']:.3f}\n")
                    f.write(f"- I/O Intensity: {data['io_intensity']:.2f}\n")
                f.write("\n")
            
            # ì˜ˆì¸¡ ê²°ê³¼
            f.write("## í–¥ìƒëœ ì˜ˆì¸¡ ê²°ê³¼\n\n")
            for phase_name, predictions in enhanced_model['enhanced_predictions'].items():
                f.write(f"### {phase_name.title()} Phase Predictions\n")
                overall = predictions['overall_amplification']
                f.write(f"- **ì˜ˆì¸¡ S_max**: {predictions['predicted_s_max']:,.0f} ops/sec\n")
                f.write(f"- **í‰ê·  WA**: {overall['avg_write_amplification']:.3f}\n")
                f.write(f"- **í‰ê·  RA**: {overall['avg_read_amplification']:.3f}\n")
                f.write(f"- **ì„±ëŠ¥ ì¸ì**: {overall['performance_factor']:.2f}\n")
                f.write(f"- **ì•ˆì •ì„± ì¸ì**: {overall['stability_factor']:.2f}\n")
                f.write(f"- **I/O ê²½í•©**: {overall['io_contention']:.2f}\n\n")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ V4.2 Enhanced Level-Wise Model ìƒì„± ì‹œì‘")
    print("=" * 60)
    
    # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
    log_file = "/home/sslab/rocksdb-put-model/experiments/2025-09-12/phase-b/rocksdb_log_phase_b.log"
    
    if not os.path.exists(log_file):
        print(f"âŒ ë¡œê·¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {log_file}")
        return
    
    # ì¶”ì¶œê¸° ìƒì„± ë° ì‹¤í–‰
    extractor = LevelWiseCompactionExtractor(log_file)
    
    # 1. ì»´íŒ©ì…˜ ì´ë²¤íŠ¸ ì¶”ì¶œ
    extractor.extract_compaction_events()
    
    # 2. ë ˆë²¨ë³„ í†µê³„ ë¶„ì„
    level_amplification = extractor.analyze_level_wise_stats()
    
    # 3. ì‹œê¸°ë³„ ë¶„ì„
    temporal_amplification = extractor.analyze_temporal_amplification()
    
    # 4. í–¥ìƒëœ ëª¨ë¸ ìƒì„±
    enhanced_model = extractor.generate_enhanced_v4_2_model(level_amplification, temporal_amplification)
    
    # 5. ê²°ê³¼ ì €ì¥
    extractor.save_results(enhanced_model)
    
    print("\nâœ… V4.2 Enhanced Level-Wise Model ìƒì„± ì™„ë£Œ!")
    print("=" * 60)

if __name__ == "__main__":
    main()
