#!/usr/bin/env python3
"""
Real-time Monitor for Phase-D
ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ì‹œìŠ¤í…œ
"""

import os
import json
import time
import threading
from datetime import datetime
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

class RealTimeMonitor:
    def __init__(self):
        self.metrics_collector = MetricsCollector()
        self.alert_system = AlertSystem()
        self.monitoring_active = False
        self.metrics_history = []
        self.alerts_history = []
        
    def start_monitoring(self, interval=5):
        """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        print(f"ğŸ“Š ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì‘ (ê°„ê²©: {interval}ì´ˆ)")
        
        self.monitoring_active = True
        
        def monitor_loop():
            while self.monitoring_active:
                try:
                    # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                    current_metrics = self.metrics_collector.collect_metrics()
                    
                    # ë©”íŠ¸ë¦­ ê¸°ë¡
                    self.record_metrics(current_metrics)
                    
                    # ì´ìƒ íƒì§€
                    anomalies = self.detect_anomalies(current_metrics)
                    
                    # ì•Œë¦¼ ìƒì„±
                    if anomalies:
                        self.alert_system.generate_alerts(anomalies)
                    
                    # ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸
                    self.update_dashboard(current_metrics)
                    
                    time.sleep(interval)
                    
                except Exception as e:
                    print(f"âŒ ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                    time.sleep(interval)
        
        # ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ ì‹œì‘
        monitor_thread = threading.Thread(target=monitor_loop, daemon=True)
        monitor_thread.start()
        
        print("âœ… ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì‘ë¨")
    
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        print("â¹ï¸ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€")
        self.monitoring_active = False
    
    def record_metrics(self, metrics):
        """ë©”íŠ¸ë¦­ ê¸°ë¡"""
        record = {
            'timestamp': datetime.now().isoformat(),
            'metrics': metrics
        }
        
        self.metrics_history.append(record)
        
        # íŒŒì¼ì— ì €ì¥
        self.save_metrics_to_file(record)
    
    def detect_anomalies(self, metrics):
        """ì´ìƒ íƒì§€"""
        anomalies = []
        
        # QPS ì´ìƒ íƒì§€
        if metrics.get('qps', 0) > 5000:  # ì„ê³„ê°’
            anomalies.append({
                'type': 'high_qps',
                'value': metrics['qps'],
                'threshold': 5000,
                'severity': 'warning'
            })
        
        # ì§€ì—°ì‹œê°„ ì´ìƒ íƒì§€
        if metrics.get('latency', 0) > 5.0:  # ì„ê³„ê°’
            anomalies.append({
                'type': 'high_latency',
                'value': metrics['latency'],
                'threshold': 5.0,
                'severity': 'critical'
            })
        
        # CPU ì‚¬ìš©ë¥  ì´ìƒ íƒì§€
        if metrics.get('cpu_usage', 0) > 90:  # ì„ê³„ê°’
            anomalies.append({
                'type': 'high_cpu',
                'value': metrics['cpu_usage'],
                'threshold': 90,
                'severity': 'warning'
            })
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì´ìƒ íƒì§€
        if metrics.get('memory_usage', 0) > 95:  # ì„ê³„ê°’
            anomalies.append({
                'type': 'high_memory',
                'value': metrics['memory_usage'],
                'threshold': 95,
                'severity': 'critical'
            })
        
        return anomalies
    
    def update_dashboard(self, metrics):
        """ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸"""
        # ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìƒì„±
        dashboard_data = {
            'timestamp': datetime.now().isoformat(),
            'current_metrics': metrics,
            'system_status': self.get_system_status(metrics),
            'performance_trend': self.get_performance_trend()
        }
        
        # ëŒ€ì‹œë³´ë“œ íŒŒì¼ ì €ì¥
        self.save_dashboard_data(dashboard_data)
    
    def get_system_status(self, metrics):
        """ì‹œìŠ¤í…œ ìƒíƒœ í‰ê°€"""
        status = 'healthy'
        
        # ìƒíƒœ í‰ê°€ ë¡œì§
        if metrics.get('latency', 0) > 5.0 or metrics.get('memory_usage', 0) > 95:
            status = 'critical'
        elif metrics.get('cpu_usage', 0) > 90 or metrics.get('qps', 0) > 5000:
            status = 'warning'
        
        return status
    
    def get_performance_trend(self):
        """ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„"""
        if len(self.metrics_history) < 10:
            return {'trend': 'insufficient_data'}
        
        # ìµœê·¼ 10ê°œ ê¸°ë¡ ë¶„ì„
        recent_metrics = self.metrics_history[-10:]
        qps_values = [record['metrics'].get('qps', 0) for record in recent_metrics]
        latency_values = [record['metrics'].get('latency', 0) for record in recent_metrics]
        
        # íŠ¸ë Œë“œ ê³„ì‚°
        qps_trend = np.polyfit(range(len(qps_values)), qps_values, 1)[0]
        latency_trend = np.polyfit(range(len(latency_values)), latency_values, 1)[0]
        
        trend_analysis = {
            'qps_trend': 'increasing' if qps_trend > 0 else 'decreasing',
            'latency_trend': 'increasing' if latency_trend > 0 else 'decreasing',
            'overall_trend': 'improving' if qps_trend > 0 and latency_trend < 0 else 'degrading'
        }
        
        return trend_analysis
    
    def save_metrics_to_file(self, record):
        """ë©”íŠ¸ë¦­ì„ íŒŒì¼ì— ì €ì¥"""
        results_dir = '/home/sslab/rocksdb-put-model/experiments/2025-09-12/phase-d/results'
        os.makedirs(results_dir, exist_ok=True)
        
        metrics_file = f"{results_dir}/real_time_metrics.json"
        
        # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
        all_metrics = []
        if os.path.exists(metrics_file):
            try:
                with open(metrics_file, 'r') as f:
                    all_metrics = json.load(f)
            except:
                all_metrics = []
        
        # ìƒˆ ê¸°ë¡ ì¶”ê°€
        all_metrics.append(record)
        
        # ìµœê·¼ 1000ê°œ ê¸°ë¡ë§Œ ìœ ì§€
        if len(all_metrics) > 1000:
            all_metrics = all_metrics[-1000:]
        
        # íŒŒì¼ ì €ì¥
        with open(metrics_file, 'w') as f:
            json.dump(all_metrics, f, indent=2)
    
    def save_dashboard_data(self, dashboard_data):
        """ëŒ€ì‹œë³´ë“œ ë°ì´í„° ì €ì¥"""
        results_dir = '/home/sslab/rocksdb-put-model/experiments/2025-09-12/phase-d/results'
        os.makedirs(results_dir, exist_ok=True)
        
        dashboard_file = f"{results_dir}/dashboard_data.json"
        
        with open(dashboard_file, 'w') as f:
            json.dump(dashboard_data, f, indent=2)
    
    def generate_performance_report(self):
        """ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±"""
        print("ğŸ“Š ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        if not self.metrics_history:
            print("âŒ ì„±ëŠ¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ë°ì´í„° ë¶„ì„
        df = pd.DataFrame([
            {
                'timestamp': record['timestamp'],
                **record['metrics']
            }
            for record in self.metrics_history
        ])
        
        # í†µê³„ ê³„ì‚°
        stats = {
            'total_records': len(df),
            'time_range': {
                'start': df['timestamp'].min(),
                'end': df['timestamp'].max()
            },
            'qps_stats': {
                'mean': df['qps'].mean(),
                'std': df['qps'].std(),
                'min': df['qps'].min(),
                'max': df['qps'].max()
            },
            'latency_stats': {
                'mean': df['latency'].mean(),
                'std': df['latency'].std(),
                'min': df['latency'].min(),
                'max': df['latency'].max()
            }
        }
        
        # ë³´ê³ ì„œ ì €ì¥
        results_dir = '/home/sslab/rocksdb-put-model/experiments/2025-09-12/phase-d/results'
        report_file = f"{results_dir}/performance_report.json"
        
        with open(report_file, 'w') as f:
            json.dump(stats, f, indent=2)
        
        print(f"âœ… ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {report_file}")
        
        return stats

class MetricsCollector:
    def __init__(self):
        self.collection_count = 0
    
    def collect_metrics(self):
        """ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        self.collection_count += 1
        
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” RocksDB í†µê³„, ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ë“±ì„ ìˆ˜ì§‘
        metrics = {
            'timestamp': datetime.now().isoformat(),
            'qps': np.random.normal(1000, 200),  # ì‹œë®¬ë ˆì´ì…˜
            'latency': np.random.normal(1.0, 0.2),
            'cpu_usage': np.random.normal(50, 15),
            'memory_usage': np.random.normal(60, 10),
            'io_utilization': np.random.normal(40, 10),
            'compaction_activity': np.random.normal(30, 8),
            'collection_count': self.collection_count
        }
        
        return metrics

class AlertSystem:
    def __init__(self):
        self.alert_count = 0
    
    def generate_alerts(self, anomalies):
        """ì•Œë¦¼ ìƒì„±"""
        for anomaly in anomalies:
            alert = {
                'timestamp': datetime.now().isoformat(),
                'type': anomaly['type'],
                'severity': anomaly['severity'],
                'value': anomaly['value'],
                'threshold': anomaly['threshold'],
                'message': self.generate_alert_message(anomaly)
            }
            
            self.alert_count += 1
            print(f"ğŸš¨ ì•Œë¦¼ #{self.alert_count}: {alert['message']}")
            
            # ì•Œë¦¼ ê¸°ë¡ ì €ì¥
            self.save_alert(alert)
    
    def generate_alert_message(self, anomaly):
        """ì•Œë¦¼ ë©”ì‹œì§€ ìƒì„±"""
        messages = {
            'high_qps': f"ë†’ì€ QPS ê°ì§€: {anomaly['value']:.2f} (ì„ê³„ê°’: {anomaly['threshold']})",
            'high_latency': f"ë†’ì€ ì§€ì—°ì‹œê°„ ê°ì§€: {anomaly['value']:.2f}ì´ˆ (ì„ê³„ê°’: {anomaly['threshold']}ì´ˆ)",
            'high_cpu': f"ë†’ì€ CPU ì‚¬ìš©ë¥  ê°ì§€: {anomaly['value']:.1f}% (ì„ê³„ê°’: {anomaly['threshold']}%)",
            'high_memory': f"ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ê°ì§€: {anomaly['value']:.1f}% (ì„ê³„ê°’: {anomaly['threshold']}%)"
        }
        
        return messages.get(anomaly['type'], f"ì•Œ ìˆ˜ ì—†ëŠ” ì´ìƒ: {anomaly['type']}")
    
    def save_alert(self, alert):
        """ì•Œë¦¼ ì €ì¥"""
        results_dir = '/home/sslab/rocksdb-put-model/experiments/2025-09-12/phase-d/results'
        os.makedirs(results_dir, exist_ok=True)
        
        alerts_file = f"{results_dir}/alerts.json"
        
        # ê¸°ì¡´ ì•Œë¦¼ ë¡œë“œ
        alerts = []
        if os.path.exists(alerts_file):
            try:
                with open(alerts_file, 'r') as f:
                    alerts = json.load(f)
            except:
                alerts = []
        
        # ìƒˆ ì•Œë¦¼ ì¶”ê°€
        alerts.append(alert)
        
        # íŒŒì¼ ì €ì¥
        with open(alerts_file, 'w') as f:
            json.dump(alerts, f, indent=2)

if __name__ == "__main__":
    # Real-time Monitor í…ŒìŠ¤íŠ¸
    monitor = RealTimeMonitor()
    
    # ëª¨ë‹ˆí„°ë§ ì‹œì‘
    monitor.start_monitoring(interval=3)
    
    # 15ì´ˆ ë™ì•ˆ ëª¨ë‹ˆí„°ë§
    time.sleep(15)
    
    # ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
    monitor.stop_monitoring()
    
    # ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±
    stats = monitor.generate_performance_report()
    print(f"ğŸ“Š ì„±ëŠ¥ í†µê³„: {json.dumps(stats, indent=2)}")
